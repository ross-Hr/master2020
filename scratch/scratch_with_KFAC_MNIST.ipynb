{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f94fd537f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import numpy as np\n",
    "#import input_data\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from math import *\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import KFAC, DiagHessian, DiagGGNMC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from tqdm import tqdm, trange\n",
    "from backpack.core.layers import Flatten\n",
    "import pytest\n",
    "from DirLPA_utils import * \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPADirNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LPADirNN, self).__init__()\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.mp = torch.nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.c1 = torch.nn.Conv2d(1, 32, 5)\n",
    "        self.c2 = torch.nn.Conv2d(32, 64, 5)\n",
    "        self.flatten = Flatten()\n",
    "        self.l1 = torch.nn.Linear(4 * 4 * 64, 250) #changed from 500\n",
    "        self.fc = torch.nn.Linear(250, 10)  #changed from 500\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.phi(x)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return(out)\n",
    "    \n",
    "    def phi(self, x): \n",
    "        \n",
    "        x = self.mp(self.relu(self.c1(x)))\n",
    "        x = self.mp(self.relu(self.c2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.l1(x))\n",
    "        \n",
    "        return(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TRAIN_MNIST = 128\n",
    "BATCH_SIZE_TEST_MNIST = 128\n",
    "MAX_ITER_MNIST = 6\n",
    "LR_TRAIN_MNIST = 10e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "MNIST_transform_normalized = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                (0.1307,), (0.3081,)\n",
    "            )\n",
    "        ])\n",
    "#\"\"\"\n",
    "\n",
    "MNIST_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "MNIST_train = torchvision.datasets.MNIST(\n",
    "        './data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "mnist_train_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_train,\n",
    "    batch_size=BATCH_SIZE_TRAIN_MNIST,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "MNIST_test = torchvision.datasets.MNIST(\n",
    "        './data',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "mnist_test_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = LPADirNN()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#mnist_train_optimizer = torch.optim.Adam(mnist_model.parameters(), lr=LR_TRAIN_MNIST)\n",
    "mnist_train_optimizer = torch.optim.Adam(mnist_model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "MNIST_PATH = \"models/mnist_test_6iter_250.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0; 0/469 \tMinibatch Loss 2.303  Accuracy 10%\n",
      "Iteration 0; 1/469 \tMinibatch Loss 2.270  Accuracy 19%\n",
      "Iteration 0; 2/469 \tMinibatch Loss 2.242  Accuracy 19%\n",
      "Iteration 0; 3/469 \tMinibatch Loss 2.186  Accuracy 43%\n",
      "Iteration 0; 4/469 \tMinibatch Loss 2.099  Accuracy 59%\n",
      "Iteration 0; 5/469 \tMinibatch Loss 2.038  Accuracy 53%\n",
      "Iteration 0; 6/469 \tMinibatch Loss 1.920  Accuracy 66%\n",
      "Iteration 0; 7/469 \tMinibatch Loss 1.775  Accuracy 70%\n",
      "Iteration 0; 8/469 \tMinibatch Loss 1.577  Accuracy 77%\n",
      "Iteration 0; 9/469 \tMinibatch Loss 1.412  Accuracy 75%\n",
      "Iteration 0; 10/469 \tMinibatch Loss 1.250  Accuracy 73%\n",
      "Iteration 0; 11/469 \tMinibatch Loss 1.266  Accuracy 66%\n",
      "Iteration 0; 12/469 \tMinibatch Loss 1.143  Accuracy 66%\n",
      "Iteration 0; 13/469 \tMinibatch Loss 0.909  Accuracy 80%\n",
      "Iteration 0; 14/469 \tMinibatch Loss 0.889  Accuracy 80%\n",
      "Iteration 0; 15/469 \tMinibatch Loss 0.781  Accuracy 83%\n",
      "Iteration 0; 16/469 \tMinibatch Loss 0.860  Accuracy 80%\n",
      "Iteration 0; 17/469 \tMinibatch Loss 0.650  Accuracy 79%\n",
      "Iteration 0; 18/469 \tMinibatch Loss 0.600  Accuracy 76%\n",
      "Iteration 0; 19/469 \tMinibatch Loss 0.490  Accuracy 84%\n",
      "Iteration 0; 20/469 \tMinibatch Loss 0.518  Accuracy 87%\n",
      "Iteration 0; 21/469 \tMinibatch Loss 0.516  Accuracy 83%\n",
      "Iteration 0; 22/469 \tMinibatch Loss 0.426  Accuracy 88%\n",
      "Iteration 0; 23/469 \tMinibatch Loss 0.530  Accuracy 84%\n",
      "Iteration 0; 24/469 \tMinibatch Loss 0.470  Accuracy 88%\n",
      "Iteration 0; 25/469 \tMinibatch Loss 0.396  Accuracy 87%\n",
      "Iteration 0; 26/469 \tMinibatch Loss 0.408  Accuracy 88%\n",
      "Iteration 0; 27/469 \tMinibatch Loss 0.434  Accuracy 86%\n",
      "Iteration 0; 28/469 \tMinibatch Loss 0.364  Accuracy 91%\n",
      "Iteration 0; 29/469 \tMinibatch Loss 0.371  Accuracy 89%\n",
      "Iteration 0; 30/469 \tMinibatch Loss 0.399  Accuracy 84%\n",
      "Iteration 0; 31/469 \tMinibatch Loss 0.403  Accuracy 91%\n",
      "Iteration 0; 32/469 \tMinibatch Loss 0.338  Accuracy 91%\n",
      "Iteration 0; 33/469 \tMinibatch Loss 0.487  Accuracy 87%\n",
      "Iteration 0; 34/469 \tMinibatch Loss 0.509  Accuracy 87%\n",
      "Iteration 0; 35/469 \tMinibatch Loss 0.412  Accuracy 86%\n",
      "Iteration 0; 36/469 \tMinibatch Loss 0.435  Accuracy 88%\n",
      "Iteration 0; 37/469 \tMinibatch Loss 0.336  Accuracy 91%\n",
      "Iteration 0; 38/469 \tMinibatch Loss 0.404  Accuracy 91%\n",
      "Iteration 0; 39/469 \tMinibatch Loss 0.348  Accuracy 91%\n",
      "Iteration 0; 40/469 \tMinibatch Loss 0.330  Accuracy 91%\n",
      "Iteration 0; 41/469 \tMinibatch Loss 0.311  Accuracy 91%\n",
      "Iteration 0; 42/469 \tMinibatch Loss 0.333  Accuracy 90%\n",
      "Iteration 0; 43/469 \tMinibatch Loss 0.454  Accuracy 91%\n",
      "Iteration 0; 44/469 \tMinibatch Loss 0.321  Accuracy 88%\n",
      "Iteration 0; 45/469 \tMinibatch Loss 0.435  Accuracy 91%\n",
      "Iteration 0; 46/469 \tMinibatch Loss 0.304  Accuracy 91%\n",
      "Iteration 0; 47/469 \tMinibatch Loss 0.224  Accuracy 93%\n",
      "Iteration 0; 48/469 \tMinibatch Loss 0.231  Accuracy 92%\n",
      "Iteration 0; 49/469 \tMinibatch Loss 0.347  Accuracy 88%\n",
      "Iteration 0; 50/469 \tMinibatch Loss 0.251  Accuracy 93%\n",
      "Iteration 0; 51/469 \tMinibatch Loss 0.252  Accuracy 91%\n",
      "Iteration 0; 52/469 \tMinibatch Loss 0.277  Accuracy 92%\n",
      "Iteration 0; 53/469 \tMinibatch Loss 0.153  Accuracy 95%\n",
      "Iteration 0; 54/469 \tMinibatch Loss 0.229  Accuracy 93%\n",
      "Iteration 0; 55/469 \tMinibatch Loss 0.318  Accuracy 92%\n",
      "Iteration 0; 56/469 \tMinibatch Loss 0.385  Accuracy 86%\n",
      "Iteration 0; 57/469 \tMinibatch Loss 0.188  Accuracy 95%\n",
      "Iteration 0; 58/469 \tMinibatch Loss 0.323  Accuracy 92%\n",
      "Iteration 0; 59/469 \tMinibatch Loss 0.187  Accuracy 95%\n",
      "Iteration 0; 60/469 \tMinibatch Loss 0.257  Accuracy 95%\n",
      "Iteration 0; 61/469 \tMinibatch Loss 0.153  Accuracy 95%\n",
      "Iteration 0; 62/469 \tMinibatch Loss 0.236  Accuracy 95%\n",
      "Iteration 0; 63/469 \tMinibatch Loss 0.251  Accuracy 92%\n",
      "Iteration 0; 64/469 \tMinibatch Loss 0.163  Accuracy 97%\n",
      "Iteration 0; 65/469 \tMinibatch Loss 0.226  Accuracy 92%\n",
      "Iteration 0; 66/469 \tMinibatch Loss 0.276  Accuracy 91%\n",
      "Iteration 0; 67/469 \tMinibatch Loss 0.199  Accuracy 94%\n",
      "Iteration 0; 68/469 \tMinibatch Loss 0.207  Accuracy 95%\n",
      "Iteration 0; 69/469 \tMinibatch Loss 0.208  Accuracy 95%\n",
      "Iteration 0; 70/469 \tMinibatch Loss 0.121  Accuracy 96%\n",
      "Iteration 0; 71/469 \tMinibatch Loss 0.183  Accuracy 95%\n",
      "Iteration 0; 72/469 \tMinibatch Loss 0.236  Accuracy 92%\n",
      "Iteration 0; 73/469 \tMinibatch Loss 0.286  Accuracy 92%\n",
      "Iteration 0; 74/469 \tMinibatch Loss 0.210  Accuracy 93%\n",
      "Iteration 0; 75/469 \tMinibatch Loss 0.264  Accuracy 92%\n",
      "Iteration 0; 76/469 \tMinibatch Loss 0.216  Accuracy 95%\n",
      "Iteration 0; 77/469 \tMinibatch Loss 0.290  Accuracy 90%\n",
      "Iteration 0; 78/469 \tMinibatch Loss 0.176  Accuracy 95%\n",
      "Iteration 0; 79/469 \tMinibatch Loss 0.150  Accuracy 95%\n",
      "Iteration 0; 80/469 \tMinibatch Loss 0.155  Accuracy 95%\n",
      "Iteration 0; 81/469 \tMinibatch Loss 0.322  Accuracy 94%\n",
      "Iteration 0; 82/469 \tMinibatch Loss 0.241  Accuracy 91%\n",
      "Iteration 0; 83/469 \tMinibatch Loss 0.197  Accuracy 93%\n",
      "Iteration 0; 84/469 \tMinibatch Loss 0.126  Accuracy 96%\n",
      "Iteration 0; 85/469 \tMinibatch Loss 0.241  Accuracy 92%\n",
      "Iteration 0; 86/469 \tMinibatch Loss 0.119  Accuracy 97%\n",
      "Iteration 0; 87/469 \tMinibatch Loss 0.256  Accuracy 92%\n",
      "Iteration 0; 88/469 \tMinibatch Loss 0.184  Accuracy 95%\n",
      "Iteration 0; 89/469 \tMinibatch Loss 0.173  Accuracy 95%\n",
      "Iteration 0; 90/469 \tMinibatch Loss 0.263  Accuracy 91%\n",
      "Iteration 0; 91/469 \tMinibatch Loss 0.170  Accuracy 94%\n",
      "Iteration 0; 92/469 \tMinibatch Loss 0.126  Accuracy 95%\n",
      "Iteration 0; 93/469 \tMinibatch Loss 0.222  Accuracy 93%\n",
      "Iteration 0; 94/469 \tMinibatch Loss 0.135  Accuracy 96%\n",
      "Iteration 0; 95/469 \tMinibatch Loss 0.185  Accuracy 96%\n",
      "Iteration 0; 96/469 \tMinibatch Loss 0.171  Accuracy 95%\n",
      "Iteration 0; 97/469 \tMinibatch Loss 0.196  Accuracy 95%\n",
      "Iteration 0; 98/469 \tMinibatch Loss 0.140  Accuracy 96%\n",
      "Iteration 0; 99/469 \tMinibatch Loss 0.166  Accuracy 95%\n",
      "Iteration 0; 100/469 \tMinibatch Loss 0.223  Accuracy 91%\n",
      "Iteration 0; 101/469 \tMinibatch Loss 0.189  Accuracy 94%\n",
      "Iteration 0; 102/469 \tMinibatch Loss 0.349  Accuracy 91%\n",
      "Iteration 0; 103/469 \tMinibatch Loss 0.179  Accuracy 93%\n",
      "Iteration 0; 104/469 \tMinibatch Loss 0.157  Accuracy 95%\n",
      "Iteration 0; 105/469 \tMinibatch Loss 0.156  Accuracy 95%\n",
      "Iteration 0; 106/469 \tMinibatch Loss 0.146  Accuracy 95%\n",
      "Iteration 0; 107/469 \tMinibatch Loss 0.234  Accuracy 95%\n",
      "Iteration 0; 108/469 \tMinibatch Loss 0.197  Accuracy 94%\n",
      "Iteration 0; 109/469 \tMinibatch Loss 0.207  Accuracy 93%\n",
      "Iteration 0; 110/469 \tMinibatch Loss 0.174  Accuracy 95%\n",
      "Iteration 0; 111/469 \tMinibatch Loss 0.108  Accuracy 96%\n",
      "Iteration 0; 112/469 \tMinibatch Loss 0.151  Accuracy 98%\n",
      "Iteration 0; 113/469 \tMinibatch Loss 0.191  Accuracy 95%\n",
      "Iteration 0; 114/469 \tMinibatch Loss 0.064  Accuracy 99%\n",
      "Iteration 0; 115/469 \tMinibatch Loss 0.198  Accuracy 93%\n",
      "Iteration 0; 116/469 \tMinibatch Loss 0.195  Accuracy 91%\n",
      "Iteration 0; 117/469 \tMinibatch Loss 0.231  Accuracy 92%\n",
      "Iteration 0; 118/469 \tMinibatch Loss 0.126  Accuracy 96%\n",
      "Iteration 0; 119/469 \tMinibatch Loss 0.152  Accuracy 94%\n",
      "Iteration 0; 120/469 \tMinibatch Loss 0.155  Accuracy 97%\n",
      "Iteration 0; 121/469 \tMinibatch Loss 0.219  Accuracy 95%\n",
      "Iteration 0; 122/469 \tMinibatch Loss 0.221  Accuracy 97%\n",
      "Iteration 0; 123/469 \tMinibatch Loss 0.100  Accuracy 97%\n",
      "Iteration 0; 124/469 \tMinibatch Loss 0.149  Accuracy 98%\n",
      "Iteration 0; 125/469 \tMinibatch Loss 0.140  Accuracy 95%\n",
      "Iteration 0; 126/469 \tMinibatch Loss 0.081  Accuracy 98%\n",
      "Iteration 0; 127/469 \tMinibatch Loss 0.135  Accuracy 95%\n",
      "Iteration 0; 128/469 \tMinibatch Loss 0.135  Accuracy 97%\n",
      "Iteration 0; 129/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 0; 130/469 \tMinibatch Loss 0.108  Accuracy 98%\n",
      "Iteration 0; 131/469 \tMinibatch Loss 0.087  Accuracy 96%\n",
      "Iteration 0; 132/469 \tMinibatch Loss 0.166  Accuracy 95%\n",
      "Iteration 0; 133/469 \tMinibatch Loss 0.178  Accuracy 92%\n",
      "Iteration 0; 134/469 \tMinibatch Loss 0.194  Accuracy 93%\n",
      "Iteration 0; 135/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 0; 136/469 \tMinibatch Loss 0.133  Accuracy 98%\n",
      "Iteration 0; 137/469 \tMinibatch Loss 0.129  Accuracy 95%\n",
      "Iteration 0; 138/469 \tMinibatch Loss 0.153  Accuracy 95%\n",
      "Iteration 0; 139/469 \tMinibatch Loss 0.093  Accuracy 98%\n",
      "Iteration 0; 140/469 \tMinibatch Loss 0.094  Accuracy 98%\n",
      "Iteration 0; 141/469 \tMinibatch Loss 0.086  Accuracy 98%\n",
      "Iteration 0; 142/469 \tMinibatch Loss 0.175  Accuracy 96%\n",
      "Iteration 0; 143/469 \tMinibatch Loss 0.154  Accuracy 95%\n",
      "Iteration 0; 144/469 \tMinibatch Loss 0.174  Accuracy 95%\n",
      "Iteration 0; 145/469 \tMinibatch Loss 0.081  Accuracy 98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0; 146/469 \tMinibatch Loss 0.092  Accuracy 98%\n",
      "Iteration 0; 147/469 \tMinibatch Loss 0.149  Accuracy 96%\n",
      "Iteration 0; 148/469 \tMinibatch Loss 0.166  Accuracy 95%\n",
      "Iteration 0; 149/469 \tMinibatch Loss 0.078  Accuracy 97%\n",
      "Iteration 0; 150/469 \tMinibatch Loss 0.044  Accuracy 100%\n",
      "Iteration 0; 151/469 \tMinibatch Loss 0.098  Accuracy 98%\n",
      "Iteration 0; 152/469 \tMinibatch Loss 0.225  Accuracy 95%\n",
      "Iteration 0; 153/469 \tMinibatch Loss 0.208  Accuracy 95%\n",
      "Iteration 0; 154/469 \tMinibatch Loss 0.092  Accuracy 96%\n",
      "Iteration 0; 155/469 \tMinibatch Loss 0.138  Accuracy 95%\n",
      "Iteration 0; 156/469 \tMinibatch Loss 0.101  Accuracy 97%\n",
      "Iteration 0; 157/469 \tMinibatch Loss 0.172  Accuracy 94%\n",
      "Iteration 0; 158/469 \tMinibatch Loss 0.241  Accuracy 91%\n",
      "Iteration 0; 159/469 \tMinibatch Loss 0.088  Accuracy 97%\n",
      "Iteration 0; 160/469 \tMinibatch Loss 0.116  Accuracy 96%\n",
      "Iteration 0; 161/469 \tMinibatch Loss 0.104  Accuracy 95%\n",
      "Iteration 0; 162/469 \tMinibatch Loss 0.165  Accuracy 95%\n",
      "Iteration 0; 163/469 \tMinibatch Loss 0.122  Accuracy 96%\n",
      "Iteration 0; 164/469 \tMinibatch Loss 0.191  Accuracy 95%\n",
      "Iteration 0; 165/469 \tMinibatch Loss 0.198  Accuracy 96%\n",
      "Iteration 0; 166/469 \tMinibatch Loss 0.093  Accuracy 98%\n",
      "Iteration 0; 167/469 \tMinibatch Loss 0.047  Accuracy 99%\n",
      "Iteration 0; 168/469 \tMinibatch Loss 0.096  Accuracy 98%\n",
      "Iteration 0; 169/469 \tMinibatch Loss 0.139  Accuracy 95%\n",
      "Iteration 0; 170/469 \tMinibatch Loss 0.149  Accuracy 93%\n",
      "Iteration 0; 171/469 \tMinibatch Loss 0.122  Accuracy 97%\n",
      "Iteration 0; 172/469 \tMinibatch Loss 0.124  Accuracy 97%\n",
      "Iteration 0; 173/469 \tMinibatch Loss 0.084  Accuracy 97%\n",
      "Iteration 0; 174/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 0; 175/469 \tMinibatch Loss 0.082  Accuracy 98%\n",
      "Iteration 0; 176/469 \tMinibatch Loss 0.115  Accuracy 96%\n",
      "Iteration 0; 177/469 \tMinibatch Loss 0.126  Accuracy 96%\n",
      "Iteration 0; 178/469 \tMinibatch Loss 0.186  Accuracy 94%\n",
      "Iteration 0; 179/469 \tMinibatch Loss 0.066  Accuracy 97%\n",
      "Iteration 0; 180/469 \tMinibatch Loss 0.092  Accuracy 97%\n",
      "Iteration 0; 181/469 \tMinibatch Loss 0.065  Accuracy 99%\n",
      "Iteration 0; 182/469 \tMinibatch Loss 0.135  Accuracy 98%\n",
      "Iteration 0; 183/469 \tMinibatch Loss 0.145  Accuracy 95%\n",
      "Iteration 0; 184/469 \tMinibatch Loss 0.055  Accuracy 99%\n",
      "Iteration 0; 185/469 \tMinibatch Loss 0.090  Accuracy 98%\n",
      "Iteration 0; 186/469 \tMinibatch Loss 0.115  Accuracy 97%\n",
      "Iteration 0; 187/469 \tMinibatch Loss 0.141  Accuracy 96%\n",
      "Iteration 0; 188/469 \tMinibatch Loss 0.229  Accuracy 92%\n",
      "Iteration 0; 189/469 \tMinibatch Loss 0.090  Accuracy 98%\n",
      "Iteration 0; 190/469 \tMinibatch Loss 0.115  Accuracy 97%\n",
      "Iteration 0; 191/469 \tMinibatch Loss 0.112  Accuracy 96%\n",
      "Iteration 0; 192/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 0; 193/469 \tMinibatch Loss 0.098  Accuracy 96%\n",
      "Iteration 0; 194/469 \tMinibatch Loss 0.117  Accuracy 96%\n",
      "Iteration 0; 195/469 \tMinibatch Loss 0.079  Accuracy 98%\n",
      "Iteration 0; 196/469 \tMinibatch Loss 0.172  Accuracy 94%\n",
      "Iteration 0; 197/469 \tMinibatch Loss 0.160  Accuracy 97%\n",
      "Iteration 0; 198/469 \tMinibatch Loss 0.237  Accuracy 92%\n",
      "Iteration 0; 199/469 \tMinibatch Loss 0.121  Accuracy 95%\n",
      "Iteration 0; 200/469 \tMinibatch Loss 0.120  Accuracy 95%\n",
      "Iteration 0; 201/469 \tMinibatch Loss 0.192  Accuracy 92%\n",
      "Iteration 0; 202/469 \tMinibatch Loss 0.177  Accuracy 95%\n",
      "Iteration 0; 203/469 \tMinibatch Loss 0.046  Accuracy 99%\n",
      "Iteration 0; 204/469 \tMinibatch Loss 0.137  Accuracy 97%\n",
      "Iteration 0; 205/469 \tMinibatch Loss 0.128  Accuracy 96%\n",
      "Iteration 0; 206/469 \tMinibatch Loss 0.234  Accuracy 95%\n",
      "Iteration 0; 207/469 \tMinibatch Loss 0.092  Accuracy 98%\n",
      "Iteration 0; 208/469 \tMinibatch Loss 0.174  Accuracy 94%\n",
      "Iteration 0; 209/469 \tMinibatch Loss 0.064  Accuracy 99%\n",
      "Iteration 0; 210/469 \tMinibatch Loss 0.092  Accuracy 98%\n",
      "Iteration 0; 211/469 \tMinibatch Loss 0.090  Accuracy 98%\n",
      "Iteration 0; 212/469 \tMinibatch Loss 0.136  Accuracy 95%\n",
      "Iteration 0; 213/469 \tMinibatch Loss 0.274  Accuracy 91%\n",
      "Iteration 0; 214/469 \tMinibatch Loss 0.153  Accuracy 95%\n",
      "Iteration 0; 215/469 \tMinibatch Loss 0.131  Accuracy 97%\n",
      "Iteration 0; 216/469 \tMinibatch Loss 0.140  Accuracy 95%\n",
      "Iteration 0; 217/469 \tMinibatch Loss 0.098  Accuracy 98%\n",
      "Iteration 0; 218/469 \tMinibatch Loss 0.115  Accuracy 98%\n",
      "Iteration 0; 219/469 \tMinibatch Loss 0.167  Accuracy 95%\n",
      "Iteration 0; 220/469 \tMinibatch Loss 0.084  Accuracy 98%\n",
      "Iteration 0; 221/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 0; 222/469 \tMinibatch Loss 0.124  Accuracy 96%\n",
      "Iteration 0; 223/469 \tMinibatch Loss 0.109  Accuracy 96%\n",
      "Iteration 0; 224/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 0; 225/469 \tMinibatch Loss 0.073  Accuracy 97%\n",
      "Iteration 0; 226/469 \tMinibatch Loss 0.103  Accuracy 98%\n",
      "Iteration 0; 227/469 \tMinibatch Loss 0.112  Accuracy 96%\n",
      "Iteration 0; 228/469 \tMinibatch Loss 0.076  Accuracy 96%\n",
      "Iteration 0; 229/469 \tMinibatch Loss 0.148  Accuracy 96%\n",
      "Iteration 0; 230/469 \tMinibatch Loss 0.087  Accuracy 97%\n",
      "Iteration 0; 231/469 \tMinibatch Loss 0.099  Accuracy 96%\n",
      "Iteration 0; 232/469 \tMinibatch Loss 0.138  Accuracy 95%\n",
      "Iteration 0; 233/469 \tMinibatch Loss 0.163  Accuracy 95%\n",
      "Iteration 0; 234/469 \tMinibatch Loss 0.094  Accuracy 98%\n",
      "Iteration 0; 235/469 \tMinibatch Loss 0.179  Accuracy 93%\n",
      "Iteration 0; 236/469 \tMinibatch Loss 0.063  Accuracy 99%\n",
      "Iteration 0; 237/469 \tMinibatch Loss 0.112  Accuracy 96%\n",
      "Iteration 0; 238/469 \tMinibatch Loss 0.156  Accuracy 94%\n",
      "Iteration 0; 239/469 \tMinibatch Loss 0.111  Accuracy 95%\n",
      "Iteration 0; 240/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 0; 241/469 \tMinibatch Loss 0.105  Accuracy 98%\n",
      "Iteration 0; 242/469 \tMinibatch Loss 0.200  Accuracy 91%\n",
      "Iteration 0; 243/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 0; 244/469 \tMinibatch Loss 0.143  Accuracy 93%\n",
      "Iteration 0; 245/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 0; 246/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 0; 247/469 \tMinibatch Loss 0.077  Accuracy 99%\n",
      "Iteration 0; 248/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 0; 249/469 \tMinibatch Loss 0.136  Accuracy 96%\n",
      "Iteration 0; 250/469 \tMinibatch Loss 0.050  Accuracy 99%\n",
      "Iteration 0; 251/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 0; 252/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 0; 253/469 \tMinibatch Loss 0.073  Accuracy 97%\n",
      "Iteration 0; 254/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 0; 255/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 0; 256/469 \tMinibatch Loss 0.080  Accuracy 98%\n",
      "Iteration 0; 257/469 \tMinibatch Loss 0.045  Accuracy 99%\n",
      "Iteration 0; 258/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 0; 259/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 0; 260/469 \tMinibatch Loss 0.084  Accuracy 97%\n",
      "Iteration 0; 261/469 \tMinibatch Loss 0.072  Accuracy 96%\n",
      "Iteration 0; 262/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 0; 263/469 \tMinibatch Loss 0.131  Accuracy 95%\n",
      "Iteration 0; 264/469 \tMinibatch Loss 0.069  Accuracy 97%\n",
      "Iteration 0; 265/469 \tMinibatch Loss 0.045  Accuracy 99%\n",
      "Iteration 0; 266/469 \tMinibatch Loss 0.095  Accuracy 96%\n",
      "Iteration 0; 267/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 0; 268/469 \tMinibatch Loss 0.189  Accuracy 95%\n",
      "Iteration 0; 269/469 \tMinibatch Loss 0.113  Accuracy 95%\n",
      "Iteration 0; 270/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 0; 271/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 0; 272/469 \tMinibatch Loss 0.079  Accuracy 97%\n",
      "Iteration 0; 273/469 \tMinibatch Loss 0.087  Accuracy 97%\n",
      "Iteration 0; 274/469 \tMinibatch Loss 0.057  Accuracy 99%\n",
      "Iteration 0; 275/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 0; 276/469 \tMinibatch Loss 0.207  Accuracy 95%\n",
      "Iteration 0; 277/469 \tMinibatch Loss 0.050  Accuracy 99%\n",
      "Iteration 0; 278/469 \tMinibatch Loss 0.147  Accuracy 95%\n",
      "Iteration 0; 279/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 0; 280/469 \tMinibatch Loss 0.055  Accuracy 99%\n",
      "Iteration 0; 281/469 \tMinibatch Loss 0.190  Accuracy 92%\n",
      "Iteration 0; 282/469 \tMinibatch Loss 0.118  Accuracy 98%\n",
      "Iteration 0; 283/469 \tMinibatch Loss 0.083  Accuracy 98%\n",
      "Iteration 0; 284/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 0; 285/469 \tMinibatch Loss 0.054  Accuracy 99%\n",
      "Iteration 0; 286/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 0; 287/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 0; 288/469 \tMinibatch Loss 0.078  Accuracy 98%\n",
      "Iteration 0; 289/469 \tMinibatch Loss 0.055  Accuracy 99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0; 290/469 \tMinibatch Loss 0.109  Accuracy 95%\n",
      "Iteration 0; 291/469 \tMinibatch Loss 0.160  Accuracy 98%\n",
      "Iteration 0; 292/469 \tMinibatch Loss 0.134  Accuracy 97%\n",
      "Iteration 0; 293/469 \tMinibatch Loss 0.117  Accuracy 95%\n",
      "Iteration 0; 294/469 \tMinibatch Loss 0.076  Accuracy 97%\n",
      "Iteration 0; 295/469 \tMinibatch Loss 0.133  Accuracy 96%\n",
      "Iteration 0; 296/469 \tMinibatch Loss 0.073  Accuracy 97%\n",
      "Iteration 0; 297/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 0; 298/469 \tMinibatch Loss 0.078  Accuracy 97%\n",
      "Iteration 0; 299/469 \tMinibatch Loss 0.142  Accuracy 98%\n",
      "Iteration 0; 300/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 0; 301/469 \tMinibatch Loss 0.045  Accuracy 99%\n",
      "Iteration 0; 302/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 0; 303/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 0; 304/469 \tMinibatch Loss 0.098  Accuracy 95%\n",
      "Iteration 0; 305/469 \tMinibatch Loss 0.079  Accuracy 98%\n",
      "Iteration 0; 306/469 \tMinibatch Loss 0.123  Accuracy 97%\n",
      "Iteration 0; 307/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 0; 308/469 \tMinibatch Loss 0.106  Accuracy 96%\n",
      "Iteration 0; 309/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 0; 310/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 0; 311/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 0; 312/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 0; 313/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 0; 314/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 0; 315/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 0; 316/469 \tMinibatch Loss 0.064  Accuracy 97%\n",
      "Iteration 0; 317/469 \tMinibatch Loss 0.175  Accuracy 97%\n",
      "Iteration 0; 318/469 \tMinibatch Loss 0.116  Accuracy 98%\n",
      "Iteration 0; 319/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 0; 320/469 \tMinibatch Loss 0.149  Accuracy 95%\n",
      "Iteration 0; 321/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 0; 322/469 \tMinibatch Loss 0.078  Accuracy 97%\n",
      "Iteration 0; 323/469 \tMinibatch Loss 0.080  Accuracy 98%\n",
      "Iteration 0; 324/469 \tMinibatch Loss 0.028  Accuracy 100%\n",
      "Iteration 0; 325/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 0; 326/469 \tMinibatch Loss 0.039  Accuracy 99%\n",
      "Iteration 0; 327/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 0; 328/469 \tMinibatch Loss 0.064  Accuracy 97%\n",
      "Iteration 0; 329/469 \tMinibatch Loss 0.126  Accuracy 97%\n",
      "Iteration 0; 330/469 \tMinibatch Loss 0.108  Accuracy 98%\n",
      "Iteration 0; 331/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 0; 332/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 0; 333/469 \tMinibatch Loss 0.033  Accuracy 100%\n",
      "Iteration 0; 334/469 \tMinibatch Loss 0.087  Accuracy 96%\n",
      "Iteration 0; 335/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 0; 336/469 \tMinibatch Loss 0.041  Accuracy 100%\n",
      "Iteration 0; 337/469 \tMinibatch Loss 0.081  Accuracy 98%\n",
      "Iteration 0; 338/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 0; 339/469 \tMinibatch Loss 0.077  Accuracy 98%\n",
      "Iteration 0; 340/469 \tMinibatch Loss 0.076  Accuracy 97%\n",
      "Iteration 0; 341/469 \tMinibatch Loss 0.096  Accuracy 98%\n",
      "Iteration 0; 342/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 0; 343/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 0; 344/469 \tMinibatch Loss 0.090  Accuracy 98%\n",
      "Iteration 0; 345/469 \tMinibatch Loss 0.086  Accuracy 98%\n",
      "Iteration 0; 346/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 0; 347/469 \tMinibatch Loss 0.057  Accuracy 99%\n",
      "Iteration 0; 348/469 \tMinibatch Loss 0.025  Accuracy 100%\n",
      "Iteration 0; 349/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 0; 350/469 \tMinibatch Loss 0.075  Accuracy 96%\n",
      "Iteration 0; 351/469 \tMinibatch Loss 0.103  Accuracy 98%\n",
      "Iteration 0; 352/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 0; 353/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 0; 354/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 0; 355/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 0; 356/469 \tMinibatch Loss 0.174  Accuracy 97%\n",
      "Iteration 0; 357/469 \tMinibatch Loss 0.156  Accuracy 95%\n",
      "Iteration 0; 358/469 \tMinibatch Loss 0.106  Accuracy 97%\n",
      "Iteration 0; 359/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 0; 360/469 \tMinibatch Loss 0.120  Accuracy 96%\n",
      "Iteration 0; 361/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 0; 362/469 \tMinibatch Loss 0.033  Accuracy 100%\n",
      "Iteration 0; 363/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 0; 364/469 \tMinibatch Loss 0.076  Accuracy 98%\n",
      "Iteration 0; 365/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 0; 366/469 \tMinibatch Loss 0.091  Accuracy 98%\n",
      "Iteration 0; 367/469 \tMinibatch Loss 0.074  Accuracy 98%\n",
      "Iteration 0; 368/469 \tMinibatch Loss 0.183  Accuracy 95%\n",
      "Iteration 0; 369/469 \tMinibatch Loss 0.079  Accuracy 98%\n",
      "Iteration 0; 370/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 0; 371/469 \tMinibatch Loss 0.111  Accuracy 96%\n",
      "Iteration 0; 372/469 \tMinibatch Loss 0.088  Accuracy 97%\n",
      "Iteration 0; 373/469 \tMinibatch Loss 0.122  Accuracy 96%\n",
      "Iteration 0; 374/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 0; 375/469 \tMinibatch Loss 0.068  Accuracy 97%\n",
      "Iteration 0; 376/469 \tMinibatch Loss 0.192  Accuracy 96%\n",
      "Iteration 0; 377/469 \tMinibatch Loss 0.119  Accuracy 97%\n",
      "Iteration 0; 378/469 \tMinibatch Loss 0.083  Accuracy 98%\n",
      "Iteration 0; 379/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 0; 380/469 \tMinibatch Loss 0.096  Accuracy 95%\n",
      "Iteration 0; 381/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 0; 382/469 \tMinibatch Loss 0.094  Accuracy 98%\n",
      "Iteration 0; 383/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 0; 384/469 \tMinibatch Loss 0.109  Accuracy 97%\n",
      "Iteration 0; 385/469 \tMinibatch Loss 0.149  Accuracy 95%\n",
      "Iteration 0; 386/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 0; 387/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 0; 388/469 \tMinibatch Loss 0.084  Accuracy 98%\n",
      "Iteration 0; 389/469 \tMinibatch Loss 0.065  Accuracy 97%\n",
      "Iteration 0; 390/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 0; 391/469 \tMinibatch Loss 0.091  Accuracy 96%\n",
      "Iteration 0; 392/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 0; 393/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 0; 394/469 \tMinibatch Loss 0.055  Accuracy 99%\n",
      "Iteration 0; 395/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 0; 396/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 0; 397/469 \tMinibatch Loss 0.119  Accuracy 97%\n",
      "Iteration 0; 398/469 \tMinibatch Loss 0.082  Accuracy 98%\n",
      "Iteration 0; 399/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 0; 400/469 \tMinibatch Loss 0.099  Accuracy 96%\n",
      "Iteration 0; 401/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 0; 402/469 \tMinibatch Loss 0.028  Accuracy 100%\n",
      "Iteration 0; 403/469 \tMinibatch Loss 0.076  Accuracy 97%\n",
      "Iteration 0; 404/469 \tMinibatch Loss 0.102  Accuracy 97%\n",
      "Iteration 0; 405/469 \tMinibatch Loss 0.136  Accuracy 96%\n",
      "Iteration 0; 406/469 \tMinibatch Loss 0.098  Accuracy 96%\n",
      "Iteration 0; 407/469 \tMinibatch Loss 0.078  Accuracy 98%\n",
      "Iteration 0; 408/469 \tMinibatch Loss 0.111  Accuracy 96%\n",
      "Iteration 0; 409/469 \tMinibatch Loss 0.093  Accuracy 98%\n",
      "Iteration 0; 410/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 0; 411/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 0; 412/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 0; 413/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 0; 414/469 \tMinibatch Loss 0.050  Accuracy 99%\n",
      "Iteration 0; 415/469 \tMinibatch Loss 0.111  Accuracy 97%\n",
      "Iteration 0; 416/469 \tMinibatch Loss 0.105  Accuracy 97%\n",
      "Iteration 0; 417/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 0; 418/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 0; 419/469 \tMinibatch Loss 0.058  Accuracy 99%\n",
      "Iteration 0; 420/469 \tMinibatch Loss 0.116  Accuracy 97%\n",
      "Iteration 0; 421/469 \tMinibatch Loss 0.077  Accuracy 97%\n",
      "Iteration 0; 422/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 0; 423/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 0; 424/469 \tMinibatch Loss 0.043  Accuracy 99%\n",
      "Iteration 0; 425/469 \tMinibatch Loss 0.162  Accuracy 96%\n",
      "Iteration 0; 426/469 \tMinibatch Loss 0.131  Accuracy 97%\n",
      "Iteration 0; 427/469 \tMinibatch Loss 0.047  Accuracy 99%\n",
      "Iteration 0; 428/469 \tMinibatch Loss 0.076  Accuracy 98%\n",
      "Iteration 0; 429/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 0; 430/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 0; 431/469 \tMinibatch Loss 0.111  Accuracy 97%\n",
      "Iteration 0; 432/469 \tMinibatch Loss 0.079  Accuracy 98%\n",
      "Iteration 0; 433/469 \tMinibatch Loss 0.082  Accuracy 98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0; 434/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 0; 435/469 \tMinibatch Loss 0.066  Accuracy 99%\n",
      "Iteration 0; 436/469 \tMinibatch Loss 0.108  Accuracy 97%\n",
      "Iteration 0; 437/469 \tMinibatch Loss 0.077  Accuracy 95%\n",
      "Iteration 0; 438/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 0; 439/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 0; 440/469 \tMinibatch Loss 0.159  Accuracy 98%\n",
      "Iteration 0; 441/469 \tMinibatch Loss 0.091  Accuracy 98%\n",
      "Iteration 0; 442/469 \tMinibatch Loss 0.085  Accuracy 98%\n",
      "Iteration 0; 443/469 \tMinibatch Loss 0.177  Accuracy 95%\n",
      "Iteration 0; 444/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 0; 445/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 0; 446/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 0; 447/469 \tMinibatch Loss 0.066  Accuracy 98%\n",
      "Iteration 0; 448/469 \tMinibatch Loss 0.058  Accuracy 99%\n",
      "Iteration 0; 449/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 0; 450/469 \tMinibatch Loss 0.078  Accuracy 99%\n",
      "Iteration 0; 451/469 \tMinibatch Loss 0.023  Accuracy 100%\n",
      "Iteration 0; 452/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 0; 453/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 0; 454/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 0; 455/469 \tMinibatch Loss 0.059  Accuracy 97%\n",
      "Iteration 0; 456/469 \tMinibatch Loss 0.078  Accuracy 98%\n",
      "Iteration 0; 457/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 0; 458/469 \tMinibatch Loss 0.074  Accuracy 96%\n",
      "Iteration 0; 459/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 0; 460/469 \tMinibatch Loss 0.177  Accuracy 98%\n",
      "Iteration 0; 461/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 0; 462/469 \tMinibatch Loss 0.090  Accuracy 98%\n",
      "Iteration 0; 463/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 0; 464/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 0; 465/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 0; 466/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 0; 467/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 0; 468/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 1; 0/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 1; 1/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 1; 2/469 \tMinibatch Loss 0.056  Accuracy 97%\n",
      "Iteration 1; 3/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 1; 4/469 \tMinibatch Loss 0.024  Accuracy 100%\n",
      "Iteration 1; 5/469 \tMinibatch Loss 0.100  Accuracy 98%\n",
      "Iteration 1; 6/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 1; 7/469 \tMinibatch Loss 0.066  Accuracy 98%\n",
      "Iteration 1; 8/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 1; 9/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 1; 10/469 \tMinibatch Loss 0.101  Accuracy 97%\n",
      "Iteration 1; 11/469 \tMinibatch Loss 0.086  Accuracy 97%\n",
      "Iteration 1; 12/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 1; 13/469 \tMinibatch Loss 0.082  Accuracy 98%\n",
      "Iteration 1; 14/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 1; 15/469 \tMinibatch Loss 0.083  Accuracy 98%\n",
      "Iteration 1; 16/469 \tMinibatch Loss 0.045  Accuracy 99%\n",
      "Iteration 1; 17/469 \tMinibatch Loss 0.155  Accuracy 97%\n",
      "Iteration 1; 18/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 1; 19/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 1; 20/469 \tMinibatch Loss 0.094  Accuracy 98%\n",
      "Iteration 1; 21/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 1; 22/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 1; 23/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 1; 24/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 1; 25/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 1; 26/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 1; 27/469 \tMinibatch Loss 0.094  Accuracy 98%\n",
      "Iteration 1; 28/469 \tMinibatch Loss 0.096  Accuracy 95%\n",
      "Iteration 1; 29/469 \tMinibatch Loss 0.040  Accuracy 99%\n",
      "Iteration 1; 30/469 \tMinibatch Loss 0.125  Accuracy 97%\n",
      "Iteration 1; 31/469 \tMinibatch Loss 0.025  Accuracy 100%\n",
      "Iteration 1; 32/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 1; 33/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 1; 34/469 \tMinibatch Loss 0.099  Accuracy 95%\n",
      "Iteration 1; 35/469 \tMinibatch Loss 0.094  Accuracy 97%\n",
      "Iteration 1; 36/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 1; 37/469 \tMinibatch Loss 0.136  Accuracy 95%\n",
      "Iteration 1; 38/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 1; 39/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 1; 40/469 \tMinibatch Loss 0.028  Accuracy 98%\n",
      "Iteration 1; 41/469 \tMinibatch Loss 0.126  Accuracy 97%\n",
      "Iteration 1; 42/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 1; 43/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 1; 44/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 1; 45/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 1; 46/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 1; 47/469 \tMinibatch Loss 0.079  Accuracy 96%\n",
      "Iteration 1; 48/469 \tMinibatch Loss 0.095  Accuracy 98%\n",
      "Iteration 1; 49/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 1; 50/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 1; 51/469 \tMinibatch Loss 0.060  Accuracy 99%\n",
      "Iteration 1; 52/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 1; 53/469 \tMinibatch Loss 0.025  Accuracy 100%\n",
      "Iteration 1; 54/469 \tMinibatch Loss 0.026  Accuracy 100%\n",
      "Iteration 1; 55/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 1; 56/469 \tMinibatch Loss 0.077  Accuracy 98%\n",
      "Iteration 1; 57/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 1; 58/469 \tMinibatch Loss 0.171  Accuracy 98%\n",
      "Iteration 1; 59/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 1; 60/469 \tMinibatch Loss 0.113  Accuracy 96%\n",
      "Iteration 1; 61/469 \tMinibatch Loss 0.130  Accuracy 96%\n",
      "Iteration 1; 62/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 1; 63/469 \tMinibatch Loss 0.105  Accuracy 98%\n",
      "Iteration 1; 64/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 1; 65/469 \tMinibatch Loss 0.123  Accuracy 97%\n",
      "Iteration 1; 66/469 \tMinibatch Loss 0.094  Accuracy 96%\n",
      "Iteration 1; 67/469 \tMinibatch Loss 0.078  Accuracy 97%\n",
      "Iteration 1; 68/469 \tMinibatch Loss 0.039  Accuracy 99%\n",
      "Iteration 1; 69/469 \tMinibatch Loss 0.063  Accuracy 97%\n",
      "Iteration 1; 70/469 \tMinibatch Loss 0.131  Accuracy 96%\n",
      "Iteration 1; 71/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 1; 72/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 1; 73/469 \tMinibatch Loss 0.097  Accuracy 96%\n",
      "Iteration 1; 74/469 \tMinibatch Loss 0.066  Accuracy 98%\n",
      "Iteration 1; 75/469 \tMinibatch Loss 0.048  Accuracy 99%\n",
      "Iteration 1; 76/469 \tMinibatch Loss 0.051  Accuracy 99%\n",
      "Iteration 1; 77/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 1; 78/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 1; 79/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 1; 80/469 \tMinibatch Loss 0.108  Accuracy 98%\n",
      "Iteration 1; 81/469 \tMinibatch Loss 0.083  Accuracy 98%\n",
      "Iteration 1; 82/469 \tMinibatch Loss 0.111  Accuracy 96%\n",
      "Iteration 1; 83/469 \tMinibatch Loss 0.104  Accuracy 96%\n",
      "Iteration 1; 84/469 \tMinibatch Loss 0.098  Accuracy 97%\n",
      "Iteration 1; 85/469 \tMinibatch Loss 0.034  Accuracy 100%\n",
      "Iteration 1; 86/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 1; 87/469 \tMinibatch Loss 0.081  Accuracy 97%\n",
      "Iteration 1; 88/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 1; 89/469 \tMinibatch Loss 0.075  Accuracy 98%\n",
      "Iteration 1; 90/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 1; 91/469 \tMinibatch Loss 0.079  Accuracy 98%\n",
      "Iteration 1; 92/469 \tMinibatch Loss 0.142  Accuracy 97%\n",
      "Iteration 1; 93/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 1; 94/469 \tMinibatch Loss 0.085  Accuracy 98%\n",
      "Iteration 1; 95/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 1; 96/469 \tMinibatch Loss 0.070  Accuracy 97%\n",
      "Iteration 1; 97/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 1; 98/469 \tMinibatch Loss 0.064  Accuracy 97%\n",
      "Iteration 1; 99/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 1; 100/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 1; 101/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 1; 102/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 1; 103/469 \tMinibatch Loss 0.110  Accuracy 98%\n",
      "Iteration 1; 104/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 1; 105/469 \tMinibatch Loss 0.111  Accuracy 98%\n",
      "Iteration 1; 106/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 1; 107/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 1; 108/469 \tMinibatch Loss 0.045  Accuracy 99%\n",
      "Iteration 1; 109/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 1; 110/469 \tMinibatch Loss 0.117  Accuracy 96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1; 111/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 1; 112/469 \tMinibatch Loss 0.087  Accuracy 97%\n",
      "Iteration 1; 113/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 1; 114/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 1; 115/469 \tMinibatch Loss 0.046  Accuracy 99%\n",
      "Iteration 1; 116/469 \tMinibatch Loss 0.089  Accuracy 98%\n",
      "Iteration 1; 117/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 1; 118/469 \tMinibatch Loss 0.039  Accuracy 99%\n",
      "Iteration 1; 119/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 1; 120/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 1; 121/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 1; 122/469 \tMinibatch Loss 0.096  Accuracy 96%\n",
      "Iteration 1; 123/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 1; 124/469 \tMinibatch Loss 0.026  Accuracy 98%\n",
      "Iteration 1; 125/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 1; 126/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 1; 127/469 \tMinibatch Loss 0.083  Accuracy 98%\n",
      "Iteration 1; 128/469 \tMinibatch Loss 0.168  Accuracy 96%\n",
      "Iteration 1; 129/469 \tMinibatch Loss 0.140  Accuracy 97%\n",
      "Iteration 1; 130/469 \tMinibatch Loss 0.098  Accuracy 97%\n",
      "Iteration 1; 131/469 \tMinibatch Loss 0.106  Accuracy 97%\n",
      "Iteration 1; 132/469 \tMinibatch Loss 0.086  Accuracy 98%\n",
      "Iteration 1; 133/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 1; 134/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 1; 135/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 1; 136/469 \tMinibatch Loss 0.095  Accuracy 98%\n",
      "Iteration 1; 137/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 1; 138/469 \tMinibatch Loss 0.139  Accuracy 96%\n",
      "Iteration 1; 139/469 \tMinibatch Loss 0.129  Accuracy 95%\n",
      "Iteration 1; 140/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 1; 141/469 \tMinibatch Loss 0.056  Accuracy 99%\n",
      "Iteration 1; 142/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 1; 143/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 1; 144/469 \tMinibatch Loss 0.082  Accuracy 97%\n",
      "Iteration 1; 145/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 1; 146/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 1; 147/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 1; 148/469 \tMinibatch Loss 0.066  Accuracy 98%\n",
      "Iteration 1; 149/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 1; 150/469 \tMinibatch Loss 0.071  Accuracy 99%\n",
      "Iteration 1; 151/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 1; 152/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 1; 153/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 1; 154/469 \tMinibatch Loss 0.076  Accuracy 98%\n",
      "Iteration 1; 155/469 \tMinibatch Loss 0.121  Accuracy 96%\n",
      "Iteration 1; 156/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 1; 157/469 \tMinibatch Loss 0.091  Accuracy 97%\n",
      "Iteration 1; 158/469 \tMinibatch Loss 0.109  Accuracy 97%\n",
      "Iteration 1; 159/469 \tMinibatch Loss 0.080  Accuracy 98%\n",
      "Iteration 1; 160/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 1; 161/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 1; 162/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 1; 163/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 1; 164/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 1; 165/469 \tMinibatch Loss 0.084  Accuracy 97%\n",
      "Iteration 1; 166/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 1; 167/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 1; 168/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 1; 169/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 1; 170/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 1; 171/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 1; 172/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 1; 173/469 \tMinibatch Loss 0.026  Accuracy 98%\n",
      "Iteration 1; 174/469 \tMinibatch Loss 0.073  Accuracy 97%\n",
      "Iteration 1; 175/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 1; 176/469 \tMinibatch Loss 0.092  Accuracy 98%\n",
      "Iteration 1; 177/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 1; 178/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 1; 179/469 \tMinibatch Loss 0.097  Accuracy 97%\n",
      "Iteration 1; 180/469 \tMinibatch Loss 0.080  Accuracy 97%\n",
      "Iteration 1; 181/469 \tMinibatch Loss 0.026  Accuracy 100%\n",
      "Iteration 1; 182/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 1; 183/469 \tMinibatch Loss 0.046  Accuracy 97%\n",
      "Iteration 1; 184/469 \tMinibatch Loss 0.071  Accuracy 97%\n",
      "Iteration 1; 185/469 \tMinibatch Loss 0.032  Accuracy 100%\n",
      "Iteration 1; 186/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 1; 187/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 1; 188/469 \tMinibatch Loss 0.024  Accuracy 98%\n",
      "Iteration 1; 189/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 1; 190/469 \tMinibatch Loss 0.105  Accuracy 95%\n",
      "Iteration 1; 191/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 1; 192/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 1; 193/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 1; 194/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 1; 195/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 1; 196/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 1; 197/469 \tMinibatch Loss 0.107  Accuracy 98%\n",
      "Iteration 1; 198/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 1; 199/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 1; 200/469 \tMinibatch Loss 0.164  Accuracy 95%\n",
      "Iteration 1; 201/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 1; 202/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 1; 203/469 \tMinibatch Loss 0.048  Accuracy 97%\n",
      "Iteration 1; 204/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 1; 205/469 \tMinibatch Loss 0.075  Accuracy 98%\n",
      "Iteration 1; 206/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 1; 207/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 1; 208/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 1; 209/469 \tMinibatch Loss 0.107  Accuracy 97%\n",
      "Iteration 1; 210/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 1; 211/469 \tMinibatch Loss 0.028  Accuracy 100%\n",
      "Iteration 1; 212/469 \tMinibatch Loss 0.103  Accuracy 98%\n",
      "Iteration 1; 213/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 1; 214/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 1; 215/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 1; 216/469 \tMinibatch Loss 0.064  Accuracy 97%\n",
      "Iteration 1; 217/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 1; 218/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 1; 219/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 1; 220/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 1; 221/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 1; 222/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 1; 223/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 1; 224/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 1; 225/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 1; 226/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 1; 227/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 1; 228/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 1; 229/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 1; 230/469 \tMinibatch Loss 0.117  Accuracy 97%\n",
      "Iteration 1; 231/469 \tMinibatch Loss 0.097  Accuracy 98%\n",
      "Iteration 1; 232/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 1; 233/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 1; 234/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 1; 235/469 \tMinibatch Loss 0.076  Accuracy 99%\n",
      "Iteration 1; 236/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 1; 237/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 1; 238/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 1; 239/469 \tMinibatch Loss 0.070  Accuracy 96%\n",
      "Iteration 1; 240/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 1; 241/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 1; 242/469 \tMinibatch Loss 0.050  Accuracy 99%\n",
      "Iteration 1; 243/469 \tMinibatch Loss 0.046  Accuracy 99%\n",
      "Iteration 1; 244/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 1; 245/469 \tMinibatch Loss 0.084  Accuracy 97%\n",
      "Iteration 1; 246/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 1; 247/469 \tMinibatch Loss 0.025  Accuracy 98%\n",
      "Iteration 1; 248/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 1; 249/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 1; 250/469 \tMinibatch Loss 0.040  Accuracy 99%\n",
      "Iteration 1; 251/469 \tMinibatch Loss 0.079  Accuracy 96%\n",
      "Iteration 1; 252/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 1; 253/469 \tMinibatch Loss 0.091  Accuracy 98%\n",
      "Iteration 1; 254/469 \tMinibatch Loss 0.060  Accuracy 97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1; 255/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 1; 256/469 \tMinibatch Loss 0.116  Accuracy 96%\n",
      "Iteration 1; 257/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 1; 258/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 1; 259/469 \tMinibatch Loss 0.095  Accuracy 97%\n",
      "Iteration 1; 260/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 1; 261/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 1; 262/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 1; 263/469 \tMinibatch Loss 0.078  Accuracy 96%\n",
      "Iteration 1; 264/469 \tMinibatch Loss 0.136  Accuracy 98%\n",
      "Iteration 1; 265/469 \tMinibatch Loss 0.136  Accuracy 97%\n",
      "Iteration 1; 266/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 1; 267/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 1; 268/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 1; 269/469 \tMinibatch Loss 0.082  Accuracy 98%\n",
      "Iteration 1; 270/469 \tMinibatch Loss 0.074  Accuracy 98%\n",
      "Iteration 1; 271/469 \tMinibatch Loss 0.092  Accuracy 97%\n",
      "Iteration 1; 272/469 \tMinibatch Loss 0.053  Accuracy 99%\n",
      "Iteration 1; 273/469 \tMinibatch Loss 0.060  Accuracy 97%\n",
      "Iteration 1; 274/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 1; 275/469 \tMinibatch Loss 0.075  Accuracy 97%\n",
      "Iteration 1; 276/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 1; 277/469 \tMinibatch Loss 0.075  Accuracy 97%\n",
      "Iteration 1; 278/469 \tMinibatch Loss 0.063  Accuracy 99%\n",
      "Iteration 1; 279/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 1; 280/469 \tMinibatch Loss 0.074  Accuracy 98%\n",
      "Iteration 1; 281/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 1; 282/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 1; 283/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 1; 284/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 1; 285/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 1; 286/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 1; 287/469 \tMinibatch Loss 0.075  Accuracy 98%\n",
      "Iteration 1; 288/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 1; 289/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 1; 290/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 1; 291/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 1; 292/469 \tMinibatch Loss 0.080  Accuracy 97%\n",
      "Iteration 1; 293/469 \tMinibatch Loss 0.093  Accuracy 98%\n",
      "Iteration 1; 294/469 \tMinibatch Loss 0.133  Accuracy 95%\n",
      "Iteration 1; 295/469 \tMinibatch Loss 0.047  Accuracy 99%\n",
      "Iteration 1; 296/469 \tMinibatch Loss 0.076  Accuracy 98%\n",
      "Iteration 1; 297/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 1; 298/469 \tMinibatch Loss 0.137  Accuracy 97%\n",
      "Iteration 1; 299/469 \tMinibatch Loss 0.077  Accuracy 98%\n",
      "Iteration 1; 300/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 1; 301/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 1; 302/469 \tMinibatch Loss 0.040  Accuracy 99%\n",
      "Iteration 1; 303/469 \tMinibatch Loss 0.074  Accuracy 97%\n",
      "Iteration 1; 304/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 1; 305/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 1; 306/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 1; 307/469 \tMinibatch Loss 0.116  Accuracy 96%\n",
      "Iteration 1; 308/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 1; 309/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 1; 310/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 1; 311/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 1; 312/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 1; 313/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 1; 314/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 1; 315/469 \tMinibatch Loss 0.118  Accuracy 98%\n",
      "Iteration 1; 316/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 1; 317/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 1; 318/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 1; 319/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 1; 320/469 \tMinibatch Loss 0.053  Accuracy 97%\n",
      "Iteration 1; 321/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 1; 322/469 \tMinibatch Loss 0.076  Accuracy 98%\n",
      "Iteration 1; 323/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 1; 324/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 1; 325/469 \tMinibatch Loss 0.105  Accuracy 98%\n",
      "Iteration 1; 326/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 1; 327/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 1; 328/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 1; 329/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 1; 330/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 1; 331/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 1; 332/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 1; 333/469 \tMinibatch Loss 0.058  Accuracy 99%\n",
      "Iteration 1; 334/469 \tMinibatch Loss 0.122  Accuracy 98%\n",
      "Iteration 1; 335/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 1; 336/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 1; 337/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 1; 338/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 1; 339/469 \tMinibatch Loss 0.092  Accuracy 97%\n",
      "Iteration 1; 340/469 \tMinibatch Loss 0.098  Accuracy 96%\n",
      "Iteration 1; 341/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 1; 342/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 1; 343/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 1; 344/469 \tMinibatch Loss 0.079  Accuracy 98%\n",
      "Iteration 1; 345/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 1; 346/469 \tMinibatch Loss 0.056  Accuracy 99%\n",
      "Iteration 1; 347/469 \tMinibatch Loss 0.066  Accuracy 98%\n",
      "Iteration 1; 348/469 \tMinibatch Loss 0.058  Accuracy 99%\n",
      "Iteration 1; 349/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 1; 350/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 1; 351/469 \tMinibatch Loss 0.115  Accuracy 96%\n",
      "Iteration 1; 352/469 \tMinibatch Loss 0.074  Accuracy 98%\n",
      "Iteration 1; 353/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 1; 354/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 1; 355/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 1; 356/469 \tMinibatch Loss 0.050  Accuracy 97%\n",
      "Iteration 1; 357/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 1; 358/469 \tMinibatch Loss 0.064  Accuracy 97%\n",
      "Iteration 1; 359/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 1; 360/469 \tMinibatch Loss 0.078  Accuracy 98%\n",
      "Iteration 1; 361/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 1; 362/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 1; 363/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 1; 364/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 1; 365/469 \tMinibatch Loss 0.088  Accuracy 98%\n",
      "Iteration 1; 366/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 1; 367/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 1; 368/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 1; 369/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 1; 370/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 1; 371/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 1; 372/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 1; 373/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 1; 374/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 1; 375/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 1; 376/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 1; 377/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 1; 378/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 1; 379/469 \tMinibatch Loss 0.093  Accuracy 98%\n",
      "Iteration 1; 380/469 \tMinibatch Loss 0.067  Accuracy 97%\n",
      "Iteration 1; 381/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 1; 382/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 1; 383/469 \tMinibatch Loss 0.040  Accuracy 99%\n",
      "Iteration 1; 384/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 1; 385/469 \tMinibatch Loss 0.081  Accuracy 98%\n",
      "Iteration 1; 386/469 \tMinibatch Loss 0.096  Accuracy 96%\n",
      "Iteration 1; 387/469 \tMinibatch Loss 0.023  Accuracy 100%\n",
      "Iteration 1; 388/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 1; 389/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 1; 390/469 \tMinibatch Loss 0.105  Accuracy 95%\n",
      "Iteration 1; 391/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 1; 392/469 \tMinibatch Loss 0.129  Accuracy 97%\n",
      "Iteration 1; 393/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 1; 394/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 1; 395/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 1; 396/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 1; 397/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 1; 398/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 1; 399/469 \tMinibatch Loss 0.026  Accuracy 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1; 400/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 1; 401/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 1; 402/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 1; 403/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 1; 404/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 1; 405/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 1; 406/469 \tMinibatch Loss 0.063  Accuracy 99%\n",
      "Iteration 1; 407/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 1; 408/469 \tMinibatch Loss 0.026  Accuracy 98%\n",
      "Iteration 1; 409/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 1; 410/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 1; 411/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 1; 412/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 1; 413/469 \tMinibatch Loss 0.080  Accuracy 97%\n",
      "Iteration 1; 414/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 1; 415/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 1; 416/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 1; 417/469 \tMinibatch Loss 0.124  Accuracy 96%\n",
      "Iteration 1; 418/469 \tMinibatch Loss 0.185  Accuracy 98%\n",
      "Iteration 1; 419/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 1; 420/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 1; 421/469 \tMinibatch Loss 0.063  Accuracy 97%\n",
      "Iteration 1; 422/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 1; 423/469 \tMinibatch Loss 0.046  Accuracy 99%\n",
      "Iteration 1; 424/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 1; 425/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 1; 426/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 1; 427/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 1; 428/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 1; 429/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 1; 430/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 1; 431/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 1; 432/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 1; 433/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 1; 434/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 1; 435/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 1; 436/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 1; 437/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 1; 438/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 1; 439/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 1; 440/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 1; 441/469 \tMinibatch Loss 0.101  Accuracy 96%\n",
      "Iteration 1; 442/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 1; 443/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 1; 444/469 \tMinibatch Loss 0.023  Accuracy 100%\n",
      "Iteration 1; 445/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 1; 446/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 1; 447/469 \tMinibatch Loss 0.066  Accuracy 98%\n",
      "Iteration 1; 448/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 1; 449/469 \tMinibatch Loss 0.094  Accuracy 97%\n",
      "Iteration 1; 450/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 1; 451/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 1; 452/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 1; 453/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 1; 454/469 \tMinibatch Loss 0.039  Accuracy 99%\n",
      "Iteration 1; 455/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 1; 456/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 1; 457/469 \tMinibatch Loss 0.101  Accuracy 96%\n",
      "Iteration 1; 458/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 1; 459/469 \tMinibatch Loss 0.051  Accuracy 99%\n",
      "Iteration 1; 460/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 1; 461/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 1; 462/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 1; 463/469 \tMinibatch Loss 0.089  Accuracy 95%\n",
      "Iteration 1; 464/469 \tMinibatch Loss 0.082  Accuracy 99%\n",
      "Iteration 1; 465/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 1; 466/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 1; 467/469 \tMinibatch Loss 0.075  Accuracy 97%\n",
      "Iteration 1; 468/469 \tMinibatch Loss 0.088  Accuracy 96%\n",
      "Iteration 2; 0/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 2; 1/469 \tMinibatch Loss 0.028  Accuracy 98%\n",
      "Iteration 2; 2/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 2; 3/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 2; 4/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 2; 5/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 6/469 \tMinibatch Loss 0.072  Accuracy 97%\n",
      "Iteration 2; 7/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 2; 8/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 2; 9/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 2; 10/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 2; 11/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 2; 12/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 2; 13/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 2; 14/469 \tMinibatch Loss 0.066  Accuracy 98%\n",
      "Iteration 2; 15/469 \tMinibatch Loss 0.044  Accuracy 99%\n",
      "Iteration 2; 16/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 2; 17/469 \tMinibatch Loss 0.050  Accuracy 99%\n",
      "Iteration 2; 18/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 19/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 2; 20/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 21/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 2; 22/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 2; 23/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 2; 24/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 2; 25/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 26/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 2; 27/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 2; 28/469 \tMinibatch Loss 0.048  Accuracy 99%\n",
      "Iteration 2; 29/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 2; 30/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 2; 31/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 2; 32/469 \tMinibatch Loss 0.149  Accuracy 95%\n",
      "Iteration 2; 33/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 2; 34/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 2; 35/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 2; 36/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 2; 37/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 2; 38/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 2; 39/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 2; 40/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 2; 41/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 2; 42/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 2; 43/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 44/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 2; 45/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 2; 46/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 2; 47/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 2; 48/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 2; 49/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 2; 50/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 2; 51/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 2; 52/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 2; 53/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 2; 54/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 2; 55/469 \tMinibatch Loss 0.097  Accuracy 97%\n",
      "Iteration 2; 56/469 \tMinibatch Loss 0.051  Accuracy 99%\n",
      "Iteration 2; 57/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 2; 58/469 \tMinibatch Loss 0.074  Accuracy 98%\n",
      "Iteration 2; 59/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 60/469 \tMinibatch Loss 0.084  Accuracy 97%\n",
      "Iteration 2; 61/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 2; 62/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 2; 63/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 2; 64/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 2; 65/469 \tMinibatch Loss 0.024  Accuracy 100%\n",
      "Iteration 2; 66/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 2; 67/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 2; 68/469 \tMinibatch Loss 0.077  Accuracy 97%\n",
      "Iteration 2; 69/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 2; 70/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 2; 71/469 \tMinibatch Loss 0.083  Accuracy 98%\n",
      "Iteration 2; 72/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 2; 73/469 \tMinibatch Loss 0.115  Accuracy 95%\n",
      "Iteration 2; 74/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 2; 75/469 \tMinibatch Loss 0.042  Accuracy 98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2; 76/469 \tMinibatch Loss 0.088  Accuracy 98%\n",
      "Iteration 2; 77/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 2; 78/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 2; 79/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 2; 80/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 2; 81/469 \tMinibatch Loss 0.088  Accuracy 96%\n",
      "Iteration 2; 82/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 2; 83/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 2; 84/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 85/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 86/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 2; 87/469 \tMinibatch Loss 0.079  Accuracy 97%\n",
      "Iteration 2; 88/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 2; 89/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 2; 90/469 \tMinibatch Loss 0.024  Accuracy 98%\n",
      "Iteration 2; 91/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 2; 92/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 2; 93/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 2; 94/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 2; 95/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 2; 96/469 \tMinibatch Loss 0.083  Accuracy 98%\n",
      "Iteration 2; 97/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 2; 98/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 2; 99/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 2; 100/469 \tMinibatch Loss 0.105  Accuracy 97%\n",
      "Iteration 2; 101/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 102/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 2; 103/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 2; 104/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 2; 105/469 \tMinibatch Loss 0.100  Accuracy 97%\n",
      "Iteration 2; 106/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 2; 107/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 2; 108/469 \tMinibatch Loss 0.120  Accuracy 96%\n",
      "Iteration 2; 109/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 2; 110/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 2; 111/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 2; 112/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 2; 113/469 \tMinibatch Loss 0.074  Accuracy 99%\n",
      "Iteration 2; 114/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 2; 115/469 \tMinibatch Loss 0.094  Accuracy 98%\n",
      "Iteration 2; 116/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 2; 117/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 2; 118/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 2; 119/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 2; 120/469 \tMinibatch Loss 0.074  Accuracy 98%\n",
      "Iteration 2; 121/469 \tMinibatch Loss 0.066  Accuracy 98%\n",
      "Iteration 2; 122/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 2; 123/469 \tMinibatch Loss 0.033  Accuracy 99%\n",
      "Iteration 2; 124/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 2; 125/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 2; 126/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 2; 127/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 2; 128/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 2; 129/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 2; 130/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 2; 131/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 2; 132/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 2; 133/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 2; 134/469 \tMinibatch Loss 0.047  Accuracy 99%\n",
      "Iteration 2; 135/469 \tMinibatch Loss 0.075  Accuracy 97%\n",
      "Iteration 2; 136/469 \tMinibatch Loss 0.082  Accuracy 97%\n",
      "Iteration 2; 137/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 2; 138/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 2; 139/469 \tMinibatch Loss 0.052  Accuracy 99%\n",
      "Iteration 2; 140/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 2; 141/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 2; 142/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 2; 143/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 2; 144/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 2; 145/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 2; 146/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 2; 147/469 \tMinibatch Loss 0.138  Accuracy 96%\n",
      "Iteration 2; 148/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 2; 149/469 \tMinibatch Loss 0.033  Accuracy 99%\n",
      "Iteration 2; 150/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 2; 151/469 \tMinibatch Loss 0.012  Accuracy 99%\n",
      "Iteration 2; 152/469 \tMinibatch Loss 0.078  Accuracy 98%\n",
      "Iteration 2; 153/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 2; 154/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 2; 155/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 2; 156/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 2; 157/469 \tMinibatch Loss 0.023  Accuracy 98%\n",
      "Iteration 2; 158/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 2; 159/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 2; 160/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 161/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 2; 162/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 2; 163/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 2; 164/469 \tMinibatch Loss 0.060  Accuracy 97%\n",
      "Iteration 2; 165/469 \tMinibatch Loss 0.115  Accuracy 98%\n",
      "Iteration 2; 166/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 2; 167/469 \tMinibatch Loss 0.067  Accuracy 99%\n",
      "Iteration 2; 168/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 2; 169/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 2; 170/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 2; 171/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 2; 172/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 2; 173/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 2; 174/469 \tMinibatch Loss 0.055  Accuracy 99%\n",
      "Iteration 2; 175/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 2; 176/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 2; 177/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 2; 178/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 2; 179/469 \tMinibatch Loss 0.027  Accuracy 100%\n",
      "Iteration 2; 180/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 2; 181/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 2; 182/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 2; 183/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 2; 184/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 2; 185/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 186/469 \tMinibatch Loss 0.103  Accuracy 96%\n",
      "Iteration 2; 187/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 2; 188/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 2; 189/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 2; 190/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 2; 191/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 192/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 2; 193/469 \tMinibatch Loss 0.106  Accuracy 97%\n",
      "Iteration 2; 194/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 2; 195/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 2; 196/469 \tMinibatch Loss 0.106  Accuracy 98%\n",
      "Iteration 2; 197/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 2; 198/469 \tMinibatch Loss 0.026  Accuracy 98%\n",
      "Iteration 2; 199/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 200/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 2; 201/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 202/469 \tMinibatch Loss 0.136  Accuracy 95%\n",
      "Iteration 2; 203/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 2; 204/469 \tMinibatch Loss 0.103  Accuracy 96%\n",
      "Iteration 2; 205/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 2; 206/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 2; 207/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 2; 208/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 2; 209/469 \tMinibatch Loss 0.099  Accuracy 95%\n",
      "Iteration 2; 210/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 2; 211/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 2; 212/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 2; 213/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 2; 214/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 2; 215/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 2; 216/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 2; 217/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 2; 218/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 219/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 2; 220/469 \tMinibatch Loss 0.008  Accuracy 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2; 221/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 2; 222/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 2; 223/469 \tMinibatch Loss 0.045  Accuracy 97%\n",
      "Iteration 2; 224/469 \tMinibatch Loss 0.080  Accuracy 98%\n",
      "Iteration 2; 225/469 \tMinibatch Loss 0.056  Accuracy 97%\n",
      "Iteration 2; 226/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 2; 227/469 \tMinibatch Loss 0.079  Accuracy 98%\n",
      "Iteration 2; 228/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 2; 229/469 \tMinibatch Loss 0.083  Accuracy 99%\n",
      "Iteration 2; 230/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 2; 231/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 2; 232/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 2; 233/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 234/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 2; 235/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 2; 236/469 \tMinibatch Loss 0.079  Accuracy 97%\n",
      "Iteration 2; 237/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 2; 238/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 239/469 \tMinibatch Loss 0.154  Accuracy 95%\n",
      "Iteration 2; 240/469 \tMinibatch Loss 0.056  Accuracy 99%\n",
      "Iteration 2; 241/469 \tMinibatch Loss 0.084  Accuracy 98%\n",
      "Iteration 2; 242/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 2; 243/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 2; 244/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 2; 245/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 246/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 2; 247/469 \tMinibatch Loss 0.086  Accuracy 96%\n",
      "Iteration 2; 248/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 2; 249/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 2; 250/469 \tMinibatch Loss 0.099  Accuracy 97%\n",
      "Iteration 2; 251/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 2; 252/469 \tMinibatch Loss 0.044  Accuracy 97%\n",
      "Iteration 2; 253/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 2; 254/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 2; 255/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 2; 256/469 \tMinibatch Loss 0.028  Accuracy 100%\n",
      "Iteration 2; 257/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 2; 258/469 \tMinibatch Loss 0.098  Accuracy 97%\n",
      "Iteration 2; 259/469 \tMinibatch Loss 0.077  Accuracy 98%\n",
      "Iteration 2; 260/469 \tMinibatch Loss 0.077  Accuracy 98%\n",
      "Iteration 2; 261/469 \tMinibatch Loss 0.094  Accuracy 95%\n",
      "Iteration 2; 262/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 2; 263/469 \tMinibatch Loss 0.087  Accuracy 97%\n",
      "Iteration 2; 264/469 \tMinibatch Loss 0.113  Accuracy 95%\n",
      "Iteration 2; 265/469 \tMinibatch Loss 0.020  Accuracy 98%\n",
      "Iteration 2; 266/469 \tMinibatch Loss 0.090  Accuracy 98%\n",
      "Iteration 2; 267/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 2; 268/469 \tMinibatch Loss 0.182  Accuracy 99%\n",
      "Iteration 2; 269/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 2; 270/469 \tMinibatch Loss 0.095  Accuracy 98%\n",
      "Iteration 2; 271/469 \tMinibatch Loss 0.062  Accuracy 97%\n",
      "Iteration 2; 272/469 \tMinibatch Loss 0.082  Accuracy 97%\n",
      "Iteration 2; 273/469 \tMinibatch Loss 0.066  Accuracy 98%\n",
      "Iteration 2; 274/469 \tMinibatch Loss 0.091  Accuracy 98%\n",
      "Iteration 2; 275/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 2; 276/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 2; 277/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 2; 278/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 2; 279/469 \tMinibatch Loss 0.028  Accuracy 98%\n",
      "Iteration 2; 280/469 \tMinibatch Loss 0.106  Accuracy 98%\n",
      "Iteration 2; 281/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 2; 282/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 2; 283/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 2; 284/469 \tMinibatch Loss 0.014  Accuracy 99%\n",
      "Iteration 2; 285/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 2; 286/469 \tMinibatch Loss 0.066  Accuracy 98%\n",
      "Iteration 2; 287/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 2; 288/469 \tMinibatch Loss 0.023  Accuracy 98%\n",
      "Iteration 2; 289/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 2; 290/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 2; 291/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 2; 292/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 2; 293/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 2; 294/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 2; 295/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 2; 296/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 2; 297/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 2; 298/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 2; 299/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 2; 300/469 \tMinibatch Loss 0.100  Accuracy 97%\n",
      "Iteration 2; 301/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 2; 302/469 \tMinibatch Loss 0.053  Accuracy 97%\n",
      "Iteration 2; 303/469 \tMinibatch Loss 0.064  Accuracy 97%\n",
      "Iteration 2; 304/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 2; 305/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 2; 306/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 307/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 2; 308/469 \tMinibatch Loss 0.077  Accuracy 98%\n",
      "Iteration 2; 309/469 \tMinibatch Loss 0.053  Accuracy 99%\n",
      "Iteration 2; 310/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 2; 311/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 2; 312/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 2; 313/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 2; 314/469 \tMinibatch Loss 0.087  Accuracy 97%\n",
      "Iteration 2; 315/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 2; 316/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 2; 317/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 2; 318/469 \tMinibatch Loss 0.024  Accuracy 100%\n",
      "Iteration 2; 319/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 2; 320/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 2; 321/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 2; 322/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 2; 323/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 2; 324/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 2; 325/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 326/469 \tMinibatch Loss 0.053  Accuracy 97%\n",
      "Iteration 2; 327/469 \tMinibatch Loss 0.009  Accuracy 99%\n",
      "Iteration 2; 328/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 2; 329/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 2; 330/469 \tMinibatch Loss 0.052  Accuracy 99%\n",
      "Iteration 2; 331/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 2; 332/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 2; 333/469 \tMinibatch Loss 0.083  Accuracy 97%\n",
      "Iteration 2; 334/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 2; 335/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 2; 336/469 \tMinibatch Loss 0.027  Accuracy 100%\n",
      "Iteration 2; 337/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 2; 338/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 2; 339/469 \tMinibatch Loss 0.074  Accuracy 98%\n",
      "Iteration 2; 340/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 2; 341/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 342/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 2; 343/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 2; 344/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 2; 345/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 2; 346/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 2; 347/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 2; 348/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 2; 349/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 2; 350/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 2; 351/469 \tMinibatch Loss 0.002  Accuracy 100%\n",
      "Iteration 2; 352/469 \tMinibatch Loss 0.010  Accuracy 99%\n",
      "Iteration 2; 353/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 2; 354/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 2; 355/469 \tMinibatch Loss 0.100  Accuracy 97%\n",
      "Iteration 2; 356/469 \tMinibatch Loss 0.087  Accuracy 98%\n",
      "Iteration 2; 357/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 2; 358/469 \tMinibatch Loss 0.039  Accuracy 99%\n",
      "Iteration 2; 359/469 \tMinibatch Loss 0.056  Accuracy 99%\n",
      "Iteration 2; 360/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 2; 361/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 362/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 2; 363/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 2; 364/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 2; 365/469 \tMinibatch Loss 0.063  Accuracy 98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2; 366/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 2; 367/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 368/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 2; 369/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 2; 370/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 2; 371/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 2; 372/469 \tMinibatch Loss 0.105  Accuracy 98%\n",
      "Iteration 2; 373/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 2; 374/469 \tMinibatch Loss 0.112  Accuracy 96%\n",
      "Iteration 2; 375/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 2; 376/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 2; 377/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 2; 378/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 2; 379/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 2; 380/469 \tMinibatch Loss 0.142  Accuracy 97%\n",
      "Iteration 2; 381/469 \tMinibatch Loss 0.091  Accuracy 96%\n",
      "Iteration 2; 382/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 2; 383/469 \tMinibatch Loss 0.058  Accuracy 99%\n",
      "Iteration 2; 384/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 2; 385/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 2; 386/469 \tMinibatch Loss 0.050  Accuracy 99%\n",
      "Iteration 2; 387/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 2; 388/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 2; 389/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 2; 390/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 2; 391/469 \tMinibatch Loss 0.090  Accuracy 98%\n",
      "Iteration 2; 392/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 2; 393/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 2; 394/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 2; 395/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 2; 396/469 \tMinibatch Loss 0.077  Accuracy 98%\n",
      "Iteration 2; 397/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 2; 398/469 \tMinibatch Loss 0.040  Accuracy 99%\n",
      "Iteration 2; 399/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 2; 400/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 2; 401/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 2; 402/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 2; 403/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 404/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 2; 405/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 406/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 407/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 408/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 2; 409/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 410/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 2; 411/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 2; 412/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 413/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 414/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 2; 415/469 \tMinibatch Loss 0.014  Accuracy 99%\n",
      "Iteration 2; 416/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 2; 417/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 418/469 \tMinibatch Loss 0.014  Accuracy 99%\n",
      "Iteration 2; 419/469 \tMinibatch Loss 0.072  Accuracy 96%\n",
      "Iteration 2; 420/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 2; 421/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 2; 422/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 2; 423/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 2; 424/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 2; 425/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 2; 426/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 2; 427/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 2; 428/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 2; 429/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 2; 430/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 431/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 2; 432/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 2; 433/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 2; 434/469 \tMinibatch Loss 0.026  Accuracy 100%\n",
      "Iteration 2; 435/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 2; 436/469 \tMinibatch Loss 0.078  Accuracy 98%\n",
      "Iteration 2; 437/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 2; 438/469 \tMinibatch Loss 0.107  Accuracy 98%\n",
      "Iteration 2; 439/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 2; 440/469 \tMinibatch Loss 0.048  Accuracy 97%\n",
      "Iteration 2; 441/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 2; 442/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 2; 443/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 2; 444/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 2; 445/469 \tMinibatch Loss 0.069  Accuracy 99%\n",
      "Iteration 2; 446/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 2; 447/469 \tMinibatch Loss 0.029  Accuracy 100%\n",
      "Iteration 2; 448/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 2; 449/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 2; 450/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 2; 451/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 2; 452/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 2; 453/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 2; 454/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 2; 455/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 2; 456/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 2; 457/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 2; 458/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 2; 459/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 2; 460/469 \tMinibatch Loss 0.095  Accuracy 97%\n",
      "Iteration 2; 461/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 2; 462/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 2; 463/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 2; 464/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 2; 465/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 2; 466/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 2; 467/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 2; 468/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 3; 0/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 3; 1/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 3; 2/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 3; 3/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 3; 4/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 3; 5/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 3; 6/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 3; 7/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 3; 8/469 \tMinibatch Loss 0.109  Accuracy 98%\n",
      "Iteration 3; 9/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 3; 10/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 11/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 12/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 3; 13/469 \tMinibatch Loss 0.029  Accuracy 100%\n",
      "Iteration 3; 14/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 3; 15/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 3; 16/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 3; 17/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 3; 18/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 19/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 3; 20/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 3; 21/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 3; 22/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 3; 23/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 24/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 3; 25/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 3; 26/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 3; 27/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 3; 28/469 \tMinibatch Loss 0.026  Accuracy 98%\n",
      "Iteration 3; 29/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 3; 30/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 3; 31/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 3; 32/469 \tMinibatch Loss 0.050  Accuracy 97%\n",
      "Iteration 3; 33/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 3; 34/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 3; 35/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 3; 36/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 3; 37/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 3; 38/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 3; 39/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 3; 40/469 \tMinibatch Loss 0.046  Accuracy 98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3; 41/469 \tMinibatch Loss 0.025  Accuracy 100%\n",
      "Iteration 3; 42/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 3; 43/469 \tMinibatch Loss 0.020  Accuracy 98%\n",
      "Iteration 3; 44/469 \tMinibatch Loss 0.061  Accuracy 99%\n",
      "Iteration 3; 45/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 46/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 3; 47/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 3; 48/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 3; 49/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 3; 50/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 51/469 \tMinibatch Loss 0.023  Accuracy 98%\n",
      "Iteration 3; 52/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 3; 53/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 3; 54/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 3; 55/469 \tMinibatch Loss 0.056  Accuracy 99%\n",
      "Iteration 3; 56/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 3; 57/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 3; 58/469 \tMinibatch Loss 0.024  Accuracy 98%\n",
      "Iteration 3; 59/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 3; 60/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 3; 61/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 3; 62/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 3; 63/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 3; 64/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 3; 65/469 \tMinibatch Loss 0.026  Accuracy 98%\n",
      "Iteration 3; 66/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 3; 67/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 3; 68/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 3; 69/469 \tMinibatch Loss 0.045  Accuracy 99%\n",
      "Iteration 3; 70/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 3; 71/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 3; 72/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 3; 73/469 \tMinibatch Loss 0.074  Accuracy 98%\n",
      "Iteration 3; 74/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 3; 75/469 \tMinibatch Loss 0.025  Accuracy 98%\n",
      "Iteration 3; 76/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 3; 77/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 3; 78/469 \tMinibatch Loss 0.072  Accuracy 97%\n",
      "Iteration 3; 79/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 3; 80/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 3; 81/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 3; 82/469 \tMinibatch Loss 0.074  Accuracy 99%\n",
      "Iteration 3; 83/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 3; 84/469 \tMinibatch Loss 0.048  Accuracy 97%\n",
      "Iteration 3; 85/469 \tMinibatch Loss 0.100  Accuracy 98%\n",
      "Iteration 3; 86/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 3; 87/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 3; 88/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 3; 89/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 3; 90/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 3; 91/469 \tMinibatch Loss 0.087  Accuracy 98%\n",
      "Iteration 3; 92/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 3; 93/469 \tMinibatch Loss 0.065  Accuracy 96%\n",
      "Iteration 3; 94/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 95/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 3; 96/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 3; 97/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 3; 98/469 \tMinibatch Loss 0.140  Accuracy 97%\n",
      "Iteration 3; 99/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 3; 100/469 \tMinibatch Loss 0.090  Accuracy 98%\n",
      "Iteration 3; 101/469 \tMinibatch Loss 0.026  Accuracy 100%\n",
      "Iteration 3; 102/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 3; 103/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 3; 104/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 3; 105/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 3; 106/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 3; 107/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 108/469 \tMinibatch Loss 0.027  Accuracy 100%\n",
      "Iteration 3; 109/469 \tMinibatch Loss 0.045  Accuracy 99%\n",
      "Iteration 3; 110/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 3; 111/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 3; 112/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 3; 113/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 3; 114/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 3; 115/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 3; 116/469 \tMinibatch Loss 0.057  Accuracy 97%\n",
      "Iteration 3; 117/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 3; 118/469 \tMinibatch Loss 0.024  Accuracy 98%\n",
      "Iteration 3; 119/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 120/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 3; 121/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 3; 122/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 123/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 124/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 3; 125/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 3; 126/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 3; 127/469 \tMinibatch Loss 0.040  Accuracy 99%\n",
      "Iteration 3; 128/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 3; 129/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 3; 130/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 131/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 3; 132/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 3; 133/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 3; 134/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 3; 135/469 \tMinibatch Loss 0.076  Accuracy 95%\n",
      "Iteration 3; 136/469 \tMinibatch Loss 0.026  Accuracy 98%\n",
      "Iteration 3; 137/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 3; 138/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 3; 139/469 \tMinibatch Loss 0.086  Accuracy 98%\n",
      "Iteration 3; 140/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 3; 141/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 3; 142/469 \tMinibatch Loss 0.070  Accuracy 96%\n",
      "Iteration 3; 143/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 144/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 3; 145/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 3; 146/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 3; 147/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 3; 148/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 3; 149/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 3; 150/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 3; 151/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 152/469 \tMinibatch Loss 0.143  Accuracy 97%\n",
      "Iteration 3; 153/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 3; 154/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 3; 155/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 156/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 157/469 \tMinibatch Loss 0.112  Accuracy 96%\n",
      "Iteration 3; 158/469 \tMinibatch Loss 0.097  Accuracy 96%\n",
      "Iteration 3; 159/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 3; 160/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 3; 161/469 \tMinibatch Loss 0.100  Accuracy 98%\n",
      "Iteration 3; 162/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 3; 163/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 3; 164/469 \tMinibatch Loss 0.038  Accuracy 100%\n",
      "Iteration 3; 165/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 3; 166/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 3; 167/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 3; 168/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 3; 169/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 170/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 3; 171/469 \tMinibatch Loss 0.062  Accuracy 99%\n",
      "Iteration 3; 172/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 3; 173/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 3; 174/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 3; 175/469 \tMinibatch Loss 0.014  Accuracy 99%\n",
      "Iteration 3; 176/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 3; 177/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 3; 178/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 3; 179/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 3; 180/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 3; 181/469 \tMinibatch Loss 0.077  Accuracy 98%\n",
      "Iteration 3; 182/469 \tMinibatch Loss 0.057  Accuracy 99%\n",
      "Iteration 3; 183/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 3; 184/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 3; 185/469 \tMinibatch Loss 0.014  Accuracy 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3; 186/469 \tMinibatch Loss 0.061  Accuracy 97%\n",
      "Iteration 3; 187/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 3; 188/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 3; 189/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 3; 190/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 3; 191/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 3; 192/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 3; 193/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 3; 194/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 3; 195/469 \tMinibatch Loss 0.119  Accuracy 97%\n",
      "Iteration 3; 196/469 \tMinibatch Loss 0.078  Accuracy 97%\n",
      "Iteration 3; 197/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 3; 198/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 3; 199/469 \tMinibatch Loss 0.033  Accuracy 99%\n",
      "Iteration 3; 200/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 3; 201/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 202/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 3; 203/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 3; 204/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 3; 205/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 206/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 3; 207/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 3; 208/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 3; 209/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 3; 210/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 3; 211/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 3; 212/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 3; 213/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 3; 214/469 \tMinibatch Loss 0.256  Accuracy 97%\n",
      "Iteration 3; 215/469 \tMinibatch Loss 0.075  Accuracy 98%\n",
      "Iteration 3; 216/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 3; 217/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 3; 218/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 3; 219/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 3; 220/469 \tMinibatch Loss 0.064  Accuracy 97%\n",
      "Iteration 3; 221/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 3; 222/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 3; 223/469 \tMinibatch Loss 0.148  Accuracy 96%\n",
      "Iteration 3; 224/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 3; 225/469 \tMinibatch Loss 0.025  Accuracy 100%\n",
      "Iteration 3; 226/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 3; 227/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 3; 228/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 3; 229/469 \tMinibatch Loss 0.128  Accuracy 95%\n",
      "Iteration 3; 230/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 3; 231/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 3; 232/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 3; 233/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 3; 234/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 3; 235/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 3; 236/469 \tMinibatch Loss 0.025  Accuracy 98%\n",
      "Iteration 3; 237/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 3; 238/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 3; 239/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 3; 240/469 \tMinibatch Loss 0.071  Accuracy 97%\n",
      "Iteration 3; 241/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 3; 242/469 \tMinibatch Loss 0.090  Accuracy 97%\n",
      "Iteration 3; 243/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 3; 244/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 3; 245/469 \tMinibatch Loss 0.075  Accuracy 98%\n",
      "Iteration 3; 246/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 3; 247/469 \tMinibatch Loss 0.064  Accuracy 97%\n",
      "Iteration 3; 248/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 249/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 3; 250/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 3; 251/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 3; 252/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 253/469 \tMinibatch Loss 0.046  Accuracy 99%\n",
      "Iteration 3; 254/469 \tMinibatch Loss 0.063  Accuracy 99%\n",
      "Iteration 3; 255/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 3; 256/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 3; 257/469 \tMinibatch Loss 0.083  Accuracy 98%\n",
      "Iteration 3; 258/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 3; 259/469 \tMinibatch Loss 0.043  Accuracy 99%\n",
      "Iteration 3; 260/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 3; 261/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 3; 262/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 263/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 3; 264/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 3; 265/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 3; 266/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 3; 267/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 3; 268/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 3; 269/469 \tMinibatch Loss 0.083  Accuracy 97%\n",
      "Iteration 3; 270/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 3; 271/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 3; 272/469 \tMinibatch Loss 0.012  Accuracy 99%\n",
      "Iteration 3; 273/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 3; 274/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 3; 275/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 3; 276/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 277/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 3; 278/469 \tMinibatch Loss 0.061  Accuracy 97%\n",
      "Iteration 3; 279/469 \tMinibatch Loss 0.053  Accuracy 99%\n",
      "Iteration 3; 280/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 3; 281/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 3; 282/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 3; 283/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 3; 284/469 \tMinibatch Loss 0.076  Accuracy 95%\n",
      "Iteration 3; 285/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 3; 286/469 \tMinibatch Loss 0.024  Accuracy 100%\n",
      "Iteration 3; 287/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 3; 288/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 3; 289/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 290/469 \tMinibatch Loss 0.078  Accuracy 97%\n",
      "Iteration 3; 291/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 3; 292/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 3; 293/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 294/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 3; 295/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 3; 296/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 3; 297/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 3; 298/469 \tMinibatch Loss 0.023  Accuracy 100%\n",
      "Iteration 3; 299/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 3; 300/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 3; 301/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 3; 302/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 3; 303/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 3; 304/469 \tMinibatch Loss 0.160  Accuracy 95%\n",
      "Iteration 3; 305/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 3; 306/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 3; 307/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 308/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 3; 309/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 3; 310/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 311/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 3; 312/469 \tMinibatch Loss 0.121  Accuracy 97%\n",
      "Iteration 3; 313/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 3; 314/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 3; 315/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 3; 316/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 3; 317/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 3; 318/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 3; 319/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 3; 320/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 3; 321/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 3; 322/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 3; 323/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 3; 324/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 3; 325/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 3; 326/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 3; 327/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 3; 328/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 329/469 \tMinibatch Loss 0.022  Accuracy 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3; 330/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 3; 331/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 3; 332/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 3; 333/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 3; 334/469 \tMinibatch Loss 0.057  Accuracy 97%\n",
      "Iteration 3; 335/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 3; 336/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 3; 337/469 \tMinibatch Loss 0.028  Accuracy 98%\n",
      "Iteration 3; 338/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 3; 339/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 340/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 3; 341/469 \tMinibatch Loss 0.057  Accuracy 96%\n",
      "Iteration 3; 342/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 343/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 3; 344/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 3; 345/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 3; 346/469 \tMinibatch Loss 0.103  Accuracy 98%\n",
      "Iteration 3; 347/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 3; 348/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 3; 349/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 3; 350/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 351/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 3; 352/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 3; 353/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 3; 354/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 3; 355/469 \tMinibatch Loss 0.092  Accuracy 98%\n",
      "Iteration 3; 356/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 3; 357/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 3; 358/469 \tMinibatch Loss 0.039  Accuracy 99%\n",
      "Iteration 3; 359/469 \tMinibatch Loss 0.044  Accuracy 99%\n",
      "Iteration 3; 360/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 3; 361/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 362/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 3; 363/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 3; 364/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 3; 365/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 3; 366/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 3; 367/469 \tMinibatch Loss 0.101  Accuracy 98%\n",
      "Iteration 3; 368/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 3; 369/469 \tMinibatch Loss 0.046  Accuracy 99%\n",
      "Iteration 3; 370/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 3; 371/469 \tMinibatch Loss 0.033  Accuracy 99%\n",
      "Iteration 3; 372/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 3; 373/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 3; 374/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 3; 375/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 3; 376/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 377/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 3; 378/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 3; 379/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 380/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 3; 381/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 3; 382/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 383/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 3; 384/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 3; 385/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 386/469 \tMinibatch Loss 0.095  Accuracy 98%\n",
      "Iteration 3; 387/469 \tMinibatch Loss 0.080  Accuracy 97%\n",
      "Iteration 3; 388/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 389/469 \tMinibatch Loss 0.053  Accuracy 99%\n",
      "Iteration 3; 390/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 3; 391/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 3; 392/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 3; 393/469 \tMinibatch Loss 0.053  Accuracy 99%\n",
      "Iteration 3; 394/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 3; 395/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 3; 396/469 \tMinibatch Loss 0.116  Accuracy 97%\n",
      "Iteration 3; 397/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 3; 398/469 \tMinibatch Loss 0.076  Accuracy 97%\n",
      "Iteration 3; 399/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 400/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 3; 401/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 3; 402/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 3; 403/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 3; 404/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 3; 405/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 3; 406/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 3; 407/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 3; 408/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 3; 409/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 3; 410/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 3; 411/469 \tMinibatch Loss 0.033  Accuracy 99%\n",
      "Iteration 3; 412/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 413/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 3; 414/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 3; 415/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 3; 416/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 3; 417/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 3; 418/469 \tMinibatch Loss 0.029  Accuracy 100%\n",
      "Iteration 3; 419/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 3; 420/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 3; 421/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 3; 422/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 3; 423/469 \tMinibatch Loss 0.065  Accuracy 98%\n",
      "Iteration 3; 424/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 3; 425/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 426/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 3; 427/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 428/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 3; 429/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 3; 430/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 3; 431/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 3; 432/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 3; 433/469 \tMinibatch Loss 0.022  Accuracy 98%\n",
      "Iteration 3; 434/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 3; 435/469 \tMinibatch Loss 0.011  Accuracy 99%\n",
      "Iteration 3; 436/469 \tMinibatch Loss 0.143  Accuracy 97%\n",
      "Iteration 3; 437/469 \tMinibatch Loss 0.046  Accuracy 99%\n",
      "Iteration 3; 438/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 3; 439/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 3; 440/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 3; 441/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 3; 442/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 443/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 3; 444/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 3; 445/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 3; 446/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 447/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 3; 448/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 3; 449/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 3; 450/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 3; 451/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 3; 452/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 3; 453/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 3; 454/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 3; 455/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 3; 456/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 3; 457/469 \tMinibatch Loss 0.059  Accuracy 97%\n",
      "Iteration 3; 458/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 3; 459/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 3; 460/469 \tMinibatch Loss 0.002  Accuracy 100%\n",
      "Iteration 3; 461/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 3; 462/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 3; 463/469 \tMinibatch Loss 0.026  Accuracy 100%\n",
      "Iteration 3; 464/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 3; 465/469 \tMinibatch Loss 0.059  Accuracy 99%\n",
      "Iteration 3; 466/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 3; 467/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 3; 468/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 4; 0/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 1/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 4; 2/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 3/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 4; 4/469 \tMinibatch Loss 0.037  Accuracy 99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4; 5/469 \tMinibatch Loss 0.026  Accuracy 100%\n",
      "Iteration 4; 6/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 4; 7/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 4; 8/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 4; 9/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 10/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 4; 11/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 4; 12/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 4; 13/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 4; 14/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 15/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 4; 16/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 4; 17/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 4; 18/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 19/469 \tMinibatch Loss 0.011  Accuracy 99%\n",
      "Iteration 4; 20/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 21/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 4; 22/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 4; 23/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 4; 24/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 4; 25/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 4; 26/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 4; 27/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 4; 28/469 \tMinibatch Loss 0.060  Accuracy 97%\n",
      "Iteration 4; 29/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 4; 30/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 4; 31/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 4; 32/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 4; 33/469 \tMinibatch Loss 0.020  Accuracy 98%\n",
      "Iteration 4; 34/469 \tMinibatch Loss 0.098  Accuracy 98%\n",
      "Iteration 4; 35/469 \tMinibatch Loss 0.060  Accuracy 99%\n",
      "Iteration 4; 36/469 \tMinibatch Loss 0.076  Accuracy 98%\n",
      "Iteration 4; 37/469 \tMinibatch Loss 0.025  Accuracy 98%\n",
      "Iteration 4; 38/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 4; 39/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 40/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 41/469 \tMinibatch Loss 0.082  Accuracy 97%\n",
      "Iteration 4; 42/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 4; 43/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 4; 44/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 4; 45/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 4; 46/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 47/469 \tMinibatch Loss 0.012  Accuracy 99%\n",
      "Iteration 4; 48/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 49/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 4; 50/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 4; 51/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 4; 52/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 4; 53/469 \tMinibatch Loss 0.058  Accuracy 97%\n",
      "Iteration 4; 54/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 4; 55/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 4; 56/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 4; 57/469 \tMinibatch Loss 0.096  Accuracy 98%\n",
      "Iteration 4; 58/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 4; 59/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 4; 60/469 \tMinibatch Loss 0.051  Accuracy 99%\n",
      "Iteration 4; 61/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 4; 62/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 4; 63/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 4; 64/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 4; 65/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 4; 66/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 4; 67/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 4; 68/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 4; 69/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 4; 70/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 4; 71/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 4; 72/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 4; 73/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 74/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 4; 75/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 4; 76/469 \tMinibatch Loss 0.048  Accuracy 97%\n",
      "Iteration 4; 77/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 4; 78/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 79/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 4; 80/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 4; 81/469 \tMinibatch Loss 0.025  Accuracy 98%\n",
      "Iteration 4; 82/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 4; 83/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 4; 84/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 4; 85/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 86/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 87/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 4; 88/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 4; 89/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 4; 90/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 4; 91/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 4; 92/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 4; 93/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 4; 94/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 4; 95/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 96/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 4; 97/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 4; 98/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 4; 99/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 4; 100/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 4; 101/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 4; 102/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 103/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 104/469 \tMinibatch Loss 0.043  Accuracy 99%\n",
      "Iteration 4; 105/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 4; 106/469 \tMinibatch Loss 0.118  Accuracy 97%\n",
      "Iteration 4; 107/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 4; 108/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 109/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 4; 110/469 \tMinibatch Loss 0.040  Accuracy 99%\n",
      "Iteration 4; 111/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 4; 112/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 4; 113/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 4; 114/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 4; 115/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 4; 116/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 4; 117/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 4; 118/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 4; 119/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 120/469 \tMinibatch Loss 0.021  Accuracy 98%\n",
      "Iteration 4; 121/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 122/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 4; 123/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 4; 124/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 125/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 126/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 4; 127/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 4; 128/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 4; 129/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 4; 130/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 131/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 4; 132/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 133/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 4; 134/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 4; 135/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 4; 136/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 4; 137/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 4; 138/469 \tMinibatch Loss 0.012  Accuracy 99%\n",
      "Iteration 4; 139/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 4; 140/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 4; 141/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 4; 142/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 143/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 4; 144/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 145/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 146/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 147/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 4; 148/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 4; 149/469 \tMinibatch Loss 0.004  Accuracy 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4; 150/469 \tMinibatch Loss 0.063  Accuracy 99%\n",
      "Iteration 4; 151/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 4; 152/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 153/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 4; 154/469 \tMinibatch Loss 0.055  Accuracy 99%\n",
      "Iteration 4; 155/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 4; 156/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 4; 157/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 4; 158/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 4; 159/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 4; 160/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 4; 161/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 4; 162/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 4; 163/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 164/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 4; 165/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 4; 166/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 167/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 168/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 169/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 4; 170/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 4; 171/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 4; 172/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 4; 173/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 174/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 4; 175/469 \tMinibatch Loss 0.010  Accuracy 99%\n",
      "Iteration 4; 176/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 4; 177/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 4; 178/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 179/469 \tMinibatch Loss 0.062  Accuracy 97%\n",
      "Iteration 4; 180/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 4; 181/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 4; 182/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 4; 183/469 \tMinibatch Loss 0.043  Accuracy 99%\n",
      "Iteration 4; 184/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 4; 185/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 4; 186/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 4; 187/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 4; 188/469 \tMinibatch Loss 0.033  Accuracy 99%\n",
      "Iteration 4; 189/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 4; 190/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 4; 191/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 4; 192/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 4; 193/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 4; 194/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 4; 195/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 4; 196/469 \tMinibatch Loss 0.115  Accuracy 97%\n",
      "Iteration 4; 197/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 4; 198/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 4; 199/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 200/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 4; 201/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 4; 202/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 4; 203/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 4; 204/469 \tMinibatch Loss 0.052  Accuracy 98%\n",
      "Iteration 4; 205/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 4; 206/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 4; 207/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 4; 208/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 4; 209/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 4; 210/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 4; 211/469 \tMinibatch Loss 0.026  Accuracy 100%\n",
      "Iteration 4; 212/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 4; 213/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 4; 214/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 4; 215/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 4; 216/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 4; 217/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 4; 218/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 4; 219/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 4; 220/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 4; 221/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 4; 222/469 \tMinibatch Loss 0.023  Accuracy 98%\n",
      "Iteration 4; 223/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 4; 224/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 4; 225/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 4; 226/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 4; 227/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 228/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 4; 229/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 4; 230/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 4; 231/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 4; 232/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 4; 233/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 4; 234/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 235/469 \tMinibatch Loss 0.090  Accuracy 96%\n",
      "Iteration 4; 236/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 4; 237/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 4; 238/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 4; 239/469 \tMinibatch Loss 0.028  Accuracy 100%\n",
      "Iteration 4; 240/469 \tMinibatch Loss 0.028  Accuracy 100%\n",
      "Iteration 4; 241/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 4; 242/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 243/469 \tMinibatch Loss 0.089  Accuracy 97%\n",
      "Iteration 4; 244/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 4; 245/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 246/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 247/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 4; 248/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 4; 249/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 4; 250/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 4; 251/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 252/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 4; 253/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 4; 254/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 4; 255/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 4; 256/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 257/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 4; 258/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 4; 259/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 260/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 4; 261/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 4; 262/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 4; 263/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 264/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 265/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 4; 266/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 4; 267/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 4; 268/469 \tMinibatch Loss 0.109  Accuracy 98%\n",
      "Iteration 4; 269/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 4; 270/469 \tMinibatch Loss 0.175  Accuracy 97%\n",
      "Iteration 4; 271/469 \tMinibatch Loss 0.084  Accuracy 98%\n",
      "Iteration 4; 272/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 4; 273/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 274/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 4; 275/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 4; 276/469 \tMinibatch Loss 0.014  Accuracy 99%\n",
      "Iteration 4; 277/469 \tMinibatch Loss 0.064  Accuracy 99%\n",
      "Iteration 4; 278/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 4; 279/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 4; 280/469 \tMinibatch Loss 0.051  Accuracy 99%\n",
      "Iteration 4; 281/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 4; 282/469 \tMinibatch Loss 0.102  Accuracy 98%\n",
      "Iteration 4; 283/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 4; 284/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 4; 285/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 4; 286/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 4; 287/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 4; 288/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 4; 289/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 290/469 \tMinibatch Loss 0.061  Accuracy 97%\n",
      "Iteration 4; 291/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 4; 292/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 4; 293/469 \tMinibatch Loss 0.029  Accuracy 98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4; 294/469 \tMinibatch Loss 0.046  Accuracy 99%\n",
      "Iteration 4; 295/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 4; 296/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 4; 297/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 4; 298/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 299/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 300/469 \tMinibatch Loss 0.071  Accuracy 97%\n",
      "Iteration 4; 301/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 4; 302/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 4; 303/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 4; 304/469 \tMinibatch Loss 0.014  Accuracy 99%\n",
      "Iteration 4; 305/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 4; 306/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 4; 307/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 4; 308/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 309/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 4; 310/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 4; 311/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 4; 312/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 4; 313/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 314/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 4; 315/469 \tMinibatch Loss 0.019  Accuracy 98%\n",
      "Iteration 4; 316/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 4; 317/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 4; 318/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 319/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 4; 320/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 4; 321/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 4; 322/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 4; 323/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 4; 324/469 \tMinibatch Loss 0.139  Accuracy 98%\n",
      "Iteration 4; 325/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 4; 326/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 4; 327/469 \tMinibatch Loss 0.144  Accuracy 95%\n",
      "Iteration 4; 328/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 4; 329/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 4; 330/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 4; 331/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 4; 332/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 4; 333/469 \tMinibatch Loss 0.046  Accuracy 99%\n",
      "Iteration 4; 334/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 4; 335/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 4; 336/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 4; 337/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 4; 338/469 \tMinibatch Loss 0.089  Accuracy 96%\n",
      "Iteration 4; 339/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 4; 340/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 341/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 4; 342/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 4; 343/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 4; 344/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 4; 345/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 4; 346/469 \tMinibatch Loss 0.047  Accuracy 97%\n",
      "Iteration 4; 347/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 4; 348/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 4; 349/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 4; 350/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 4; 351/469 \tMinibatch Loss 0.044  Accuracy 99%\n",
      "Iteration 4; 352/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 4; 353/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 4; 354/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 4; 355/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 4; 356/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 4; 357/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 4; 358/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 4; 359/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 360/469 \tMinibatch Loss 0.056  Accuracy 99%\n",
      "Iteration 4; 361/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 362/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 4; 363/469 \tMinibatch Loss 0.023  Accuracy 98%\n",
      "Iteration 4; 364/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 4; 365/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 4; 366/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 4; 367/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 4; 368/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 4; 369/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 4; 370/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 4; 371/469 \tMinibatch Loss 0.076  Accuracy 98%\n",
      "Iteration 4; 372/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 4; 373/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 4; 374/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 4; 375/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 4; 376/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 4; 377/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 378/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 4; 379/469 \tMinibatch Loss 0.085  Accuracy 98%\n",
      "Iteration 4; 380/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 4; 381/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 4; 382/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 4; 383/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 4; 384/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 4; 385/469 \tMinibatch Loss 0.048  Accuracy 99%\n",
      "Iteration 4; 386/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 4; 387/469 \tMinibatch Loss 0.121  Accuracy 96%\n",
      "Iteration 4; 388/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 4; 389/469 \tMinibatch Loss 0.022  Accuracy 98%\n",
      "Iteration 4; 390/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 391/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 4; 392/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 4; 393/469 \tMinibatch Loss 0.024  Accuracy 100%\n",
      "Iteration 4; 394/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 4; 395/469 \tMinibatch Loss 0.084  Accuracy 98%\n",
      "Iteration 4; 396/469 \tMinibatch Loss 0.081  Accuracy 95%\n",
      "Iteration 4; 397/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 4; 398/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 4; 399/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 4; 400/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 4; 401/469 \tMinibatch Loss 0.115  Accuracy 97%\n",
      "Iteration 4; 402/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 403/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 4; 404/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 4; 405/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 4; 406/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 4; 407/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 4; 408/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 4; 409/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 410/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 4; 411/469 \tMinibatch Loss 0.014  Accuracy 99%\n",
      "Iteration 4; 412/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 4; 413/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 4; 414/469 \tMinibatch Loss 0.043  Accuracy 99%\n",
      "Iteration 4; 415/469 \tMinibatch Loss 0.083  Accuracy 98%\n",
      "Iteration 4; 416/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 4; 417/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 418/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 4; 419/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 4; 420/469 \tMinibatch Loss 0.047  Accuracy 99%\n",
      "Iteration 4; 421/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 4; 422/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 4; 423/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 4; 424/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 4; 425/469 \tMinibatch Loss 0.033  Accuracy 99%\n",
      "Iteration 4; 426/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 4; 427/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 4; 428/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 4; 429/469 \tMinibatch Loss 0.080  Accuracy 98%\n",
      "Iteration 4; 430/469 \tMinibatch Loss 0.074  Accuracy 97%\n",
      "Iteration 4; 431/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 4; 432/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 4; 433/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 4; 434/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 435/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 4; 436/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 4; 437/469 \tMinibatch Loss 0.044  Accuracy 98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4; 438/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 4; 439/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 4; 440/469 \tMinibatch Loss 0.049  Accuracy 99%\n",
      "Iteration 4; 441/469 \tMinibatch Loss 0.011  Accuracy 99%\n",
      "Iteration 4; 442/469 \tMinibatch Loss 0.040  Accuracy 99%\n",
      "Iteration 4; 443/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 4; 444/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 4; 445/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 4; 446/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 4; 447/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 4; 448/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 4; 449/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 4; 450/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 4; 451/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 4; 452/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 4; 453/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 4; 454/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 4; 455/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 4; 456/469 \tMinibatch Loss 0.055  Accuracy 97%\n",
      "Iteration 4; 457/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 4; 458/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 459/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 4; 460/469 \tMinibatch Loss 0.052  Accuracy 99%\n",
      "Iteration 4; 461/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 4; 462/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 4; 463/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 4; 464/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 4; 465/469 \tMinibatch Loss 0.040  Accuracy 99%\n",
      "Iteration 4; 466/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 4; 467/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 4; 468/469 \tMinibatch Loss 0.129  Accuracy 97%\n",
      "Iteration 5; 0/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 1/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 2/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 5; 3/469 \tMinibatch Loss 0.075  Accuracy 98%\n",
      "Iteration 5; 4/469 \tMinibatch Loss 0.057  Accuracy 99%\n",
      "Iteration 5; 5/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 5; 6/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 5; 7/469 \tMinibatch Loss 0.025  Accuracy 98%\n",
      "Iteration 5; 8/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 5; 9/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 10/469 \tMinibatch Loss 0.040  Accuracy 99%\n",
      "Iteration 5; 11/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 5; 12/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 5; 13/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 5; 14/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 5; 15/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 5; 16/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 5; 17/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 5; 18/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 5; 19/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 5; 20/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 5; 21/469 \tMinibatch Loss 0.024  Accuracy 99%\n",
      "Iteration 5; 22/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 23/469 \tMinibatch Loss 0.023  Accuracy 100%\n",
      "Iteration 5; 24/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 25/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 26/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 5; 27/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 5; 28/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 5; 29/469 \tMinibatch Loss 0.017  Accuracy 98%\n",
      "Iteration 5; 30/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 31/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 5; 32/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 5; 33/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 5; 34/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 5; 35/469 \tMinibatch Loss 0.091  Accuracy 98%\n",
      "Iteration 5; 36/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 5; 37/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 5; 38/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 5; 39/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 40/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 5; 41/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 5; 42/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 43/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 5; 44/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 45/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 5; 46/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 5; 47/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 5; 48/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 5; 49/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 5; 50/469 \tMinibatch Loss 0.080  Accuracy 98%\n",
      "Iteration 5; 51/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 5; 52/469 \tMinibatch Loss 0.009  Accuracy 99%\n",
      "Iteration 5; 53/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 5; 54/469 \tMinibatch Loss 0.022  Accuracy 98%\n",
      "Iteration 5; 55/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 5; 56/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 5; 57/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 5; 58/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 5; 59/469 \tMinibatch Loss 0.046  Accuracy 97%\n",
      "Iteration 5; 60/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 5; 61/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 5; 62/469 \tMinibatch Loss 0.030  Accuracy 100%\n",
      "Iteration 5; 63/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 5; 64/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 5; 65/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 5; 66/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 5; 67/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 5; 68/469 \tMinibatch Loss 0.033  Accuracy 99%\n",
      "Iteration 5; 69/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 5; 70/469 \tMinibatch Loss 0.060  Accuracy 98%\n",
      "Iteration 5; 71/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 5; 72/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 5; 73/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 74/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 5; 75/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 76/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 5; 77/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 5; 78/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 79/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 5; 80/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 5; 81/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 82/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 83/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 84/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 5; 85/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 86/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 5; 87/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 88/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 5; 89/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 90/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 5; 91/469 \tMinibatch Loss 0.064  Accuracy 99%\n",
      "Iteration 5; 92/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 5; 93/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 5; 94/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 5; 95/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 96/469 \tMinibatch Loss 0.023  Accuracy 100%\n",
      "Iteration 5; 97/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 5; 98/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 5; 99/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 5; 100/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 5; 101/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 5; 102/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 5; 103/469 \tMinibatch Loss 0.146  Accuracy 97%\n",
      "Iteration 5; 104/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 5; 105/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 5; 106/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 5; 107/469 \tMinibatch Loss 0.019  Accuracy 98%\n",
      "Iteration 5; 108/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 5; 109/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 110/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 5; 111/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 5; 112/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 5; 113/469 \tMinibatch Loss 0.011  Accuracy 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5; 114/469 \tMinibatch Loss 0.038  Accuracy 99%\n",
      "Iteration 5; 115/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 5; 116/469 \tMinibatch Loss 0.048  Accuracy 99%\n",
      "Iteration 5; 117/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 5; 118/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 5; 119/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 5; 120/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 5; 121/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 122/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 5; 123/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 124/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 5; 125/469 \tMinibatch Loss 0.021  Accuracy 98%\n",
      "Iteration 5; 126/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 127/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 5; 128/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 129/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 5; 130/469 \tMinibatch Loss 0.036  Accuracy 99%\n",
      "Iteration 5; 131/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 5; 132/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 5; 133/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 134/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 135/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 5; 136/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 137/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 138/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 5; 139/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 5; 140/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 5; 141/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 142/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 5; 143/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 5; 144/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 5; 145/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 5; 146/469 \tMinibatch Loss 0.019  Accuracy 98%\n",
      "Iteration 5; 147/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 5; 148/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 5; 149/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 150/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 5; 151/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 5; 152/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 5; 153/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 5; 154/469 \tMinibatch Loss 0.002  Accuracy 100%\n",
      "Iteration 5; 155/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 156/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 5; 157/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 5; 158/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 159/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 5; 160/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 5; 161/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 162/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 5; 163/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 5; 164/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 5; 165/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 5; 166/469 \tMinibatch Loss 0.081  Accuracy 97%\n",
      "Iteration 5; 167/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 5; 168/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 169/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 5; 170/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 5; 171/469 \tMinibatch Loss 0.043  Accuracy 99%\n",
      "Iteration 5; 172/469 \tMinibatch Loss 0.061  Accuracy 99%\n",
      "Iteration 5; 173/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 5; 174/469 \tMinibatch Loss 0.053  Accuracy 97%\n",
      "Iteration 5; 175/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 5; 176/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 5; 177/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 5; 178/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 5; 179/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 5; 180/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 5; 181/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 5; 182/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 5; 183/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 184/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 5; 185/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 186/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 187/469 \tMinibatch Loss 0.120  Accuracy 97%\n",
      "Iteration 5; 188/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 5; 189/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 5; 190/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 5; 191/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 5; 192/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 193/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 5; 194/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 5; 195/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 5; 196/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 5; 197/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 5; 198/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 5; 199/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 5; 200/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 201/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 5; 202/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 5; 203/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 5; 204/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 5; 205/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 5; 206/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 5; 207/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 5; 208/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 209/469 \tMinibatch Loss 0.097  Accuracy 96%\n",
      "Iteration 5; 210/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 5; 211/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 5; 212/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 5; 213/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 214/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 215/469 \tMinibatch Loss 0.020  Accuracy 98%\n",
      "Iteration 5; 216/469 \tMinibatch Loss 0.020  Accuracy 98%\n",
      "Iteration 5; 217/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 218/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 5; 219/469 \tMinibatch Loss 0.024  Accuracy 98%\n",
      "Iteration 5; 220/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 5; 221/469 \tMinibatch Loss 0.023  Accuracy 100%\n",
      "Iteration 5; 222/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 223/469 \tMinibatch Loss 0.055  Accuracy 98%\n",
      "Iteration 5; 224/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 5; 225/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 5; 226/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 227/469 \tMinibatch Loss 0.051  Accuracy 99%\n",
      "Iteration 5; 228/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 229/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 5; 230/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 5; 231/469 \tMinibatch Loss 0.051  Accuracy 98%\n",
      "Iteration 5; 232/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 5; 233/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 234/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 5; 235/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 5; 236/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 237/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 5; 238/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 5; 239/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 5; 240/469 \tMinibatch Loss 0.048  Accuracy 98%\n",
      "Iteration 5; 241/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 5; 242/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 5; 243/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 5; 244/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 5; 245/469 \tMinibatch Loss 0.066  Accuracy 99%\n",
      "Iteration 5; 246/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 5; 247/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 5; 248/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 5; 249/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 250/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 5; 251/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 5; 252/469 \tMinibatch Loss 0.039  Accuracy 99%\n",
      "Iteration 5; 253/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 5; 254/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 5; 255/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 256/469 \tMinibatch Loss 0.016  Accuracy 99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5; 257/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 258/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 5; 259/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 5; 260/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 261/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 5; 262/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 5; 263/469 \tMinibatch Loss 0.036  Accuracy 98%\n",
      "Iteration 5; 264/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 265/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 5; 266/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 5; 267/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 5; 268/469 \tMinibatch Loss 0.124  Accuracy 97%\n",
      "Iteration 5; 269/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 5; 270/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 271/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 5; 272/469 \tMinibatch Loss 0.070  Accuracy 98%\n",
      "Iteration 5; 273/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 274/469 \tMinibatch Loss 0.012  Accuracy 99%\n",
      "Iteration 5; 275/469 \tMinibatch Loss 0.033  Accuracy 98%\n",
      "Iteration 5; 276/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 5; 277/469 \tMinibatch Loss 0.025  Accuracy 98%\n",
      "Iteration 5; 278/469 \tMinibatch Loss 0.085  Accuracy 98%\n",
      "Iteration 5; 279/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 5; 280/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 281/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 282/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 283/469 \tMinibatch Loss 0.053  Accuracy 99%\n",
      "Iteration 5; 284/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 285/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 5; 286/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 5; 287/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 5; 288/469 \tMinibatch Loss 0.041  Accuracy 99%\n",
      "Iteration 5; 289/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 290/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 5; 291/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 5; 292/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 293/469 \tMinibatch Loss 0.027  Accuracy 98%\n",
      "Iteration 5; 294/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 5; 295/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 5; 296/469 \tMinibatch Loss 0.002  Accuracy 100%\n",
      "Iteration 5; 297/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 5; 298/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 5; 299/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 5; 300/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 5; 301/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 5; 302/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 5; 303/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 5; 304/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 5; 305/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 306/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 307/469 \tMinibatch Loss 0.067  Accuracy 98%\n",
      "Iteration 5; 308/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 309/469 \tMinibatch Loss 0.135  Accuracy 99%\n",
      "Iteration 5; 310/469 \tMinibatch Loss 0.032  Accuracy 99%\n",
      "Iteration 5; 311/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 312/469 \tMinibatch Loss 0.079  Accuracy 98%\n",
      "Iteration 5; 313/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 314/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 315/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 5; 316/469 \tMinibatch Loss 0.037  Accuracy 98%\n",
      "Iteration 5; 317/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 5; 318/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 5; 319/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 5; 320/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 5; 321/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 5; 322/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 5; 323/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 5; 324/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 5; 325/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 5; 326/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 5; 327/469 \tMinibatch Loss 0.078  Accuracy 98%\n",
      "Iteration 5; 328/469 \tMinibatch Loss 0.014  Accuracy 99%\n",
      "Iteration 5; 329/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 330/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 5; 331/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 5; 332/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 5; 333/469 \tMinibatch Loss 0.035  Accuracy 99%\n",
      "Iteration 5; 334/469 \tMinibatch Loss 0.022  Accuracy 99%\n",
      "Iteration 5; 335/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 5; 336/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 5; 337/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 5; 338/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 339/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 5; 340/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 5; 341/469 \tMinibatch Loss 0.069  Accuracy 98%\n",
      "Iteration 5; 342/469 \tMinibatch Loss 0.061  Accuracy 98%\n",
      "Iteration 5; 343/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 344/469 \tMinibatch Loss 0.028  Accuracy 98%\n",
      "Iteration 5; 345/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 346/469 \tMinibatch Loss 0.062  Accuracy 97%\n",
      "Iteration 5; 347/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 5; 348/469 \tMinibatch Loss 0.017  Accuracy 99%\n",
      "Iteration 5; 349/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 5; 350/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 5; 351/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 5; 352/469 \tMinibatch Loss 0.026  Accuracy 98%\n",
      "Iteration 5; 353/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 5; 354/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 5; 355/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 5; 356/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 5; 357/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 5; 358/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 5; 359/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 5; 360/469 \tMinibatch Loss 0.038  Accuracy 97%\n",
      "Iteration 5; 361/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 5; 362/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 5; 363/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 5; 364/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 365/469 \tMinibatch Loss 0.015  Accuracy 100%\n",
      "Iteration 5; 366/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "Iteration 5; 367/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 5; 368/469 \tMinibatch Loss 0.013  Accuracy 100%\n",
      "Iteration 5; 369/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 5; 370/469 \tMinibatch Loss 0.102  Accuracy 97%\n",
      "Iteration 5; 371/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 5; 372/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 373/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 5; 374/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 5; 375/469 \tMinibatch Loss 0.034  Accuracy 99%\n",
      "Iteration 5; 376/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 5; 377/469 \tMinibatch Loss 0.058  Accuracy 98%\n",
      "Iteration 5; 378/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 5; 379/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 5; 380/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 381/469 \tMinibatch Loss 0.068  Accuracy 97%\n",
      "Iteration 5; 382/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 383/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 5; 384/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 5; 385/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 5; 386/469 \tMinibatch Loss 0.012  Accuracy 99%\n",
      "Iteration 5; 387/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 5; 388/469 \tMinibatch Loss 0.007  Accuracy 100%\n",
      "Iteration 5; 389/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 5; 390/469 \tMinibatch Loss 0.113  Accuracy 97%\n",
      "Iteration 5; 391/469 \tMinibatch Loss 0.030  Accuracy 98%\n",
      "Iteration 5; 392/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 5; 393/469 \tMinibatch Loss 0.011  Accuracy 99%\n",
      "Iteration 5; 394/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 5; 395/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 5; 396/469 \tMinibatch Loss 0.029  Accuracy 99%\n",
      "Iteration 5; 397/469 \tMinibatch Loss 0.047  Accuracy 98%\n",
      "Iteration 5; 398/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 399/469 \tMinibatch Loss 0.054  Accuracy 98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5; 400/469 \tMinibatch Loss 0.084  Accuracy 98%\n",
      "Iteration 5; 401/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 5; 402/469 \tMinibatch Loss 0.089  Accuracy 98%\n",
      "Iteration 5; 403/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 5; 404/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 5; 405/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 5; 406/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 5; 407/469 \tMinibatch Loss 0.014  Accuracy 100%\n",
      "Iteration 5; 408/469 \tMinibatch Loss 0.020  Accuracy 99%\n",
      "Iteration 5; 409/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 5; 410/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 411/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 5; 412/469 \tMinibatch Loss 0.010  Accuracy 100%\n",
      "Iteration 5; 413/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 5; 414/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 5; 415/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 416/469 \tMinibatch Loss 0.084  Accuracy 98%\n",
      "Iteration 5; 417/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 5; 418/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 5; 419/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 420/469 \tMinibatch Loss 0.018  Accuracy 99%\n",
      "Iteration 5; 421/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 5; 422/469 \tMinibatch Loss 0.054  Accuracy 98%\n",
      "Iteration 5; 423/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 5; 424/469 \tMinibatch Loss 0.033  Accuracy 99%\n",
      "Iteration 5; 425/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 426/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 427/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 428/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 5; 429/469 \tMinibatch Loss 0.008  Accuracy 100%\n",
      "Iteration 5; 430/469 \tMinibatch Loss 0.031  Accuracy 98%\n",
      "Iteration 5; 431/469 \tMinibatch Loss 0.003  Accuracy 100%\n",
      "Iteration 5; 432/469 \tMinibatch Loss 0.004  Accuracy 100%\n",
      "Iteration 5; 433/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 5; 434/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 5; 435/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 436/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 5; 437/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 5; 438/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 5; 439/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 440/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 5; 441/469 \tMinibatch Loss 0.016  Accuracy 99%\n",
      "Iteration 5; 442/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 5; 443/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 5; 444/469 \tMinibatch Loss 0.015  Accuracy 99%\n",
      "Iteration 5; 445/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 5; 446/469 \tMinibatch Loss 0.039  Accuracy 99%\n",
      "Iteration 5; 447/469 \tMinibatch Loss 0.037  Accuracy 99%\n",
      "Iteration 5; 448/469 \tMinibatch Loss 0.019  Accuracy 99%\n",
      "Iteration 5; 449/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 450/469 \tMinibatch Loss 0.077  Accuracy 98%\n",
      "Iteration 5; 451/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 452/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 5; 453/469 \tMinibatch Loss 0.062  Accuracy 98%\n",
      "Iteration 5; 454/469 \tMinibatch Loss 0.012  Accuracy 99%\n",
      "Iteration 5; 455/469 \tMinibatch Loss 0.005  Accuracy 100%\n",
      "Iteration 5; 456/469 \tMinibatch Loss 0.023  Accuracy 99%\n",
      "Iteration 5; 457/469 \tMinibatch Loss 0.052  Accuracy 99%\n",
      "Iteration 5; 458/469 \tMinibatch Loss 0.021  Accuracy 99%\n",
      "Iteration 5; 459/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 5; 460/469 \tMinibatch Loss 0.006  Accuracy 100%\n",
      "Iteration 5; 461/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 5; 462/469 \tMinibatch Loss 0.009  Accuracy 100%\n",
      "Iteration 5; 463/469 \tMinibatch Loss 0.042  Accuracy 98%\n",
      "Iteration 5; 464/469 \tMinibatch Loss 0.078  Accuracy 98%\n",
      "Iteration 5; 465/469 \tMinibatch Loss 0.090  Accuracy 98%\n",
      "Iteration 5; 466/469 \tMinibatch Loss 0.087  Accuracy 98%\n",
      "Iteration 5; 467/469 \tMinibatch Loss 0.064  Accuracy 98%\n",
      "Iteration 5; 468/469 \tMinibatch Loss 0.046  Accuracy 97%\n",
      "saving model at: models/mnist_test_6iter_250.pth\n"
     ]
    }
   ],
   "source": [
    "#Training routine\n",
    "\n",
    "for iter in range(MAX_ITER_MNIST):\n",
    "    for batch_idx, (x, y) in enumerate(mnist_train_loader):\n",
    "        max_len = int(np.ceil(len(mnist_train_loader.dataset)/BATCH_SIZE_TRAIN_MNIST))\n",
    "        output = mnist_model(x)\n",
    "\n",
    "        accuracy = get_accuracy(output, y)\n",
    "\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        mnist_train_optimizer.step()\n",
    "        mnist_train_optimizer.zero_grad()\n",
    "\n",
    "        print(\n",
    "            \"Iteration {}; {}/{} \\t\".format(iter, batch_idx, max_len) +\n",
    "            \"Minibatch Loss %.3f  \" % (loss) +\n",
    "            \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "        )\n",
    "\n",
    "print(\"saving model at: {}\".format(MNIST_PATH))\n",
    "torch.save(mnist_model.state_dict(), MNIST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: models/mnist_test_6iter_250.pth\n",
      "Batch 0/79 \tAccuracy 100%\n",
      "Batch 10/79 \tAccuracy 97%\n",
      "Batch 20/79 \tAccuracy 98%\n",
      "Batch 30/79 \tAccuracy 98%\n",
      "Batch 40/79 \tAccuracy 100%\n",
      "Batch 50/79 \tAccuracy 99%\n",
      "Batch 60/79 \tAccuracy 100%\n",
      "Batch 70/79 \tAccuracy 100%\n",
      "overall test accuracy on MNIST: 98.86 %\n"
     ]
    }
   ],
   "source": [
    "#predict in distribution\n",
    "MNIST_PATH = \"models/mnist_test_6iter_250.pth\"\n",
    "\n",
    "mnist_model = LPADirNN()\n",
    "print(\"loading model from: {}\".format(MNIST_PATH))\n",
    "mnist_model.load_state_dict(torch.load(MNIST_PATH))\n",
    "mnist_model.eval()\n",
    "\n",
    "acc = []\n",
    "\n",
    "for batch_idx, (x, y) in enumerate(mnist_test_loader):\n",
    "        max_len = int(np.ceil(len(mnist_test_loader.dataset)/BATCH_SIZE_TEST_MNIST))\n",
    "        output = mnist_model(x)\n",
    "        \n",
    "        accuracy = get_accuracy(output, y)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Batch {}/{} \\t\".format(batch_idx, max_len) + \n",
    "                \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "            )\n",
    "        acc.append(accuracy)\n",
    "    \n",
    "avg_acc = np.mean(acc)\n",
    "print('overall test accuracy on MNIST: {:.02f} %'.format(avg_acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/469\n",
      "Batch: 1/469\n",
      "Batch: 2/469\n",
      "Batch: 3/469\n",
      "Batch: 4/469\n",
      "Batch: 5/469\n",
      "Batch: 6/469\n",
      "Batch: 7/469\n",
      "Batch: 8/469\n",
      "Batch: 9/469\n",
      "Batch: 10/469\n",
      "Batch: 11/469\n",
      "Batch: 12/469\n",
      "Batch: 13/469\n",
      "Batch: 14/469\n",
      "Batch: 15/469\n",
      "Batch: 16/469\n",
      "Batch: 17/469\n",
      "Batch: 18/469\n",
      "Batch: 19/469\n",
      "Batch: 20/469\n",
      "Batch: 21/469\n",
      "Batch: 22/469\n",
      "Batch: 23/469\n",
      "Batch: 24/469\n",
      "Batch: 25/469\n",
      "Batch: 26/469\n",
      "Batch: 27/469\n",
      "Batch: 28/469\n",
      "Batch: 29/469\n",
      "Batch: 30/469\n",
      "Batch: 31/469\n",
      "Batch: 32/469\n",
      "Batch: 33/469\n",
      "Batch: 34/469\n",
      "Batch: 35/469\n",
      "Batch: 36/469\n",
      "Batch: 37/469\n",
      "Batch: 38/469\n",
      "Batch: 39/469\n",
      "Batch: 40/469\n",
      "Batch: 41/469\n",
      "Batch: 42/469\n",
      "Batch: 43/469\n",
      "Batch: 44/469\n",
      "Batch: 45/469\n",
      "Batch: 46/469\n",
      "Batch: 47/469\n",
      "Batch: 48/469\n",
      "Batch: 49/469\n",
      "Batch: 50/469\n",
      "Batch: 51/469\n",
      "Batch: 52/469\n",
      "Batch: 53/469\n",
      "Batch: 54/469\n",
      "Batch: 55/469\n",
      "Batch: 56/469\n",
      "Batch: 57/469\n",
      "Batch: 58/469\n",
      "Batch: 59/469\n",
      "Batch: 60/469\n",
      "Batch: 61/469\n",
      "Batch: 62/469\n",
      "Batch: 63/469\n",
      "Batch: 64/469\n",
      "Batch: 65/469\n",
      "Batch: 66/469\n",
      "Batch: 67/469\n",
      "Batch: 68/469\n",
      "Batch: 69/469\n",
      "Batch: 70/469\n",
      "Batch: 71/469\n",
      "Batch: 72/469\n",
      "Batch: 73/469\n",
      "Batch: 74/469\n",
      "Batch: 75/469\n",
      "Batch: 76/469\n",
      "Batch: 77/469\n",
      "Batch: 78/469\n",
      "Batch: 79/469\n",
      "Batch: 80/469\n",
      "Batch: 81/469\n",
      "Batch: 82/469\n",
      "Batch: 83/469\n",
      "Batch: 84/469\n",
      "Batch: 85/469\n",
      "Batch: 86/469\n",
      "Batch: 87/469\n",
      "Batch: 88/469\n",
      "Batch: 89/469\n",
      "Batch: 90/469\n",
      "Batch: 91/469\n",
      "Batch: 92/469\n",
      "Batch: 93/469\n",
      "Batch: 94/469\n",
      "Batch: 95/469\n",
      "Batch: 96/469\n",
      "Batch: 97/469\n",
      "Batch: 98/469\n",
      "Batch: 99/469\n",
      "Batch: 100/469\n",
      "Batch: 101/469\n",
      "Batch: 102/469\n",
      "Batch: 103/469\n",
      "Batch: 104/469\n",
      "Batch: 105/469\n",
      "Batch: 106/469\n",
      "Batch: 107/469\n",
      "Batch: 108/469\n",
      "Batch: 109/469\n",
      "Batch: 110/469\n",
      "Batch: 111/469\n",
      "Batch: 112/469\n",
      "Batch: 113/469\n",
      "Batch: 114/469\n",
      "Batch: 115/469\n",
      "Batch: 116/469\n",
      "Batch: 117/469\n",
      "Batch: 118/469\n",
      "Batch: 119/469\n",
      "Batch: 120/469\n",
      "Batch: 121/469\n",
      "Batch: 122/469\n",
      "Batch: 123/469\n",
      "Batch: 124/469\n",
      "Batch: 125/469\n",
      "Batch: 126/469\n",
      "Batch: 127/469\n",
      "Batch: 128/469\n",
      "Batch: 129/469\n",
      "Batch: 130/469\n",
      "Batch: 131/469\n",
      "Batch: 132/469\n",
      "Batch: 133/469\n",
      "Batch: 134/469\n",
      "Batch: 135/469\n",
      "Batch: 136/469\n",
      "Batch: 137/469\n",
      "Batch: 138/469\n",
      "Batch: 139/469\n",
      "Batch: 140/469\n",
      "Batch: 141/469\n",
      "Batch: 142/469\n",
      "Batch: 143/469\n",
      "Batch: 144/469\n",
      "Batch: 145/469\n",
      "Batch: 146/469\n",
      "Batch: 147/469\n",
      "Batch: 148/469\n",
      "Batch: 149/469\n",
      "Batch: 150/469\n",
      "Batch: 151/469\n",
      "Batch: 152/469\n",
      "Batch: 153/469\n",
      "Batch: 154/469\n",
      "Batch: 155/469\n",
      "Batch: 156/469\n",
      "Batch: 157/469\n",
      "Batch: 158/469\n",
      "Batch: 159/469\n",
      "Batch: 160/469\n",
      "Batch: 161/469\n",
      "Batch: 162/469\n",
      "Batch: 163/469\n",
      "Batch: 164/469\n",
      "Batch: 165/469\n",
      "Batch: 166/469\n",
      "Batch: 167/469\n",
      "Batch: 168/469\n",
      "Batch: 169/469\n",
      "Batch: 170/469\n",
      "Batch: 171/469\n",
      "Batch: 172/469\n",
      "Batch: 173/469\n",
      "Batch: 174/469\n",
      "Batch: 175/469\n",
      "Batch: 176/469\n",
      "Batch: 177/469\n",
      "Batch: 178/469\n",
      "Batch: 179/469\n",
      "Batch: 180/469\n",
      "Batch: 181/469\n",
      "Batch: 182/469\n",
      "Batch: 183/469\n",
      "Batch: 184/469\n",
      "Batch: 185/469\n",
      "Batch: 186/469\n",
      "Batch: 187/469\n",
      "Batch: 188/469\n",
      "Batch: 189/469\n",
      "Batch: 190/469\n",
      "Batch: 191/469\n",
      "Batch: 192/469\n",
      "Batch: 193/469\n",
      "Batch: 194/469\n",
      "Batch: 195/469\n",
      "Batch: 196/469\n",
      "Batch: 197/469\n",
      "Batch: 198/469\n",
      "Batch: 199/469\n",
      "Batch: 200/469\n",
      "Batch: 201/469\n",
      "Batch: 202/469\n",
      "Batch: 203/469\n",
      "Batch: 204/469\n",
      "Batch: 205/469\n",
      "Batch: 206/469\n",
      "Batch: 207/469\n",
      "Batch: 208/469\n",
      "Batch: 209/469\n",
      "Batch: 210/469\n",
      "Batch: 211/469\n",
      "Batch: 212/469\n",
      "Batch: 213/469\n",
      "Batch: 214/469\n",
      "Batch: 215/469\n",
      "Batch: 216/469\n",
      "Batch: 217/469\n",
      "Batch: 218/469\n",
      "Batch: 219/469\n",
      "Batch: 220/469\n",
      "Batch: 221/469\n",
      "Batch: 222/469\n",
      "Batch: 223/469\n",
      "Batch: 224/469\n",
      "Batch: 225/469\n",
      "Batch: 226/469\n",
      "Batch: 227/469\n",
      "Batch: 228/469\n",
      "Batch: 229/469\n",
      "Batch: 230/469\n",
      "Batch: 231/469\n",
      "Batch: 232/469\n",
      "Batch: 233/469\n",
      "Batch: 234/469\n",
      "Batch: 235/469\n",
      "Batch: 236/469\n",
      "Batch: 237/469\n",
      "Batch: 238/469\n",
      "Batch: 239/469\n",
      "Batch: 240/469\n",
      "Batch: 241/469\n",
      "Batch: 242/469\n",
      "Batch: 243/469\n",
      "Batch: 244/469\n",
      "Batch: 245/469\n",
      "Batch: 246/469\n",
      "Batch: 247/469\n",
      "Batch: 248/469\n",
      "Batch: 249/469\n",
      "Batch: 250/469\n",
      "Batch: 251/469\n",
      "Batch: 252/469\n",
      "Batch: 253/469\n",
      "Batch: 254/469\n",
      "Batch: 255/469\n",
      "Batch: 256/469\n",
      "Batch: 257/469\n",
      "Batch: 258/469\n",
      "Batch: 259/469\n",
      "Batch: 260/469\n",
      "Batch: 261/469\n",
      "Batch: 262/469\n",
      "Batch: 263/469\n",
      "Batch: 264/469\n",
      "Batch: 265/469\n",
      "Batch: 266/469\n",
      "Batch: 267/469\n",
      "Batch: 268/469\n",
      "Batch: 269/469\n",
      "Batch: 270/469\n",
      "Batch: 271/469\n",
      "Batch: 272/469\n",
      "Batch: 273/469\n",
      "Batch: 274/469\n",
      "Batch: 275/469\n",
      "Batch: 276/469\n",
      "Batch: 277/469\n",
      "Batch: 278/469\n",
      "Batch: 279/469\n",
      "Batch: 280/469\n",
      "Batch: 281/469\n",
      "Batch: 282/469\n",
      "Batch: 283/469\n",
      "Batch: 284/469\n",
      "Batch: 285/469\n",
      "Batch: 286/469\n",
      "Batch: 287/469\n",
      "Batch: 288/469\n",
      "Batch: 289/469\n",
      "Batch: 290/469\n",
      "Batch: 291/469\n",
      "Batch: 292/469\n",
      "Batch: 293/469\n",
      "Batch: 294/469\n",
      "Batch: 295/469\n",
      "Batch: 296/469\n",
      "Batch: 297/469\n",
      "Batch: 298/469\n",
      "Batch: 299/469\n",
      "Batch: 300/469\n",
      "Batch: 301/469\n",
      "Batch: 302/469\n",
      "Batch: 303/469\n",
      "Batch: 304/469\n",
      "Batch: 305/469\n",
      "Batch: 306/469\n",
      "Batch: 307/469\n",
      "Batch: 308/469\n",
      "Batch: 309/469\n",
      "Batch: 310/469\n",
      "Batch: 311/469\n",
      "Batch: 312/469\n",
      "Batch: 313/469\n",
      "Batch: 314/469\n",
      "Batch: 315/469\n",
      "Batch: 316/469\n",
      "Batch: 317/469\n",
      "Batch: 318/469\n",
      "Batch: 319/469\n",
      "Batch: 320/469\n",
      "Batch: 321/469\n",
      "Batch: 322/469\n",
      "Batch: 323/469\n",
      "Batch: 324/469\n",
      "Batch: 325/469\n",
      "Batch: 326/469\n",
      "Batch: 327/469\n",
      "Batch: 328/469\n",
      "Batch: 329/469\n",
      "Batch: 330/469\n",
      "Batch: 331/469\n",
      "Batch: 332/469\n",
      "Batch: 333/469\n",
      "Batch: 334/469\n",
      "Batch: 335/469\n",
      "Batch: 336/469\n",
      "Batch: 337/469\n",
      "Batch: 338/469\n",
      "Batch: 339/469\n",
      "Batch: 340/469\n",
      "Batch: 341/469\n",
      "Batch: 342/469\n",
      "Batch: 343/469\n",
      "Batch: 344/469\n",
      "Batch: 345/469\n",
      "Batch: 346/469\n",
      "Batch: 347/469\n",
      "Batch: 348/469\n",
      "Batch: 349/469\n",
      "Batch: 350/469\n",
      "Batch: 351/469\n",
      "Batch: 352/469\n",
      "Batch: 353/469\n",
      "Batch: 354/469\n",
      "Batch: 355/469\n",
      "Batch: 356/469\n",
      "Batch: 357/469\n",
      "Batch: 358/469\n",
      "Batch: 359/469\n",
      "Batch: 360/469\n",
      "Batch: 361/469\n",
      "Batch: 362/469\n",
      "Batch: 363/469\n",
      "Batch: 364/469\n",
      "Batch: 365/469\n",
      "Batch: 366/469\n",
      "Batch: 367/469\n",
      "Batch: 368/469\n",
      "Batch: 369/469\n",
      "Batch: 370/469\n",
      "Batch: 371/469\n",
      "Batch: 372/469\n",
      "Batch: 373/469\n",
      "Batch: 374/469\n",
      "Batch: 375/469\n",
      "Batch: 376/469\n",
      "Batch: 377/469\n",
      "Batch: 378/469\n",
      "Batch: 379/469\n",
      "Batch: 380/469\n",
      "Batch: 381/469\n",
      "Batch: 382/469\n",
      "Batch: 383/469\n",
      "Batch: 384/469\n",
      "Batch: 385/469\n",
      "Batch: 386/469\n",
      "Batch: 387/469\n",
      "Batch: 388/469\n",
      "Batch: 389/469\n",
      "Batch: 390/469\n",
      "Batch: 391/469\n",
      "Batch: 392/469\n",
      "Batch: 393/469\n",
      "Batch: 394/469\n",
      "Batch: 395/469\n",
      "Batch: 396/469\n",
      "Batch: 397/469\n",
      "Batch: 398/469\n",
      "Batch: 399/469\n",
      "Batch: 400/469\n",
      "Batch: 401/469\n",
      "Batch: 402/469\n",
      "Batch: 403/469\n",
      "Batch: 404/469\n",
      "Batch: 405/469\n",
      "Batch: 406/469\n",
      "Batch: 407/469\n",
      "Batch: 408/469\n",
      "Batch: 409/469\n",
      "Batch: 410/469\n",
      "Batch: 411/469\n",
      "Batch: 412/469\n",
      "Batch: 413/469\n",
      "Batch: 414/469\n",
      "Batch: 415/469\n",
      "Batch: 416/469\n",
      "Batch: 417/469\n",
      "Batch: 418/469\n",
      "Batch: 419/469\n",
      "Batch: 420/469\n",
      "Batch: 421/469\n",
      "Batch: 422/469\n",
      "Batch: 423/469\n",
      "Batch: 424/469\n",
      "Batch: 425/469\n",
      "Batch: 426/469\n",
      "Batch: 427/469\n",
      "Batch: 428/469\n",
      "Batch: 429/469\n",
      "Batch: 430/469\n",
      "Batch: 431/469\n",
      "Batch: 432/469\n",
      "Batch: 433/469\n",
      "Batch: 434/469\n",
      "Batch: 435/469\n",
      "Batch: 436/469\n",
      "Batch: 437/469\n",
      "Batch: 438/469\n",
      "Batch: 439/469\n",
      "Batch: 440/469\n",
      "Batch: 441/469\n",
      "Batch: 442/469\n",
      "Batch: 443/469\n",
      "Batch: 444/469\n",
      "Batch: 445/469\n",
      "Batch: 446/469\n",
      "Batch: 447/469\n",
      "Batch: 448/469\n",
      "Batch: 449/469\n",
      "Batch: 450/469\n",
      "Batch: 451/469\n",
      "Batch: 452/469\n",
      "Batch: 453/469\n",
      "Batch: 454/469\n",
      "Batch: 455/469\n",
      "Batch: 456/469\n",
      "Batch: 457/469\n",
      "Batch: 458/469\n",
      "Batch: 459/469\n",
      "Batch: 460/469\n",
      "Batch: 461/469\n",
      "Batch: 462/469\n",
      "Batch: 463/469\n",
      "Batch: 464/469\n",
      "Batch: 465/469\n",
      "Batch: 466/469\n",
      "Batch: 467/469\n",
      "Batch: 468/469\n"
     ]
    }
   ],
   "source": [
    "M_W_post, M_b_post, U_post, V_post, B_post = KFLP_second_order(model=mnist_model,\n",
    "                                                               batch_size=BATCH_SIZE_TRAIN_MNIST,\n",
    "                                                               train_loader=mnist_train_loader,\n",
    "                                                               var0 = 10,\n",
    "                                                               device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 250 inputs to linear layer with m: 10 classes\n",
      "Batch: 0/469\n",
      "Batch: 1/469\n",
      "Batch: 2/469\n",
      "Batch: 3/469\n",
      "Batch: 4/469\n",
      "Batch: 5/469\n",
      "Batch: 6/469\n",
      "Batch: 7/469\n",
      "Batch: 8/469\n",
      "Batch: 9/469\n",
      "Batch: 10/469\n",
      "Batch: 11/469\n",
      "Batch: 12/469\n",
      "Batch: 13/469\n",
      "Batch: 14/469\n",
      "Batch: 15/469\n",
      "Batch: 16/469\n",
      "Batch: 17/469\n",
      "Batch: 18/469\n",
      "Batch: 19/469\n",
      "Batch: 20/469\n",
      "Batch: 21/469\n",
      "Batch: 22/469\n",
      "Batch: 23/469\n",
      "Batch: 24/469\n",
      "Batch: 25/469\n",
      "Batch: 26/469\n",
      "Batch: 27/469\n",
      "Batch: 28/469\n",
      "Batch: 29/469\n",
      "Batch: 30/469\n",
      "Batch: 31/469\n",
      "Batch: 32/469\n",
      "Batch: 33/469\n",
      "Batch: 34/469\n",
      "Batch: 35/469\n",
      "Batch: 36/469\n",
      "Batch: 37/469\n",
      "Batch: 38/469\n",
      "Batch: 39/469\n",
      "Batch: 40/469\n",
      "Batch: 41/469\n",
      "Batch: 42/469\n",
      "Batch: 43/469\n",
      "Batch: 44/469\n",
      "Batch: 45/469\n",
      "Batch: 46/469\n",
      "Batch: 47/469\n",
      "Batch: 48/469\n",
      "Batch: 49/469\n",
      "Batch: 50/469\n",
      "Batch: 51/469\n",
      "Batch: 52/469\n",
      "Batch: 53/469\n",
      "Batch: 54/469\n",
      "Batch: 55/469\n",
      "Batch: 56/469\n",
      "Batch: 57/469\n",
      "Batch: 58/469\n",
      "Batch: 59/469\n",
      "Batch: 60/469\n",
      "Batch: 61/469\n",
      "Batch: 62/469\n",
      "Batch: 63/469\n",
      "Batch: 64/469\n",
      "Batch: 65/469\n",
      "Batch: 66/469\n",
      "Batch: 67/469\n",
      "Batch: 68/469\n",
      "Batch: 69/469\n",
      "Batch: 70/469\n",
      "Batch: 71/469\n",
      "Batch: 72/469\n",
      "Batch: 73/469\n",
      "Batch: 74/469\n",
      "Batch: 75/469\n",
      "Batch: 76/469\n",
      "Batch: 77/469\n",
      "Batch: 78/469\n",
      "Batch: 79/469\n",
      "Batch: 80/469\n",
      "Batch: 81/469\n",
      "Batch: 82/469\n",
      "Batch: 83/469\n",
      "Batch: 84/469\n",
      "Batch: 85/469\n",
      "Batch: 86/469\n",
      "Batch: 87/469\n",
      "Batch: 88/469\n",
      "Batch: 89/469\n",
      "Batch: 90/469\n",
      "Batch: 91/469\n",
      "Batch: 92/469\n",
      "Batch: 93/469\n",
      "Batch: 94/469\n",
      "Batch: 95/469\n",
      "Batch: 96/469\n",
      "Batch: 97/469\n",
      "Batch: 98/469\n",
      "Batch: 99/469\n",
      "Batch: 100/469\n",
      "Batch: 101/469\n",
      "Batch: 102/469\n",
      "Batch: 103/469\n",
      "Batch: 104/469\n",
      "Batch: 105/469\n",
      "Batch: 106/469\n",
      "Batch: 107/469\n",
      "Batch: 108/469\n",
      "Batch: 109/469\n",
      "Batch: 110/469\n",
      "Batch: 111/469\n",
      "Batch: 112/469\n",
      "Batch: 113/469\n",
      "Batch: 114/469\n",
      "Batch: 115/469\n",
      "Batch: 116/469\n",
      "Batch: 117/469\n",
      "Batch: 118/469\n",
      "Batch: 119/469\n",
      "Batch: 120/469\n",
      "Batch: 121/469\n",
      "Batch: 122/469\n",
      "Batch: 123/469\n",
      "Batch: 124/469\n",
      "Batch: 125/469\n",
      "Batch: 126/469\n",
      "Batch: 127/469\n",
      "Batch: 128/469\n",
      "Batch: 129/469\n",
      "Batch: 130/469\n",
      "Batch: 131/469\n",
      "Batch: 132/469\n",
      "Batch: 133/469\n",
      "Batch: 134/469\n",
      "Batch: 135/469\n",
      "Batch: 136/469\n",
      "Batch: 137/469\n",
      "Batch: 138/469\n",
      "Batch: 139/469\n",
      "Batch: 140/469\n",
      "Batch: 141/469\n",
      "Batch: 142/469\n",
      "Batch: 143/469\n",
      "Batch: 144/469\n",
      "Batch: 145/469\n",
      "Batch: 146/469\n",
      "Batch: 147/469\n",
      "Batch: 148/469\n",
      "Batch: 149/469\n",
      "Batch: 150/469\n",
      "Batch: 151/469\n",
      "Batch: 152/469\n",
      "Batch: 153/469\n",
      "Batch: 154/469\n",
      "Batch: 155/469\n",
      "Batch: 156/469\n",
      "Batch: 157/469\n",
      "Batch: 158/469\n",
      "Batch: 159/469\n",
      "Batch: 160/469\n",
      "Batch: 161/469\n",
      "Batch: 162/469\n",
      "Batch: 163/469\n",
      "Batch: 164/469\n",
      "Batch: 165/469\n",
      "Batch: 166/469\n",
      "Batch: 167/469\n",
      "Batch: 168/469\n",
      "Batch: 169/469\n",
      "Batch: 170/469\n",
      "Batch: 171/469\n",
      "Batch: 172/469\n",
      "Batch: 173/469\n",
      "Batch: 174/469\n",
      "Batch: 175/469\n",
      "Batch: 176/469\n",
      "Batch: 177/469\n",
      "Batch: 178/469\n",
      "Batch: 179/469\n",
      "Batch: 180/469\n",
      "Batch: 181/469\n",
      "Batch: 182/469\n",
      "Batch: 183/469\n",
      "Batch: 184/469\n",
      "Batch: 185/469\n",
      "Batch: 186/469\n",
      "Batch: 187/469\n",
      "Batch: 188/469\n",
      "Batch: 189/469\n",
      "Batch: 190/469\n",
      "Batch: 191/469\n",
      "Batch: 192/469\n",
      "Batch: 193/469\n",
      "Batch: 194/469\n",
      "Batch: 195/469\n",
      "Batch: 196/469\n",
      "Batch: 197/469\n",
      "Batch: 198/469\n",
      "Batch: 199/469\n",
      "Batch: 200/469\n",
      "Batch: 201/469\n",
      "Batch: 202/469\n",
      "Batch: 203/469\n",
      "Batch: 204/469\n",
      "Batch: 205/469\n",
      "Batch: 206/469\n",
      "Batch: 207/469\n",
      "Batch: 208/469\n",
      "Batch: 209/469\n",
      "Batch: 210/469\n",
      "Batch: 211/469\n",
      "Batch: 212/469\n",
      "Batch: 213/469\n",
      "Batch: 214/469\n",
      "Batch: 215/469\n",
      "Batch: 216/469\n",
      "Batch: 217/469\n",
      "Batch: 218/469\n",
      "Batch: 219/469\n",
      "Batch: 220/469\n",
      "Batch: 221/469\n",
      "Batch: 222/469\n",
      "Batch: 223/469\n",
      "Batch: 224/469\n",
      "Batch: 225/469\n",
      "Batch: 226/469\n",
      "Batch: 227/469\n",
      "Batch: 228/469\n",
      "Batch: 229/469\n",
      "Batch: 230/469\n",
      "Batch: 231/469\n",
      "Batch: 232/469\n",
      "Batch: 233/469\n",
      "Batch: 234/469\n",
      "Batch: 235/469\n",
      "Batch: 236/469\n",
      "Batch: 237/469\n",
      "Batch: 238/469\n",
      "Batch: 239/469\n",
      "Batch: 240/469\n",
      "Batch: 241/469\n",
      "Batch: 242/469\n",
      "Batch: 243/469\n",
      "Batch: 244/469\n",
      "Batch: 245/469\n",
      "Batch: 246/469\n",
      "Batch: 247/469\n",
      "Batch: 248/469\n",
      "Batch: 249/469\n",
      "Batch: 250/469\n",
      "Batch: 251/469\n",
      "Batch: 252/469\n",
      "Batch: 253/469\n",
      "Batch: 254/469\n",
      "Batch: 255/469\n",
      "Batch: 256/469\n",
      "Batch: 257/469\n",
      "Batch: 258/469\n",
      "Batch: 259/469\n",
      "Batch: 260/469\n",
      "Batch: 261/469\n",
      "Batch: 262/469\n",
      "Batch: 263/469\n",
      "Batch: 264/469\n",
      "Batch: 265/469\n",
      "Batch: 266/469\n",
      "Batch: 267/469\n",
      "Batch: 268/469\n",
      "Batch: 269/469\n",
      "Batch: 270/469\n",
      "Batch: 271/469\n",
      "Batch: 272/469\n",
      "Batch: 273/469\n",
      "Batch: 274/469\n",
      "Batch: 275/469\n",
      "Batch: 276/469\n",
      "Batch: 277/469\n",
      "Batch: 278/469\n",
      "Batch: 279/469\n",
      "Batch: 280/469\n",
      "Batch: 281/469\n",
      "Batch: 282/469\n",
      "Batch: 283/469\n",
      "Batch: 284/469\n",
      "Batch: 285/469\n",
      "Batch: 286/469\n",
      "Batch: 287/469\n",
      "Batch: 288/469\n",
      "Batch: 289/469\n",
      "Batch: 290/469\n",
      "Batch: 291/469\n",
      "Batch: 292/469\n",
      "Batch: 293/469\n",
      "Batch: 294/469\n",
      "Batch: 295/469\n",
      "Batch: 296/469\n",
      "Batch: 297/469\n",
      "Batch: 298/469\n",
      "Batch: 299/469\n",
      "Batch: 300/469\n",
      "Batch: 301/469\n",
      "Batch: 302/469\n",
      "Batch: 303/469\n",
      "Batch: 304/469\n",
      "Batch: 305/469\n",
      "Batch: 306/469\n",
      "Batch: 307/469\n",
      "Batch: 308/469\n",
      "Batch: 309/469\n",
      "Batch: 310/469\n",
      "Batch: 311/469\n",
      "Batch: 312/469\n",
      "Batch: 313/469\n",
      "Batch: 314/469\n",
      "Batch: 315/469\n",
      "Batch: 316/469\n",
      "Batch: 317/469\n",
      "Batch: 318/469\n",
      "Batch: 319/469\n",
      "Batch: 320/469\n",
      "Batch: 321/469\n",
      "Batch: 322/469\n",
      "Batch: 323/469\n",
      "Batch: 324/469\n",
      "Batch: 325/469\n",
      "Batch: 326/469\n",
      "Batch: 327/469\n",
      "Batch: 328/469\n",
      "Batch: 329/469\n",
      "Batch: 330/469\n",
      "Batch: 331/469\n",
      "Batch: 332/469\n",
      "Batch: 333/469\n",
      "Batch: 334/469\n",
      "Batch: 335/469\n",
      "Batch: 336/469\n",
      "Batch: 337/469\n",
      "Batch: 338/469\n",
      "Batch: 339/469\n",
      "Batch: 340/469\n",
      "Batch: 341/469\n",
      "Batch: 342/469\n",
      "Batch: 343/469\n",
      "Batch: 344/469\n",
      "Batch: 345/469\n",
      "Batch: 346/469\n",
      "Batch: 347/469\n",
      "Batch: 348/469\n",
      "Batch: 349/469\n",
      "Batch: 350/469\n",
      "Batch: 351/469\n",
      "Batch: 352/469\n",
      "Batch: 353/469\n",
      "Batch: 354/469\n",
      "Batch: 355/469\n",
      "Batch: 356/469\n",
      "Batch: 357/469\n",
      "Batch: 358/469\n",
      "Batch: 359/469\n",
      "Batch: 360/469\n",
      "Batch: 361/469\n",
      "Batch: 362/469\n",
      "Batch: 363/469\n",
      "Batch: 364/469\n",
      "Batch: 365/469\n",
      "Batch: 366/469\n",
      "Batch: 367/469\n",
      "Batch: 368/469\n",
      "Batch: 369/469\n",
      "Batch: 370/469\n",
      "Batch: 371/469\n",
      "Batch: 372/469\n",
      "Batch: 373/469\n",
      "Batch: 374/469\n",
      "Batch: 375/469\n",
      "Batch: 376/469\n",
      "Batch: 377/469\n",
      "Batch: 378/469\n",
      "Batch: 379/469\n",
      "Batch: 380/469\n",
      "Batch: 381/469\n",
      "Batch: 382/469\n",
      "Batch: 383/469\n",
      "Batch: 384/469\n",
      "Batch: 385/469\n",
      "Batch: 386/469\n",
      "Batch: 387/469\n",
      "Batch: 388/469\n",
      "Batch: 389/469\n",
      "Batch: 390/469\n",
      "Batch: 391/469\n",
      "Batch: 392/469\n",
      "Batch: 393/469\n",
      "Batch: 394/469\n",
      "Batch: 395/469\n",
      "Batch: 396/469\n",
      "Batch: 397/469\n",
      "Batch: 398/469\n",
      "Batch: 399/469\n",
      "Batch: 400/469\n",
      "Batch: 401/469\n",
      "Batch: 402/469\n",
      "Batch: 403/469\n",
      "Batch: 404/469\n",
      "Batch: 405/469\n",
      "Batch: 406/469\n",
      "Batch: 407/469\n",
      "Batch: 408/469\n",
      "Batch: 409/469\n",
      "Batch: 410/469\n",
      "Batch: 411/469\n",
      "Batch: 412/469\n",
      "Batch: 413/469\n",
      "Batch: 414/469\n",
      "Batch: 415/469\n",
      "Batch: 416/469\n",
      "Batch: 417/469\n",
      "Batch: 418/469\n",
      "Batch: 419/469\n",
      "Batch: 420/469\n",
      "Batch: 421/469\n",
      "Batch: 422/469\n",
      "Batch: 423/469\n",
      "Batch: 424/469\n",
      "Batch: 425/469\n",
      "Batch: 426/469\n",
      "Batch: 427/469\n",
      "Batch: 428/469\n",
      "Batch: 429/469\n",
      "Batch: 430/469\n",
      "Batch: 431/469\n",
      "Batch: 432/469\n",
      "Batch: 433/469\n",
      "Batch: 434/469\n",
      "Batch: 435/469\n",
      "Batch: 436/469\n",
      "Batch: 437/469\n",
      "Batch: 438/469\n",
      "Batch: 439/469\n",
      "Batch: 440/469\n",
      "Batch: 441/469\n",
      "Batch: 442/469\n",
      "Batch: 443/469\n",
      "Batch: 444/469\n",
      "Batch: 445/469\n",
      "Batch: 446/469\n",
      "Batch: 447/469\n",
      "Batch: 448/469\n",
      "Batch: 449/469\n",
      "Batch: 450/469\n",
      "Batch: 451/469\n",
      "Batch: 452/469\n",
      "Batch: 453/469\n",
      "Batch: 454/469\n",
      "Batch: 455/469\n",
      "Batch: 456/469\n",
      "Batch: 457/469\n",
      "Batch: 458/469\n",
      "Batch: 459/469\n",
      "Batch: 460/469\n",
      "Batch: 461/469\n",
      "Batch: 462/469\n",
      "Batch: 463/469\n",
      "Batch: 464/469\n",
      "Batch: 465/469\n",
      "Batch: 466/469\n",
      "Batch: 467/469\n",
      "Batch: 468/469\n",
      "469\n"
     ]
    }
   ],
   "source": [
    "M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D =  Diag_second_order(model=mnist_model,\n",
    "                                                                batch_size=BATCH_SIZE_TRAIN_MNIST,\n",
    "                                                                train_loader=mnist_train_loader,\n",
    "                                                                var0 = 10,\n",
    "                                                                device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TEST_FMNIST = 128\n",
    "BATCH_SIZE_TRAIN_FMNIST = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FMNIST_dataset = torchvision.datasets.FashionMNIST(\n",
    "        './fmnist', train=True, download=True,\n",
    "        transform=MNIST_transform) #torchvision.transforms.ToTensor())\n",
    "\n",
    "train_size = int(0.8 * len(FMNIST_dataset))\n",
    "val_size = len(FMNIST_dataset) - train_size\n",
    "FMNIST_train_dataset, FMNIST_val_dataset = torch.utils.data.random_split(FMNIST_dataset, [train_size, val_size])\n",
    "\n",
    "FMNIST_train_loader = torch.utils.data.DataLoader(\n",
    "    FMNIST_train_dataset,\n",
    "    batch_size=BATCH_SIZE_TRAIN_FMNIST, shuffle=True)\n",
    "\n",
    "FMNIST_val_loader = torch.utils.data.DataLoader(\n",
    "    FMNIST_val_dataset,\n",
    "    batch_size=BATCH_SIZE_TRAIN_FMNIST, shuffle=False)\n",
    "\n",
    "\n",
    "FMNIST_test = torchvision.datasets.FashionMNIST(\n",
    "        './fmnist', train=False, download=False,\n",
    "        transform=MNIST_transform)   #torchvision.transforms.ToTensor())\n",
    "\n",
    "FMNIST_test_loader = torch.utils.data.DataLoader(\n",
    "    FMNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_FMNIST, shuffle=False)\n",
    "\n",
    "\n",
    "#### load with normalization\n",
    "FMNIST_test_normalized = torchvision.datasets.FashionMNIST(\n",
    "        './fmnist', train=False, download=False,\n",
    "        transform=MNIST_transform_normalized)   #torchvision.transforms.ToTensor())\n",
    "\n",
    "FMNIST_test_loader_normalized = torch.utils.data.DataLoader(\n",
    "    FMNIST_test_normalized,\n",
    "    batch_size=BATCH_SIZE_TEST_FMNIST, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load notMNIST\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from scipy.misc import imread\n",
    "from torch import Tensor\n",
    "\n",
    "\"\"\"\n",
    "Loads the train/test set. \n",
    "Every image in the dataset is 28x28 pixels and the labels are numbered from 0-9\n",
    "for A-J respectively.\n",
    "Set root to point to the Train/Test folders.\n",
    "\"\"\"\n",
    "\n",
    "# Creating a sub class of torch.utils.data.dataset.Dataset\n",
    "class notMNIST(Dataset):\n",
    "\n",
    "    # The init method is called when this class will be instantiated\n",
    "    def __init__(self, root, transform):\n",
    "        \n",
    "        #super(notMNIST, self).__init__(root, transform=transform)\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "        Images, Y = [], []\n",
    "        folders = os.listdir(root)\n",
    "\n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            for ims in os.listdir(folder_path):\n",
    "                try:\n",
    "                    img_path = os.path.join(folder_path, ims)\n",
    "                    Images.append(np.array(imread(img_path)))\n",
    "                    Y.append(ord(folder) - 65)  # Folders are A-J so labels will be 0-9\n",
    "                except:\n",
    "                    # Some images in the dataset are damaged\n",
    "                    print(\"File {}/{} is broken\".format(folder, ims))\n",
    "        data = [(x, y) for x, y in zip(Images, Y)]\n",
    "        self.data = data\n",
    "        self.targets = torch.Tensor(Y)\n",
    "\n",
    "    # The number of items in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # The Dataloader is a generator that repeatedly calls the getitem method.\n",
    "    # getitem is supposed to return (X, Y) for the specified index.\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index][0]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        # Input for Conv2D should be Channels x Height x Width\n",
    "        img_tensor = Tensor(img).view(1, 28, 28).float()\n",
    "        label = self.data[index][1]\n",
    "        return (img_tensor, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marius/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png is broken\n",
      "File A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png is broken\n"
     ]
    }
   ],
   "source": [
    "root = os.path.abspath('')\n",
    "\n",
    "# Instantiating the notMNIST dataset class we created\n",
    "notMNIST_test = notMNIST(root=os.path.join(root, 'notMNIST_small'),\n",
    "                               transform=MNIST_transform)\n",
    "\n",
    "# Creating a dataloader\n",
    "not_mnist_test_loader = torch.utils.data.dataloader.DataLoader(\n",
    "                            dataset=notMNIST_test,\n",
    "                            batch_size=128,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas_from_mS(mu, Sigma):\n",
    "\n",
    "    alpha_hat = torch.softmax(mu, 0)\n",
    "    K = mu.size(-1)\n",
    "    z = 1/K * (torch.sum(torch.diag(Sigma)/(alpha_hat*(1-alpha_hat) + 10e-8)))\n",
    "    \n",
    "    return(alpha_hat * z)\n",
    "\n",
    "def get_alphas_mS_batch(mu, Sigma):\n",
    "    \n",
    "    n = mu.size(0)\n",
    "    alphas = torch.zeros(n, mu.size(1))\n",
    "    for i in range(n):\n",
    "        alphas[i] = get_alphas_from_mS(mu[i], Sigma[i])\n",
    "        \n",
    "    return(alphas)\n",
    "\n",
    "def get_alphas_mS_variance_match(mu, Sigma):\n",
    "    \n",
    "    diag = torch.inverse(Sigma).diagonal()\n",
    "    alphas = torch.zeros(len(mu))\n",
    "    alpha_hat = mu.sum()\n",
    "    #alpha_hat = Sigma.diagonal().sum()\n",
    "    #alpha_hat = (mu * Sigma.diagonal()).sum()\n",
    "    #print(\"mu, Sigma ratio: \", mu.sum()/Sigma.diagonal().sum())\n",
    "    #print(\"mu, Sigma: \", mu, diag)\n",
    "    #print(\"alpha_hat: \", alpha_hat)\n",
    "    for i in range(len(mu)):\n",
    "        alphas[i] = 0.5 * alpha_hat + torch.sqrt(alpha_hat**2 *4 * (1 - alpha_hat*diag[i] + diag[i]))\n",
    "\n",
    "    #alphas *= mu #torch.softmax(mu, 0)\n",
    "    return(alphas)      \n",
    "\n",
    "def get_alphas_mS_variance_match_batch(mu, Sigma):\n",
    "    \n",
    "    n = mu.size(0)\n",
    "    alphas = torch.zeros(n, mu.size(1))\n",
    "    for i in range(n):\n",
    "        alphas[i] = get_alphas_mS_variance_match(mu[i], Sigma[i])\n",
    "        \n",
    "    return(alphas)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_DIR_LPA(model, test_loader, M_W_post, M_b_post, U_post, V_post, B_post, verbose=False, method='McKay'):\n",
    "    #variance is any of ['McKay', 'MAP_match', 'variance_match']\n",
    "\n",
    "    alphas = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        max_len = int(np.ceil(len(test_loader.dataset)/len(test_loader)))\n",
    "\n",
    "        phi = model.phi(x)\n",
    "\n",
    "        mu_pred = phi @ M_W_post + M_b_post\n",
    "        Cov_pred = torch.diag(phi @ U_post @ phi.t()).reshape(-1, 1, 1) * V_post.unsqueeze(0) + B_post.unsqueeze(0)\n",
    "\n",
    "        if method == 'MAP_match':\n",
    "            alpha = get_alphas_mS_batch(mu_pred, Cov_pred).detach()\n",
    "        elif method == 'variance_match':\n",
    "            alpha = get_alphas_mS_variance_match_batch(mu_pred, Cov_pred).detach()\n",
    "        elif method == 'McKay':\n",
    "            alpha = get_alpha_from_Normal(mu_pred, Cov_pred).detach()\n",
    "        else:\n",
    "            raise('The method you currently chose does not exist')\n",
    "        \n",
    "        #print(alpha)\n",
    "        #alpha /= alpha.sum(dim=1).view(-1,1).detach()\n",
    "        #alpha = alpha.detach()\n",
    "\n",
    "        #assert(torch.sum(alpha.sum(dim=1) - torch.ones(len(x))). == pytest.approx(0, 10e-5))\n",
    "\n",
    "        alphas.append(alpha)\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "\n",
    "    return(torch.cat(alphas, dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_dist_values(py_in, targets):\n",
    "    acc_in = np.mean(np.argmax(py_in, 1) == targets)\n",
    "    prob_correct = np.choose(targets, py_in.T).mean()\n",
    "    average_entropy = -np.sum(py_in*np.log(py_in+1e-8), axis=1).mean()\n",
    "    MMC = py_in.max(1).mean()\n",
    "    return(acc_in, prob_correct, average_entropy, MMC)\n",
    "    \n",
    "def get_out_dist_values(py_in, py_out, targets):\n",
    "    average_entropy = -np.sum(py_out*np.log(py_out+1e-8), axis=1).mean()\n",
    "    acc_out = np.mean(np.argmax(py_out, 1) == targets)\n",
    "    prob_correct = np.choose(targets, py_out.T).mean()\n",
    "    labels = np.zeros(len(py_in)+len(py_out), dtype='int32')\n",
    "    labels[:len(py_in)] = 1\n",
    "    examples = np.concatenate([py_in.max(1), py_out.max(1)])\n",
    "    auroc = roc_auc_score(labels, examples)\n",
    "    MMC = py_out.max(1).mean()\n",
    "    return(acc_out, prob_correct, average_entropy, MMC, auroc)\n",
    "\n",
    "def print_in_dist_values(acc_in, prob_correct, average_entropy, MMC, train='mnist', method='LLLA-KF'):\n",
    "    \n",
    "    print(f'[In, {method}, {train}] Accuracy: {acc_in:.3f}; average entropy: {average_entropy:.3f}; \\\n",
    "    MMC: {MMC:.3f}; Prob @ correct: {prob_correct:.3f}')\n",
    "\n",
    "\n",
    "def print_out_dist_values(acc_out, prob_correct, average_entropy, MMC, auroc, train='mnist', test='FMNIST', method='LLLA-KF'):\n",
    "   \n",
    "    print(f'[Out-{test}, {method}, {train}] Accuracy: {acc_out:.3f}; Average entropy: {average_entropy:.3f};\\\n",
    "    MMC: {MMC:.3f}; AUROC: {auroc:.3f}; Prob @ correct: {prob_correct:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = MNIST_test.targets.numpy()\n",
    "targets_FMNIST = FMNIST_test.targets.numpy()\n",
    "targets_notMNIST = notMNIST_test.targets.numpy().astype(int)\n",
    "targets_FMNIST_normalized = FMNIST_test_normalized.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_MAP = predict_MAP(mnist_model, mnist_test_loader).numpy()\n",
    "mnist_test_out_fmnist_MAP = predict_MAP(mnist_model, FMNIST_test_loader).numpy()\n",
    "mnist_test_out_notMNIST_MAP = predict_MAP(mnist_model, not_mnist_test_loader).numpy()\n",
    "mnist_test_out_FMNIST_normalized_MAP = predict_MAP(mnist_model, FMNIST_test_loader_normalized).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP = get_in_dist_values(mnist_test_in_MAP, targets)\n",
    "acc_out_FMNIST_MAP, prob_correct_out_FMNIST_MAP, ent_out_FMNIST_MAP, MMC_out_FMNIST_MAP, auroc_out_FMNIST_MAP = get_out_dist_values(mnist_test_in_MAP, mnist_test_out_fmnist_MAP, targets_FMNIST)\n",
    "acc_out_notMNIST_MAP, prob_correct_out_notMNIST_MAP, ent_out_notMNIST_MAP, MMC_out_notMNIST_MAP, auroc_out_notMNIST_MAP = get_out_dist_values(mnist_test_in_MAP, mnist_test_out_notMNIST_MAP, targets_notMNIST)\n",
    "acc_out_FMNISTn_MAP, prob_correct_out_FMNISTn_MAP, ent_out_FMNISTn_MAP, MMC_out_FMNISTn_MAP, auroc_out_FMNISTn_MAP = get_out_dist_values(mnist_test_in_MAP, mnist_test_out_FMNIST_normalized_MAP, targets_FMNIST_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, MAP, mnist] Accuracy: 0.989; average entropy: 0.041;     MMC: 0.988; Prob @ correct: 0.981\n",
      "[Out-MAP, LLLA-KF, fmnist] Accuracy: 0.090; Average entropy: 1.363;    MMC: 0.531; AUROC: 0.991; Prob @ correct: 0.106\n",
      "[Out-MAP, LLLA-KF, notMNIST] Accuracy: 0.129; Average entropy: 0.806;    MMC: 0.718; AUROC: 0.934; Prob @ correct: 0.122\n",
      "[Out-MAP, LLLA-KF, FMNISTn] Accuracy: 0.089; Average entropy: 0.421;    MMC: 0.841; AUROC: 0.880; Prob @ correct: 0.094\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP, 'mnist', 'MAP')\n",
    "print_out_dist_values(acc_out_FMNIST_MAP, prob_correct_out_FMNIST_MAP, ent_out_FMNIST_MAP, MMC_out_FMNIST_MAP, auroc_out_FMNIST_MAP, 'fmnist', 'MAP')\n",
    "print_out_dist_values(acc_out_notMNIST_MAP, prob_correct_out_notMNIST_MAP, ent_out_notMNIST_MAP, MMC_out_notMNIST_MAP, auroc_out_notMNIST_MAP, 'notMNIST', 'MAP')\n",
    "print_out_dist_values(acc_out_FMNISTn_MAP, prob_correct_out_FMNISTn_MAP, ent_out_FMNISTn_MAP, MMC_out_FMNISTn_MAP, auroc_out_FMNISTn_MAP, 'FMNISTn', 'MAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diag Hessian Sampling estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_D = predict_diagonal_sampling(mnist_model, mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=100).numpy()\n",
    "mnist_test_out_FMNIST_D = predict_diagonal_sampling(mnist_model, FMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=100).numpy()\n",
    "mnist_test_out_notMNIST_D = predict_diagonal_sampling(mnist_model, not_mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=100).numpy()\n",
    "mnist_test_out_FMNIST_normalized_D = predict_diagonal_sampling(mnist_model, FMNIST_test_loader_normalized, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=100).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D = get_in_dist_values(mnist_test_in_D, targets)\n",
    "acc_out_FMNIST_D, prob_correct_out_FMNIST_D, ent_out_FMNIST_D, MMC_out_FMNIST_D, auroc_out_FMNIST_D = get_out_dist_values(mnist_test_in_D, mnist_test_out_FMNIST_D, targets_FMNIST)\n",
    "acc_out_notMNIST_D, prob_correct_out_notMNIST_D, ent_out_notMNIST_D, MMC_out_notMNIST_D, auroc_out_notMNIST_D = get_out_dist_values(mnist_test_in_D, mnist_test_out_notMNIST_D, targets_notMNIST)\n",
    "acc_out_FMNISTn_D, prob_correct_out_FMNISTn_D, ent_out_FMNISTn_D, MMC_out_FMNISTn_D, auroc_out_FMNISTn_D = get_out_dist_values(mnist_test_in_D, mnist_test_out_FMNIST_normalized_D, targets_FMNIST_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, Diag, mnist] Accuracy: 0.989; average entropy: 0.310;     MMC: 0.908; Prob @ correct: 0.905\n",
      "[Out-fmnist, Diag, mnist] Accuracy: 0.092; Average entropy: 1.514;    MMC: 0.470; AUROC: 0.972; Prob @ correct: 0.107\n",
      "[Out-notMNIST, Diag, mnist] Accuracy: 0.130; Average entropy: 1.093;    MMC: 0.613; AUROC: 0.889; Prob @ correct: 0.123\n",
      "[Out-FMNISTn, Diag, mnist] Accuracy: 0.091; Average entropy: 0.955;    MMC: 0.634; AUROC: 0.908; Prob @ correct: 0.105\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D, 'mnist', 'Diag')\n",
    "print_out_dist_values(acc_out_FMNIST_D, prob_correct_out_FMNIST_D, ent_out_FMNIST_D, MMC_out_FMNIST_D, auroc_out_FMNIST_D, test='fmnist', method='Diag')\n",
    "print_out_dist_values(acc_out_notMNIST_D, prob_correct_out_notMNIST_D, ent_out_notMNIST_D, MMC_out_notMNIST_D, auroc_out_notMNIST_D, test='notMNIST', method='Diag')\n",
    "print_out_dist_values(acc_out_FMNISTn_D, prob_correct_out_FMNISTn_D, ent_out_FMNISTn_D, MMC_out_FMNISTn_D, auroc_out_FMNISTn_D, test='FMNISTn', method='Diag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFAC Hessian Sampling estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_KFAC = predict_laplace(mnist_model, mnist_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, n_samples=100).numpy()\n",
    "mnist_test_out_FMNIST_KFAC = predict_laplace(mnist_model, FMNIST_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, n_samples=100).numpy()\n",
    "mnist_test_out_notMNIST_KFAC = predict_laplace(mnist_model, not_mnist_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, n_samples=100).numpy()\n",
    "mnist_test_out_FMNIST_normalized_KFAC = predict_laplace(mnist_model, FMNIST_test_loader_normalized, M_W_post, M_b_post, U_post, V_post, B_post, n_samples=100).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_KFAC, prob_correct_in_KFAC, ent_in_KFAC, MMC_in_KFAC = get_in_dist_values(mnist_test_in_KFAC, targets)\n",
    "acc_out_FMNIST_KFAC, prob_correct_out_FMNIST_KFAC, ent_out_FMNIST_KFAC, MMC_out_FMNIST_KFAC, auroc_out_FMNIST_KFAC = get_out_dist_values(mnist_test_in_KFAC, mnist_test_out_FMNIST_KFAC, targets_FMNIST)\n",
    "acc_out_notMNIST_KFAC, prob_correct_out_notMNIST_KFAC, ent_out_notMNIST_KFAC, MMC_out_notMNIST_KFAC, auroc_out_notMNIST_KFAC = get_out_dist_values(mnist_test_in_KFAC, mnist_test_out_notMNIST_KFAC, targets_notMNIST)\n",
    "acc_out_FMNISTn_KFAC, prob_correct_out_FMNISTn_KFAC, ent_out_FMNISTn_KFAC, MMC_out_FMNISTn_KFAC, auroc_out_FMNISTn_KFAC = get_out_dist_values(mnist_test_in_KFAC, mnist_test_out_FMNIST_normalized_KFAC, targets_FMNIST_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, KFAC, mnist] Accuracy: 0.986; average entropy: 0.994;     MMC: 0.714; Prob @ correct: 0.713\n",
      "[Out-fmnist, KFAC, mnist] Accuracy: 0.102; Average entropy: 2.087;    MMC: 0.244; AUROC: 0.991; Prob @ correct: 0.103\n",
      "[Out-notMNIST, KFAC, mnist] Accuracy: 0.125; Average entropy: 1.984;    MMC: 0.299; AUROC: 0.961; Prob @ correct: 0.109\n",
      "[Out-FMNISTn, KFAC, mnist] Accuracy: 0.097; Average entropy: 2.013;    MMC: 0.273; AUROC: 0.984; Prob @ correct: 0.102\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_KFAC, prob_correct_in_KFAC, ent_in_KFAC, MMC_in_KFAC, 'mnist', 'KFAC')\n",
    "print_out_dist_values(acc_out_FMNIST_KFAC, prob_correct_out_FMNIST_KFAC, ent_out_FMNIST_KFAC, MMC_out_FMNIST_KFAC, auroc_out_FMNIST_KFAC, test='fmnist', method='KFAC')\n",
    "print_out_dist_values(acc_out_notMNIST_KFAC, prob_correct_out_notMNIST_KFAC, ent_out_notMNIST_KFAC, MMC_out_notMNIST_KFAC, auroc_out_notMNIST_KFAC, test='notMNIST', method='KFAC')\n",
    "print_out_dist_values(acc_out_FMNISTn_KFAC, prob_correct_out_FMNISTn_KFAC, ent_out_FMNISTn_KFAC, MMC_out_FMNISTn_KFAC, auroc_out_FMNISTn_KFAC, test='FMNISTn', method='KFAC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet Laplace estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma\n",
    "\n",
    "def beta_function(alpha):\n",
    "    nom = np.prod([gamma(a_i) for a_i in alpha])\n",
    "    den = gamma(np.sum(alpha))\n",
    "    return(nom/den)\n",
    "\n",
    "def alphas_norm(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    return(alphas/alphas.sum(axis=1).reshape(-1,1))\n",
    "\n",
    "def alphas_variance(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    norm = alphas_norm(alphas)\n",
    "    nom = norm * (1 - norm)\n",
    "    den = alphas.sum(axis=1).reshape(-1,1) + 1\n",
    "    return(nom/den)\n",
    "\n",
    "def alpha_entropy(alpha):\n",
    "    K = len(alpha)\n",
    "    alpha = np.array(alpha)\n",
    "    B = np.log(beta_function(alpha))\n",
    "    print(\"B: \", B)\n",
    "    alpha_0 = np.sum(alpha)\n",
    "    C = (alpha_0 - K)*digamma(alpha_0)\n",
    "    print(\"C: \", C)\n",
    "    D = np.sum((alpha-1)*digamma(alpha))\n",
    "    print(\"D: \", D)\n",
    "    entropy = B + C - D\n",
    "    \n",
    "    return(np.array(entropy))\n",
    "        \n",
    "\n",
    "def alphas_log_prob(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    dig_sum = digamma(alphas.sum(axis=1).reshape(-1,1))\n",
    "    log_prob = digamma(alphas) - dig_sum\n",
    "    return(log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0067498e-08 1.5308382e-07 1.1374119e-06 5.5640821e-06 3.0374029e-08\n",
      " 2.6246235e-08 8.0688961e-10 9.9999177e-01 9.5726790e-08 1.1614281e-06] [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "B:  144.7332614500752\n",
      "C:  5.194942151142108\n",
      "D:  1379081000.0\n",
      "B:  22.52712651734206\n",
      "C:  5.194940984113796\n",
      "D:  93.81379446369971\n",
      "-1379080810.0717964 -66.09172696224385\n"
     ]
    }
   ],
   "source": [
    "#test the entropy\n",
    "x = mnist_test_in_DIR_LPA_MC[0]/mnist_test_in_DIR_LPA_MC[0].sum()\n",
    "x_test = np.ones(10)/10\n",
    "print(x, x_test)\n",
    "print(alpha_entropy(x), alpha_entropy(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_DIR_LPA_MC = predict_DIR_LPA(mnist_model, mnist_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, method='McKay').numpy()\n",
    "mnist_test_out_FMNIST_DIR_LPA_MC = predict_DIR_LPA(mnist_model, FMNIST_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, method='McKay').numpy()\n",
    "mnist_test_out_notMNIST_DIR_LPA_MC = predict_DIR_LPA(mnist_model, not_mnist_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, method='McKay').numpy()\n",
    "mnist_test_out_FMNIST_normalized_DIR_LPA_MC = predict_DIR_LPA(mnist_model, FMNIST_test_loader_normalized, M_W_post, M_b_post, U_post, V_post, B_post, method='McKay').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_DIR_LPA_MM = predict_DIR_LPA(mnist_model, mnist_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, method='MAP_match').numpy()\n",
    "mnist_test_out_FMNIST_DIR_LPA_MM = predict_DIR_LPA(mnist_model, FMNIST_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, method='MAP_match').numpy()\n",
    "mnist_test_out_notMNIST_DIR_LPA_MM = predict_DIR_LPA(mnist_model, not_mnist_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, method='MAP_match').numpy()\n",
    "mnist_test_out_FMNIST_normalized_DIR_LPA_MM = predict_DIR_LPA(mnist_model, FMNIST_test_loader_normalized, M_W_post, M_b_post, U_post, V_post, B_post, method='MAP_match').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_DIR_LPA_VM = predict_DIR_LPA(mnist_model, mnist_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, method='variance_match').numpy()\n",
    "mnist_test_out_FMNIST_DIR_LPA_VM = predict_DIR_LPA(mnist_model, FMNIST_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, method='variance_match').numpy()\n",
    "mnist_test_out_notMNIST_DIR_LPA_VM = predict_DIR_LPA(mnist_model, not_mnist_test_loader, M_W_post, M_b_post, U_post, V_post, B_post, method='variance_match').numpy()\n",
    "mnist_test_out_FMNIST_normalized_DIR_LPA_VM = predict_DIR_LPA(mnist_model, FMNIST_test_loader_normalized, M_W_post, M_b_post, U_post, V_post, B_post, method='variance_match').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs: 0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Infs: 0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "NaNs: 1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Infs: 1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "NaNs: 2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Infs: 2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "DIR_LPA_McKay = [mnist_test_in_DIR_LPA_MC, mnist_test_out_FMNIST_DIR_LPA_MC, mnist_test_out_notMNIST_DIR_LPA_MC, mnist_test_out_FMNIST_normalized_DIR_LPA_MC]\n",
    "DIR_LPA_MAP_match = [mnist_test_in_DIR_LPA_MM, mnist_test_out_FMNIST_DIR_LPA_MM, mnist_test_out_notMNIST_DIR_LPA_MM, mnist_test_out_FMNIST_normalized_DIR_LPA_MM]\n",
    "DIR_LPA_variance_match = [mnist_test_in_DIR_LPA_VM, mnist_test_out_FMNIST_DIR_LPA_VM, mnist_test_out_notMNIST_DIR_LPA_VM, mnist_test_out_FMNIST_normalized_DIR_LPA_VM]\n",
    "\n",
    "for i, y in enumerate([DIR_LPA_McKay, DIR_LPA_MAP_match, DIR_LPA_variance_match]):\n",
    "    print(\"NaNs: {}\".format(i))\n",
    "    for x in y:\n",
    "        print(np.sum(np.isnan(x)))\n",
    "    print(\"Infs: {}\".format(i))\n",
    "    for x in y:\n",
    "        print(np.sum(np.isinf(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#McKay\n",
    "mnist_test_in_DIR_LPA_MCn = mnist_test_in_DIR_LPA_MC/mnist_test_in_DIR_LPA_MC.sum(1).reshape(-1,1)\n",
    "mnist_test_out_FMNIST_DIR_LPA_MCn = mnist_test_out_FMNIST_DIR_LPA_MC/mnist_test_out_FMNIST_DIR_LPA_MC.sum(1).reshape(-1,1)\n",
    "mnist_test_out_notMNIST_DIR_LPA_MCn = mnist_test_out_notMNIST_DIR_LPA_MC/mnist_test_out_notMNIST_DIR_LPA_MC.sum(1).reshape(-1,1)\n",
    "mnist_test_out_FMNIST_normalized_DIR_LPA_MCn = mnist_test_out_FMNIST_normalized_DIR_LPA_MC/mnist_test_out_FMNIST_normalized_DIR_LPA_MC.sum(1).reshape(-1,1)\n",
    "#MAP match\n",
    "mnist_test_in_DIR_LPA_MMn = mnist_test_in_DIR_LPA_MM/mnist_test_in_DIR_LPA_MM.sum(1).reshape(-1,1)\n",
    "mnist_test_out_FMNIST_DIR_LPA_MMn = mnist_test_out_FMNIST_DIR_LPA_MM/mnist_test_out_FMNIST_DIR_LPA_MM.sum(1).reshape(-1,1)\n",
    "mnist_test_out_notMNIST_DIR_LPA_MMn = mnist_test_out_notMNIST_DIR_LPA_MM/mnist_test_out_notMNIST_DIR_LPA_MM.sum(1).reshape(-1,1)\n",
    "mnist_test_out_FMNIST_normalized_DIR_LPA_MMn = mnist_test_out_FMNIST_normalized_DIR_LPA_MM/mnist_test_out_FMNIST_normalized_DIR_LPA_MM.sum(1).reshape(-1,1)\n",
    "#variance match\n",
    "mnist_test_in_DIR_LPA_VMn = mnist_test_in_DIR_LPA_VM/mnist_test_in_DIR_LPA_VM.sum(1).reshape(-1,1)\n",
    "mnist_test_out_FMNIST_DIR_LPA_VMn = mnist_test_out_FMNIST_DIR_LPA_VM/mnist_test_out_FMNIST_DIR_LPA_VM.sum(1).reshape(-1,1)\n",
    "mnist_test_out_notMNIST_DIR_LPA_VMn = mnist_test_out_notMNIST_DIR_LPA_VM/mnist_test_out_notMNIST_DIR_LPA_VM.sum(1).reshape(-1,1)\n",
    "mnist_test_out_FMNIST_normalized_DIR_LPA_VMn = mnist_test_out_FMNIST_normalized_DIR_LPA_VM/mnist_test_out_FMNIST_normalized_DIR_LPA_VM.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, DIR_LPA_MC, mnist] Accuracy: 0.989; average entropy: 0.043;     MMC: 0.987; Prob @ correct: 0.981\n",
      "[Out-fmnist, DIR_LPA_MC, mnist] Accuracy: 0.100; Average entropy: 1.773;    MMC: 0.391; AUROC: 0.995; Prob @ correct: 0.103\n",
      "[Out-notMNIST, DIR_LPA_MC, mnist] Accuracy: 0.131; Average entropy: 1.022;    MMC: 0.657; AUROC: 0.938; Prob @ correct: 0.120\n",
      "[Out-FMNISTn, DIR_LPA_MC, mnist] Accuracy: 0.089; Average entropy: 0.463;    MMC: 0.832; AUROC: 0.878; Prob @ correct: 0.094\n"
     ]
    }
   ],
   "source": [
    "acc_in_DIR_LPA_MC, prob_correct_in_DIR_LPA_MC, ent_in_DIR_LPA_MC, MMC_in_DIR_LPA_MC = get_in_dist_values(mnist_test_in_DIR_LPA_MCn, targets)\n",
    "acc_out_FMNIST_DIR_LPA_MC, prob_correct_out_FMNIST_DIR_LPA_MC, ent_out_FMNIST_DIR_LPA_MC, MMC_out_FMNIST_DIR_LPA_MC, auroc_out_FMNIST_DIR_LPA_MC = get_out_dist_values(mnist_test_in_DIR_LPA_MCn, mnist_test_out_FMNIST_DIR_LPA_MCn, targets_FMNIST)\n",
    "acc_out_notMNIST_DIR_LPA_MC, prob_correct_out_notMNIST_DIR_LPA_MC, ent_out_notMNIST_DIR_LPA_MC, MMC_out_notMNIST_DIR_LPA_MC, auroc_out_notMNIST_DIR_LPA_MC = get_out_dist_values(mnist_test_in_DIR_LPA_MCn, mnist_test_out_notMNIST_DIR_LPA_MCn, targets_notMNIST)\n",
    "acc_out_FMNISTn_DIR_LPA_MC, prob_correct_out_FMNISTn_DIR_LPA_MC, ent_out_FMNISTn_DIR_LPA_MC, MMC_out_FMNISTn_DIR_LPA_MC, auroc_out_FMNISTn_DIR_LPA_MC = get_out_dist_values(mnist_test_in_DIR_LPA_MCn, mnist_test_out_FMNIST_normalized_DIR_LPA_MCn, targets_FMNIST_normalized)\n",
    "print_in_dist_values(acc_in_DIR_LPA_MC, prob_correct_in_DIR_LPA_MC, ent_in_DIR_LPA_MC, MMC_in_DIR_LPA_MC, 'mnist', 'DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_FMNIST_DIR_LPA_MC, prob_correct_out_FMNIST_DIR_LPA_MC, ent_out_FMNIST_DIR_LPA_MC, MMC_out_FMNIST_DIR_LPA_MC, auroc_out_FMNIST_DIR_LPA_MC, test='fmnist', method='DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_notMNIST_DIR_LPA_MC, prob_correct_out_notMNIST_DIR_LPA_MC, ent_out_notMNIST_DIR_LPA_MC, MMC_out_notMNIST_DIR_LPA_MC, auroc_out_notMNIST_DIR_LPA_MC, test='notMNIST', method='DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_FMNISTn_DIR_LPA_MC, prob_correct_out_FMNISTn_DIR_LPA_MC, ent_out_FMNISTn_DIR_LPA_MC, MMC_out_FMNISTn_DIR_LPA_MC, auroc_out_FMNISTn_DIR_LPA_MC, test='FMNISTn', method='DIR_LPA_MC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, DIR_LPA_MM, mnist] Accuracy: 0.989; average entropy: 0.041;     MMC: 0.988; Prob @ correct: 0.981\n",
      "[Out-fmnist, DIR_LPA_MM, mnist] Accuracy: 0.090; Average entropy: 1.363;    MMC: 0.531; AUROC: 0.991; Prob @ correct: 0.106\n",
      "[Out-notMNIST, DIR_LPA_MM, mnist] Accuracy: 0.129; Average entropy: 0.806;    MMC: 0.718; AUROC: 0.934; Prob @ correct: 0.122\n",
      "[Out-FMNISTn, DIR_LPA_MM, mnist] Accuracy: 0.089; Average entropy: 0.421;    MMC: 0.841; AUROC: 0.880; Prob @ correct: 0.094\n"
     ]
    }
   ],
   "source": [
    "acc_in_DIR_LPA_MM, prob_correct_in_DIR_LPA_MM, ent_in_DIR_LPA_MM, MMC_in_DIR_LPA_MM = get_in_dist_values(mnist_test_in_DIR_LPA_MMn, targets)\n",
    "acc_out_FMNIST_DIR_LPA_MM, prob_correct_out_FMNIST_DIR_LPA_MM, ent_out_FMNIST_DIR_LPA_MM, MMC_out_FMNIST_DIR_LPA_MM, auroc_out_FMNIST_DIR_LPA_MM = get_out_dist_values(mnist_test_in_DIR_LPA_MMn, mnist_test_out_FMNIST_DIR_LPA_MMn, targets_FMNIST)\n",
    "acc_out_notMNIST_DIR_LPA_MM, prob_correct_out_notMNIST_DIR_LPA_MM, ent_out_notMNIST_DIR_LPA_MM, MMC_out_notMNIST_DIR_LPA_MM, auroc_out_notMNIST_DIR_LPA_MM = get_out_dist_values(mnist_test_in_DIR_LPA_MMn, mnist_test_out_notMNIST_DIR_LPA_MMn, targets_notMNIST)\n",
    "acc_out_FMNISTn_DIR_LPA_MM, prob_correct_out_FMNISTn_DIR_LPA_MM, ent_out_FMNISTn_DIR_LPA_MM, MMC_out_FMNISTn_DIR_LPA_MM, auroc_out_FMNISTn_DIR_LPA_MM = get_out_dist_values(mnist_test_in_DIR_LPA_MMn, mnist_test_out_FMNIST_normalized_DIR_LPA_MMn, targets_FMNIST_normalized)\n",
    "print_in_dist_values(acc_in_DIR_LPA_MM, prob_correct_in_DIR_LPA_MM, ent_in_DIR_LPA_MM, MMC_in_DIR_LPA_MM, 'mnist', 'DIR_LPA_MM')\n",
    "print_out_dist_values(acc_out_FMNIST_DIR_LPA_MM, prob_correct_out_FMNIST_DIR_LPA_MM, ent_out_FMNIST_DIR_LPA_MM, MMC_out_FMNIST_DIR_LPA_MM, auroc_out_FMNIST_DIR_LPA_MM, test='fmnist', method='DIR_LPA_MM')\n",
    "print_out_dist_values(acc_out_notMNIST_DIR_LPA_MM, prob_correct_out_notMNIST_DIR_LPA_MM, ent_out_notMNIST_DIR_LPA_MM, MMC_out_notMNIST_DIR_LPA_MM, auroc_out_notMNIST_DIR_LPA_MM, test='notMNIST', method='DIR_LPA_MM')\n",
    "print_out_dist_values(acc_out_FMNISTn_DIR_LPA_MM, prob_correct_out_FMNISTn_DIR_LPA_MM, ent_out_FMNISTn_DIR_LPA_MM, MMC_out_FMNISTn_DIR_LPA_MM, auroc_out_FMNISTn_DIR_LPA_MM, test='FMNISTn', method='DIR_LPA_MM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, DIR_LPA_VM, mnist] Accuracy: 0.101; average entropy: 2.303;     MMC: 0.102; Prob @ correct: 0.100\n",
      "[Out-fmnist, DIR_LPA_VM, mnist] Accuracy: 0.100; Average entropy: 2.303;    MMC: 0.102; AUROC: 0.474; Prob @ correct: 0.100\n",
      "[Out-notMNIST, DIR_LPA_VM, mnist] Accuracy: 0.100; Average entropy: 2.303;    MMC: 0.101; AUROC: 0.811; Prob @ correct: 0.100\n",
      "[Out-FMNISTn, DIR_LPA_VM, mnist] Accuracy: 0.100; Average entropy: 2.303;    MMC: 0.100; AUROC: 0.989; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "acc_in_DIR_LPA_VM, prob_correct_in_DIR_LPA_VM, ent_in_DIR_LPA_VM, MMC_in_DIR_LPA_VM = get_in_dist_values(mnist_test_in_DIR_LPA_VMn, targets)\n",
    "acc_out_FMNIST_DIR_LPA_VM, prob_correct_out_FMNIST_DIR_LPA_VM, ent_out_FMNIST_DIR_LPA_VM, MMC_out_FMNIST_DIR_LPA_VM, auroc_out_FMNIST_DIR_LPA_VM = get_out_dist_values(mnist_test_in_DIR_LPA_VMn, mnist_test_out_FMNIST_DIR_LPA_VMn, targets_FMNIST)\n",
    "acc_out_notMNIST_DIR_LPA_VM, prob_correct_out_notMNIST_DIR_LPA_VM, ent_out_notMNIST_DIR_LPA_VM, MMC_out_notMNIST_DIR_LPA_VM, auroc_out_notMNIST_DIR_LPA_VM = get_out_dist_values(mnist_test_in_DIR_LPA_VMn, mnist_test_out_notMNIST_DIR_LPA_VMn, targets_notMNIST)\n",
    "acc_out_FMNISTn_DIR_LPA_VM, prob_correct_out_FMNISTn_DIR_LPA_VM, ent_out_FMNISTn_DIR_LPA_VM, MMC_out_FMNISTn_DIR_LPA_VM, auroc_out_FMNISTn_DIR_LPA_VM = get_out_dist_values(mnist_test_in_DIR_LPA_VMn, mnist_test_out_FMNIST_normalized_DIR_LPA_VMn, targets_FMNIST_normalized)\n",
    "print_in_dist_values(acc_in_DIR_LPA_VM, prob_correct_in_DIR_LPA_VM, ent_in_DIR_LPA_VM, MMC_in_DIR_LPA_VM, 'mnist', 'DIR_LPA_VM')\n",
    "print_out_dist_values(acc_out_FMNIST_DIR_LPA_VM, prob_correct_out_FMNIST_DIR_LPA_VM, ent_out_FMNIST_DIR_LPA_VM, MMC_out_FMNIST_DIR_LPA_VM, auroc_out_FMNIST_DIR_LPA_VM, test='fmnist', method='DIR_LPA_VM')\n",
    "print_out_dist_values(acc_out_notMNIST_DIR_LPA_VM, prob_correct_out_notMNIST_DIR_LPA_VM, ent_out_notMNIST_DIR_LPA_VM, MMC_out_notMNIST_DIR_LPA_VM, auroc_out_notMNIST_DIR_LPA_VM, test='notMNIST', method='DIR_LPA_VM')\n",
    "print_out_dist_values(acc_out_FMNISTn_DIR_LPA_VM, prob_correct_out_FMNISTn_DIR_LPA_VM, ent_out_FMNISTn_DIR_LPA_VM, MMC_out_FMNISTn_DIR_LPA_VM, auroc_out_FMNISTn_DIR_LPA_VM, test='FMNISTn', method='DIR_LPA_VM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphas_norm(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    return(alphas/alphas.sum(axis=1).reshape(-1,1))\n",
    "\n",
    "def alphas_variance(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    norm = alphas_norm(alphas)\n",
    "    nom = norm * (1 - norm)\n",
    "    den = alphas.sum(axis=1).reshape(-1,1) + 1\n",
    "    return(nom/den)\n",
    "\n",
    "def get_norm_variance_alphas(alphas):\n",
    "    eps = 10e-20\n",
    "    a = np.array(alphas)\n",
    "    a /= (alphas_variance(a) + eps)\n",
    "    a /= a.sum(1).reshape(-1,1)\n",
    "    return(a)\n",
    "\n",
    "def get_variance_alphas(alphas):\n",
    "    eps = 10e-20\n",
    "    a = np.array(alphas)\n",
    "    a /= (alphas_variance(a) + eps)\n",
    "    return(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marius/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/marius/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#McKay\n",
    "in_mnist_test_MC = get_norm_variance_alphas(mnist_test_in_DIR_LPA_MC)\n",
    "out_FMNIST_test_MC = get_norm_variance_alphas(mnist_test_out_FMNIST_DIR_LPA_MC)\n",
    "out_notMNIST_test_MC = get_norm_variance_alphas(mnist_test_out_notMNIST_DIR_LPA_MC)\n",
    "out_FMNIST_normalized_test_MC = get_norm_variance_alphas(mnist_test_out_FMNIST_normalized_DIR_LPA_MC)\n",
    "#MAP match\n",
    "in_mnist_test_MM = get_norm_variance_alphas(mnist_test_in_DIR_LPA_MM)\n",
    "out_FMNIST_test_MM = get_norm_variance_alphas(mnist_test_out_FMNIST_DIR_LPA_MM)\n",
    "out_notMNIST_test_MM = get_norm_variance_alphas(mnist_test_out_notMNIST_DIR_LPA_MM)\n",
    "out_FMNIST_normalized_test_MM = get_norm_variance_alphas(mnist_test_out_FMNIST_normalized_DIR_LPA_MM)\n",
    "#variance match\n",
    "in_mnist_test_VM = get_norm_variance_alphas(mnist_test_in_DIR_LPA_VM)\n",
    "out_FMNIST_test_VM = get_norm_variance_alphas(mnist_test_out_FMNIST_DIR_LPA_VM)\n",
    "out_notMNIST_test_VM = get_norm_variance_alphas(mnist_test_out_notMNIST_DIR_LPA_VM)\n",
    "out_FMNIST_normalized_test_VM = get_norm_variance_alphas(mnist_test_out_FMNIST_normalized_DIR_LPA_VM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs: 0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "Infs: 0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "NaNs: 1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Infs: 1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "NaNs: 2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Infs: 2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "DIR_LPA_var_McKay = [in_mnist_test_MC, out_FMNIST_test_MC, out_notMNIST_test_MC, out_FMNIST_normalized_test_MC]\n",
    "DIR_LPA_var_MAP_match = [in_mnist_test_MM, out_FMNIST_test_MM, out_notMNIST_test_MM, out_FMNIST_normalized_test_MM]\n",
    "DIR_LPA_var_variance_match = [in_mnist_test_VM, out_FMNIST_test_VM, out_notMNIST_test_VM, out_FMNIST_normalized_test_VM]\n",
    "\n",
    "for i, y in enumerate([DIR_LPA_var_McKay, DIR_LPA_var_MAP_match, DIR_LPA_var_variance_match]):\n",
    "    print(\"NaNs: {}\".format(i))\n",
    "    for x in y:\n",
    "        print(np.sum(np.isnan(x)))\n",
    "    print(\"Infs: {}\".format(i))\n",
    "    for x in y:\n",
    "        print(np.sum(np.isinf(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, var_DIR_LPA_MC, mnist] Accuracy: 0.989; average entropy: 0.196;     MMC: 0.952; Prob @ correct: 0.948\n",
      "[Out-fmnist, var_DIR_LPA_MC, mnist] Accuracy: 0.100; Average entropy: 2.228;    MMC: 0.182; AUROC: 0.994; Prob @ correct: 0.098\n",
      "[Out-notMNIST, var_DIR_LPA_MC, mnist] Accuracy: 0.131; Average entropy: 1.729;    MMC: 0.413; AUROC: 0.938; Prob @ correct: 0.109\n"
     ]
    }
   ],
   "source": [
    "acc_in_var_DIR_LPA_MC, prob_correct_in_var_DIR_LPA_MC, ent_in_var_DIR_LPA_MC, MMC_in_var_DIR_LPA_MC = get_in_dist_values(in_mnist_test_MC, targets)\n",
    "acc_out_FMNIST_var_DIR_LPA_MC, prob_correct_out_FMNIST_var_DIR_LPA_MC, ent_out_FMNIST_var_DIR_LPA_MC, MMC_out_FMNIST_var_DIR_LPA_MC, auroc_out_FMNIST_var_DIR_LPA_MC = get_out_dist_values(in_mnist_test_MC, out_FMNIST_test_MC, targets_FMNIST)\n",
    "acc_out_notMNIST_var_DIR_LPA_MC, prob_correct_out_notMNIST_var_DIR_LPA_MC, ent_out_notMNIST_var_DIR_LPA_MC, MMC_out_notMNIST_var_DIR_LPA_MC, auroc_out_notMNIST_var_DIR_LPA_MC = get_out_dist_values(in_mnist_test_MC, out_notMNIST_test_MC, targets_notMNIST)\n",
    "#acc_out_FMNISTn_var_DIR_LPA_MC, prob_correct_out_FMNISTn_var_DIR_LPA_MC, ent_out_FMNISTn_var_DIR_LPA_MC, MMC_out_FMNISTn_var_DIR_LPA_MC, auroc_out_FMNISTn_var_DIR_LPA_MC = get_out_dist_values(in_mnist_test_MC, out_FMNIST_normalized_test_MC, targets_FMNIST_normalized)\n",
    "print_in_dist_values(acc_in_var_DIR_LPA_MC, prob_correct_in_var_DIR_LPA_MC, ent_in_var_DIR_LPA_MC, MMC_in_var_DIR_LPA_MC, 'mnist', 'var_DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_FMNIST_var_DIR_LPA_MC, prob_correct_out_FMNIST_var_DIR_LPA_MC, ent_out_FMNIST_var_DIR_LPA_MC, MMC_out_FMNIST_var_DIR_LPA_MC, auroc_out_FMNIST_var_DIR_LPA_MC, test='fmnist', method='var_DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_notMNIST_var_DIR_LPA_MC, prob_correct_out_notMNIST_var_DIR_LPA_MC, ent_out_notMNIST_var_DIR_LPA_MC, MMC_out_notMNIST_var_DIR_LPA_MC, auroc_out_notMNIST_var_DIR_LPA_MC, test='notMNIST', method='var_DIR_LPA_MC')\n",
    "#print_out_dist_values(acc_out_FMNISTn_var_DIR_LPA_MC, prob_correct_out_FMNISTn_var_DIR_LPA_MC, ent_out_FMNISTn_var_DIR_LPA_MC, MMC_out_FMNISTn_var_DIR_LPA_MC, auroc_out_FMNISTn_var_DIR_LPA_MC, test='FMNISTn', method='var_DIR_LPA_MC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, var_DIR_LPA_MM, mnist] Accuracy: 0.989; average entropy: 0.191;     MMC: 0.953; Prob @ correct: 0.949\n",
      "[Out-fmnist, var_DIR_LPA_MM, mnist] Accuracy: 0.090; Average entropy: 2.181;    MMC: 0.227; AUROC: 0.991; Prob @ correct: 0.099\n",
      "[Out-notMNIST, var_DIR_LPA_MM, mnist] Accuracy: 0.129; Average entropy: 1.673;    MMC: 0.447; AUROC: 0.934; Prob @ correct: 0.109\n",
      "[Out-FMNISTn, var_DIR_LPA_MM, mnist] Accuracy: 0.089; Average entropy: 1.288;    MMC: 0.602; AUROC: 0.873; Prob @ correct: 0.089\n"
     ]
    }
   ],
   "source": [
    "acc_in_var_DIR_LPA_MM, prob_correct_in_var_DIR_LPA_MM, ent_in_var_DIR_LPA_MM, MMC_in_var_DIR_LPA_MM = get_in_dist_values(in_mnist_test_MM, targets)\n",
    "acc_out_FMNIST_var_DIR_LPA_MM, prob_correct_out_FMNIST_var_DIR_LPA_MM, ent_out_FMNIST_var_DIR_LPA_MM, MMC_out_FMNIST_var_DIR_LPA_MM, auroc_out_FMNIST_var_DIR_LPA_MM = get_out_dist_values(in_mnist_test_MM, out_FMNIST_test_MM, targets_FMNIST)\n",
    "acc_out_notMNIST_var_DIR_LPA_MM, prob_correct_out_notMNIST_var_DIR_LPA_MM, ent_out_notMNIST_var_DIR_LPA_MM, MMC_out_notMNIST_var_DIR_LPA_MM, auroc_out_notMNIST_var_DIR_LPA_MM = get_out_dist_values(in_mnist_test_MM, out_notMNIST_test_MM, targets_notMNIST)\n",
    "acc_out_FMNISTn_var_DIR_LPA_MM, prob_correct_out_FMNISTn_var_DIR_LPA_MM, ent_out_FMNISTn_var_DIR_LPA_MM, MMC_out_FMNISTn_var_DIR_LPA_MM, auroc_out_FMNISTn_var_DIR_LPA_MM = get_out_dist_values(in_mnist_test_MM, out_FMNIST_normalized_test_MM, targets_FMNIST_normalized)\n",
    "print_in_dist_values(acc_in_var_DIR_LPA_MM, prob_correct_in_var_DIR_LPA_MM, ent_in_var_DIR_LPA_MM, MMC_in_var_DIR_LPA_MM, 'mnist', 'var_DIR_LPA_MM')\n",
    "print_out_dist_values(acc_out_FMNIST_var_DIR_LPA_MM, prob_correct_out_FMNIST_var_DIR_LPA_MM, ent_out_FMNIST_var_DIR_LPA_MM, MMC_out_FMNIST_var_DIR_LPA_MM, auroc_out_FMNIST_var_DIR_LPA_MM, test='fmnist', method='var_DIR_LPA_MM')\n",
    "print_out_dist_values(acc_out_notMNIST_var_DIR_LPA_MM, prob_correct_out_notMNIST_var_DIR_LPA_MM, ent_out_notMNIST_var_DIR_LPA_MM, MMC_out_notMNIST_var_DIR_LPA_MM, auroc_out_notMNIST_var_DIR_LPA_MM, test='notMNIST', method='var_DIR_LPA_MM')\n",
    "print_out_dist_values(acc_out_FMNISTn_var_DIR_LPA_MM, prob_correct_out_FMNISTn_var_DIR_LPA_MM, ent_out_FMNISTn_var_DIR_LPA_MM, MMC_out_FMNISTn_var_DIR_LPA_MM, auroc_out_FMNISTn_var_DIR_LPA_MM, test='FMNISTn', method='var_DIR_LPA_MM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot for accuracy\n",
    "labels=['In_dist_MNIST', 'Out_dist_FMNIST', 'Out_dist_notMNIST', 'Out_dist_FMNISTnorm']\n",
    "#labels = ['MAP', 'Diag', 'KFAC', 'DIR_LPA_MC', 'DIR_LPA_MM', 'DIR_LPA_VM', 'var_DIR_LPA_MC', 'var_DIR_LPA_MM']\n",
    "MAP_accuracies = np.array([acc_in_MAP, acc_out_FMNIST_MAP, acc_out_notMNIST_MAP, acc_out_FMNISTn_MAP])\n",
    "Diag_accuracies = np.array([acc_in_D, acc_out_FMNIST_D, acc_out_notMNIST_D, acc_out_FMNISTn_D])\n",
    "KFAC_accuracies = np.array([acc_in_KFAC, acc_out_FMNIST_KFAC, acc_out_notMNIST_KFAC, acc_out_FMNISTn_KFAC])\n",
    "DIR_LPA_MC_accuracies = np.array([acc_in_DIR_LPA_MC, acc_out_FMNIST_DIR_LPA_MC, acc_out_notMNIST_DIR_LPA_MC, acc_out_FMNISTn_DIR_LPA_MC])\n",
    "DIR_LPA_MM_accuracies = np.array([acc_in_DIR_LPA_MM, acc_out_FMNIST_DIR_LPA_MM, acc_out_notMNIST_DIR_LPA_MM, acc_out_FMNISTn_DIR_LPA_MM])\n",
    "DIR_LPA_VM_accuracies = np.array([acc_in_DIR_LPA_VM, acc_out_FMNIST_DIR_LPA_VM, acc_out_notMNIST_DIR_LPA_VM, acc_out_FMNISTn_DIR_LPA_VM])\n",
    "var_DIR_LPA_MC_accuracies = np.array([acc_in_var_DIR_LPA_MC, acc_out_FMNIST_var_DIR_LPA_MC, acc_out_notMNIST_var_DIR_LPA_MC, 0])\n",
    "var_DIR_LPA_MM_accuracies = np.array([acc_in_var_DIR_LPA_MM, acc_out_FMNIST_var_DIR_LPA_MM, acc_out_notMNIST_var_DIR_LPA_MM, acc_out_FMNISTn_var_DIR_LPA_MM])\n",
    "\n",
    "X = np.vstack((MAP_accuracies, Diag_accuracies, KFAC_accuracies , DIR_LPA_MC_accuracies, DIR_LPA_MM_accuracies, DIR_LPA_VM_accuracies, var_DIR_LPA_MC_accuracies, var_DIR_LPA_MM_accuracies))\n",
    "In_dist_accuracies = X[:,0]\n",
    "Out_dist_accuracies_FMNIST = X[:,1]\n",
    "Out_dist_accuracies_notMNIST = X[:,2]\n",
    "Out_dist_accuracies_FMNISTnorm = X[:,3]\n",
    "\n",
    "width = 0.10  # the width of the bars\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 5))\n",
    "ax.bar(x - 4*width, MAP_accuracies, width, label='MAP_accuracies')\n",
    "ax.bar(x - 3*width, Diag_accuracies, width, label='Diag_accuracies')\n",
    "ax.bar(x - 2*width, KFAC_accuracies, width, label='KFAC_accuracies')\n",
    "ax.bar(x - 1*width, DIR_LPA_MC_accuracies, width, label='DIR_LPA_MC_accuracies')\n",
    "ax.bar(x, DIR_LPA_MM_accuracies, width, label='DIR_LPA_MM_accuracie')\n",
    "ax.bar(x + 1*width, DIR_LPA_VM_accuracies, width, label='DIR_LPA_VM_accuracies')\n",
    "ax.bar(x + 2*width, var_DIR_LPA_MC_accuracies, width, label='var_DIR_LPA_MC_accuracies')\n",
    "ax.bar(x + 3*width, var_DIR_LPA_MM_accuracies, width, label='var_DIR_LPA_MM_accuracies')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('accuracy')\n",
    "plt.title('Accuracies of all methods in comparison')\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig('results_bar_plot.jpg')\n",
    "#plt.savefig('results_bar_plot.pdf')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABXEAAAFOCAYAAADAcl5eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xt4VNXZ///3SoKcDJQzCKQhUEqMCWmMRi1YFFsIJwWBJK2I4NdCgT4oBAERDVZQkCpYsPwQ+6CgkxgqUAOo4fSIYEEOEYZGhcYgUeSkYhBiE1i/P2YyTcgkkwBhAnxe1zWX2Wutve577xmV3KxZ21hrEREREREREREREZGaKcDfCYiIiIiIiIiIiIhI+VTEFREREREREREREanBVMQVERERERERERERqcFUxBURERERERERERGpwVTEFREREREREREREanBVMQVERERERERERERqcFUxBURERERn4zL/xpjvjXGbDuP83ONMXe5f04xxiy9+FlWKo8HjDEfXKS5uhlj8s7jvAXGmKkXI4fLgTEmxBhz0hgT6O9cRERERC5XKuKKiIiIXELuYuZ/jDFNz2nPMsZYY0yo+3ix+7jfOePmuNsfKNHWyhjzijHmkDEm3xjziTFmmjGm/kVMvQvwa6CNtfbmizhvtTHGhLrvVZC/cynJWjvSWvsnf+dxqVhrv7DWXmutPePvXEREREQuVyriioiIiFx6nwNJxQfGmEigrpdxnwFDS4wLAgYB/y7R1hj40H3+rdbaYFzF1p8A7S9izj8Fcq21P1zEOeUKV9MK6CIiIiKXKxVxRURERC69JcD9JY6HAq95Gfc28EtjTCP3cU9gN/B1iTHjgHzgPmttLoC19qC1dqy1drd7G4QXjDFHjDEnjDG7jTE3eEvKGHOdMeYfxphvjDH7jTEPudsfBBYBt7q/Fj/Ny7ntjTHrjTHHjTHHjDGvG2N+UpWb4p6nmzEmzxjzqDvnQ8aYe4wxvYwxn7lze6zE+ABjzCRjzL/dsd90F7YB3nf/8zt33reWOG+2e2uIz40x8b7ugbuvrnuF9LfGmH8BN52T+0RjzJfu1dCfGmO6l3ONi40xT59zveNLXO+wCu5PY/e2Fl+581hRou8hd87fuK/huhJ91hgzyhizz53fn9zv2YfGmO/d9+2ac3J6zP1e5hpjfldirt7GmF3u8w4aY1JK9BWvfn7QGPMFsP7cFdHGtaVFjjuPz4vndr+XjxtjDrjvxWvGmIbnzDvUGPOFO68p5d0nERERkSuNirgiIiIil94/gQbGmHDj2ic0AfC2R2wB8A8g0X18P2WLvXcBb1lrz5YT6zfA7UBHXKtzE4Dj5Yx1AHnAdcBAYIYxpru19hVgJPCh+2vxT3o51wDPuM8NB9oCKeXE8aUlUAdoDTwBvAzcB9wIdAWeMMaEucf+D3AP8Ct37G+B+e6+293//Ik77w/dx3HAp0BTYBbwijHGVHQP3H1P4lrd3B7oQelV0j8HxgA3uVdD9wByq3C9Dd3X+yAw3/y3cH+uJUA9IAJoDrzgjn8nrvs/GGgFHABSzzm3J657eAvwKLAQ+B2u9+oGSqwOd+fU1J3TUGCh+xoBfsD1WfwJ0Bv4gzHmnnNi/QrX56BHyUbj2uLjRSDefZ9uA7Lc3Q+4X3cAYcC1wLxz5u0C/BzojutzEO7lHomIiIhccVTEFREREfGP4tW4vwY+Ab4sZ9xrwP3uFYm/Alac098EOFRBnEIgGOgEGGtttrW2zHhjTFtcBbKJ1toCa20WrtW3QypzMdba/dbaTGvtj9bao8Dz7nzPRyEw3VpbiKsQ2RSYa63Nt9buBfYCUe6xI4Ap1to8a+2PuArHA03FX+M/YK192b1H66u4ip4tKnEPBrvz+sZaexBXMbLYGaA2cL0xppa1Ntda+28qpxB4ylpbaK1dDZzEVagsxRjTCogHRlprv3WP/z939++Av1lrd7rvw2RcK6dDS0wx01r7vfseOoH3rLU51toTwBrgF+eEnOp+P/8PWOW+fqy1G621e6y1Z621u3EVvs99r1OstT9Ya097ud6zwA3GmLrW2kPufIqv4Xl3Tifd15B4zns5zVp72lr7MfAx0NnL/CIiIiJXHBVxRURERPxjCfBbXCsPvW2lAIC19gOgGfA4kOGlKHYcVxGyvPPX41rNOB84bIxZaIxp4GXodcA31tr8Em0HcK3E9MkY09wYk+reTuB7XCuLm/o6rxzHSzwEq/h6D5foP41rlSa49updboz5zhjzHZCNq6DaooL5PdtRWGtPuX+8Ft/34Drg4Dl9xfPsBx7GVUQ+4r4X11E5x621RSWOT/Hf6yuprTu/b730XXdOPidxfTZKvn/n3sPy7inAt+fsf3zAHQNjTJwxZoMx5qgx5gSuVdrnvtcH8cI9Z4L7nEPGmFXGmE7ersH9cxCl38uSW4mUd59ERERErjgq4oqIiIj4gbX2AK4HnPUC3vIxfCkwHu/F3rVAf2NMuX+us9a+aK29EddX8DsCE7wM+wpobIwJLtEWQvkrhM/1DGCBKGttA1zbH5iKT7koDuL6av5PSrzqWGu/dOdTFb7uwSFchdSSfR7W2jestV1wFZYtMLOK8X056M7P217DX7njAp5tC5pQ+ffvXI3ccxQLcccAeAPXNh9trbUNgQWUfa/LvffW2nettb/G9ZcPn+DaLqPMNbhjFlG62CwiIiJyVVIRV0RERMR/HgTuPGfFozcv4tp24X0vfc8DDYBXjTE/BTDGtDbGPG+MiTLG3OReOVkL116mBbhWqpbi3h5gC/CMMaaOMSbKnd/rlbyWYFzbAHxnjGmN90JxdVgATC9x7c2MMXe7+47i+up+WHknl1SJe/AmMNkY08gY0wb4Y/G5xpifG2PuNMbUxnWPT+PlPl8I9zYYa4CX3DnUMsYU7/v7BjDMGBPtzmEGsLX4YXfnaZox5hpjTFegD5Dubg/GtSK4wBhzM64V5ZVijGlhjOnnLhD/iOszU3yfHMAjxph2xphr3deQds4qZREREZGrkoq4IiIiIn5irf23tXZ7JcZ9Y61dZ60ts7rRWvsNrodDFQJbjTH5wDrgBLAfV4H3ZVwP/DqA6yv2s8sJlQSE4loRuRx40lqbWcnLmQbEuOOuwvfq4otlLq5Voe+5r/2fuB5cVrxVwnRgs3u7hVsqMV9F92Aarnv4OfAeri0xitUGngWO4frKf3PgsQu6Mu+G4HqvPwGO4NrCAWvtOmAq8HdcK4bb898H4p2Pr3F9Zr7CVcQeaa39xN03CnjKfb+fwFXcrqwAXKvKvwK+wbWX7ih3399w3dP3cd3jAkoUykVERESuZsbL7wIiIiIiInKVMsZ0A5Zaa9v4OxcRERERcdFKXBEREREREREREZEaTEVcERERERERERERkRpM2ymIiIiIiIiIiIiI1GBaiSsiIiIiIiIiIiJSg6mIKyIiIiIiIiIiIlKDBfk7gapq2rSpDQ0N9XcaIiIiIiIiIiIiIhdkx44dx6y1zXyNu+yKuKGhoWzfvt3faYiIiIiIiIiIiIhcEGPMgcqM03YKIiIiIiIiIiIiIjWYirgiIiIiIiIiIiIiNZiKuCIiIiIiIiIiIiI12GW3J66IiIiIiIiIiEh1KywsJC8vj4KCAn+nIleAOnXq0KZNG2rVqnVe51dbEdcY8zegD3DEWnuDl34DzAV6AaeAB6y1O6srHxERERERERERkcrKy8sjODiY0NBQXGUskfNjreX48ePk5eXRrl2785qjOrdTWAz0rKA/HviZ+/V74K/VmIuIiIiIiIiIiEilFRQU0KRJExVw5YIZY2jSpMkFrequtiKutfZ94JsKhtwNvGZd/gn8xBjTqrryERERERERERERqQoVcOViudDPkj8fbNYaOFjiOM/dJiIiIiIiIiIiIiJu/nywmbfys/U60Jjf49pygZCQkOrMSUREREREREREpIzQSasu6ny5z/b2OcYYw3333ceSJUsAKCoqolWrVsTFxZGRkeEZd/fdd3PkyBE+/PBDT1tKSgovv/wyzZo1o6ioiBkzZtCvXz+vcVJSUpg2bRr79u2jQ4cOALzwwguMGzeOjz76iNjYWEJDQ2nbti2bNm3ynBcdHU1RURFOpxOAbdu2kZyczOHDhzHG0KVLF1588UXq1atX9RskpfhzJW4e0LbEcRvgK28DrbULrbWx1trYZs2aXZLkRERERERERERE/Kl+/fo4nU5Onz4NQGZmJq1bl/4i+3fffcfOnTv57rvv+Pzzz0v1PfLII2RlZZGens7w4cM5e/ZsubEiIyNJTU31HC9btozrr7++1Jj8/HwOHnR9sT47O7tU3+HDhxk0aBAzZ87k008/JTs7m549e5Kfn1/1C5cy/FnE/Qdwv3G5BThhrT3kx3xERERERERERERqlPj4eFatcq0CdjgcJCUller/+9//Tt++fUlMTCxVhC0pPDycoKAgjh07Vm6ce+65h5UrVwKQk5NDw4YNOXcx5eDBg0lLS/Oay/z58xk6dCi33nor4FpFPHDgQFq0aFHFKxZvqm07BWOMA+gGNDXG5AFPArUArLULgNVAL2A/cAoYVl25XO4u9nL9qqrM8v4rWXancL/GX99tvl/jj15wp1/ji4iIiIiIiFzNEhMTeeqpp+jTpw+7d+9m+PDhpbY0cDgcPPnkk7Ro0YKBAwcyefLkMnNs3bqVgICAMkXZkho0aEDbtm1xOp2sXLmShIQE/vd//7fUmIEDB/LAAw+QnJzM22+/zeuvv+7Z6sHpdDJ06NCLdNVyrmor4lprk3z0W2B0dcUXERERERERERG53EVFRZGbm4vD4aBXr16l+g4fPsz+/fvp0qULxhiCgoJwOp3ccMMNgGtf26VLlxIcHExaWhrGeHtE1X8Vr+Z99913WbduXZkibuPGjWnUqBGpqamEh4drr9tLyJ/bKYiIiIiIiIiIiIgP/fr1Izk5ucxWCmlpaXz77be0a9eO0NBQcnNzS22pULwn7qZNm+jatavPOH379mXJkiWEhITQoEEDr2MSEhIYPXp0mVwiIiLYsWPHeVydVEa1rcQVERERERERERGRCzd8+HAaNmxIZGQkGzdu9LQ7HA7eeecdzz60n3/+Ob/+9a95+umnzytO3bp1mTlzJh07dix3TP/+/Tl06BA9evTgq6++8rSPGTOGm2++md69exMXFwfA0qVLueuuu2jZsuV55SP/pSKuiIiIiIiIiIiID/58ZlCbNm0YO3Zsqbbc3Fy++OILbrnlFk9bu3btaNCgAVu3bj3vWImJiRX2BwcHM3HixDLtLVq0IDU1leTkZI4cOUJAQAC33347AwYMOO9c5L+Ma2vay0dsbKzdvn27v9O4pPRgM//Sg830YDMRERERERG5+mRnZxMe7t+agFxZvH2mjDE7rLWxvs7VnrgiIiIiIiIiIiIiNZi2UxAREREREREREbkKTJ8+nfT09FJtgwYNYsqUKX7KSCpLRVwREREREREREZGrwJQpU1SwvUypiCsiNVrepE1+jb+ozjq/xk9JSfFrfBERERERERHxP+2JKyIiIiIiIiIiIlKDqYgrIiIiIiIiIiIiUoOpiCsiIiIiIiIiIiJSg2lPXBEREREREREREV9SGl7k+U74HBIYGEhkZCSFhYUEBQUxdOhQHn74YQICAti+fTuvvfYaL7744kVLKTQ0lLZt27Jp03+fTxMdHU1RURFOp5ONGzdyxx13sGjRIh588EEAdu3aRUxMDM899xzJyckAzJ49m0WLFhEUFERgYCDjx4/n/vvvv2h5Xo20EldERERERERERKQGqlu3LllZWezdu5fMzExWr17NtGnTAIiNjb2oBdxi+fn5HDx4EIDs7Owy/ZGRkaSlpXmOU1NT6dy5s+d4wYIFZGZmsm3bNpxOJ++//z7W2oue59VGRVwREREREREREZEarnnz5ixcuJB58+ZhrWXjxo306dMHgG3btnHbbbfxi1/8gttuu41PP/0UgFOnTjF48GCioqJISEggLi6O7du3Vxhn8ODBniKtw+EgKSmpVH9ISAgFBQUcPnwYay3vvPMO8fHxnv4ZM2bw0ksv0aBBAwAaNmzI0KFDL9p9uFqpiCsiIiIiIiIiInIZCAsL4+zZsxw5cqRUe6dOnXj//ffZtWsXTz31FI899hgAL730Eo0aNWL37t1MnTqVHTt2+IwxcOBA3nrrLQDefvtt+vbt63VMeno6W7ZsISYmhtq1awOuVbz5+fm0b9/+Qi9VzqE9cUVERERERERERC4T3rYmOHHiBEOHDmXfvn0YYygsLATggw8+YOzYsQDccMMNREVF+Zy/cePGNGrUiNTUVMLDw6lXr16ZMYMHDyYhIYFPPvmEpKQktmzZ4snNGHMhlyfl0EpcERERERERERGRy0BOTg6BgYE0b968VPvUqVO54447cDqdvP322xQUFADeC76VkZCQwOjRo8tspVCsZcuW1KpVi8zMTLp37+5pb9CgAfXr1ycnJ+e84kr5VMQVERERERERERGp4Y4ePcrIkSMZM2ZMmdWuJ06coHXr1gAsXrzY096lSxfefPNNAP71r3+xZ8+eSsXq378/jz76KD169Ch3zFNPPcXMmTMJDAws1T558mRGjx7N999/D8D333/PwoULKxVXyqftFERERERERERERHxJOXHJQ54+fZro6GgKCwsJCgpiyJAhjBs3rsy4Rx99lKFDh/L8889z5513etpHjRrF0KFDiYqK4he/+AVRUVE0bNjQZ9zg4GAmTpxY4ZjbbrvNa/sf/vAHTp48yU033UStWrWoVasW48eP9xlTKmbOd1m1v8TGxlpfT9G70oROWuXX+Ll1fuvX+P74j2RJ2Z3C/Rp/fbf5fo0/esGdvgdVo7xJm/waf1GddX6Nn5KS4tf4IiIiIiIiV6vs7GzCw/1bE7hQZ86cobCwkDp16vDvf/+b7t2789lnn3HNNdf4O7WrkrfPlDFmh7U21te5WokrIiIiIiIiIiJyBTp16hR33HEHhYWFWGv561//qgLuZUpFXBERERERERERkStQcHAw3r7RHhcXx48//liqbcmSJURGRl6q1KSKVMQVERERERERERG5imzdutXfKUgVBfg7AREREREREREREREpn4q4IiIiIiIiIiIiIjWYirgiIiIiIiIiIiIiNZiKuCIiIiIiIiIiIiI1mB5sJiIiIiIiIiIi4kPkq5EXdb49Q/f4HHPttddy8uRJAFavXs3YsWNZt24df/vb33j55Zdp1qwZAD179uTZZ58F4OjRo1x33XXMmzePESNGeOY6efIk48ePZ+3atdSpU4cmTZrw3HPPERcX5zW2MYb77ruPJUuWAFBUVESrVq2Ii4sjIyODxYsXM2zYMNauXUv37t0BWL58OQMGDCA9PZ2BAwdSWFjI1KlT+fvf/07t2rWpV68e06ZNIz4+/vxv3FVKRVwREREREREREZEabN26dfzxj3/kvffeIyQkBIBHHnmE5OTkMmPT09O55ZZbcDgcpYq4/+///T/atWvHvn37CAgIICcnh+zs7HJj1q9fH6fTyenTp6lbty6ZmZm0bt261JjIyEgcDoeniJuamkrnzp09/VOnTuXQoUM4nU5q167N4cOH+b//+78LuhdXK22nICIiIiIiIiIiUkNt2rSJhx56iFWrVtG+fXuf4x0OB3/+85/Jy8vjyy+/BODf//43W7du5emnnyYgwFUODAsLo3fv3hXOFR8fz6pVqzzzJiUllerv2rUr27Zto7CwkJMnT7J//36io6MBOHXqFC+//DJ/+ctfqF27NgAtWrRg8ODBVbsBAqiIKyIiIiIiIiIiUiP9+OOP3H333axYsYJOnTqV6nvhhReIjo4mOjqad999F4CDBw/y9ddfc/PNNzN48GDS0tIA2Lt3L9HR0QQGBlYpfmJiIqmpqRQUFLB79+4yWy8YY7jrrrt49913WblyJf369fP07d+/n5CQEBo0aHA+ly7nUBFXRERERERERESkBqpVqxa33XYbr7zySpm+Rx55hKysLLKysujRowfg2s6geKVrYmIiDofjguJHRUWRm5uLw+GgV69eXscUF3pTU1PLrNSVi0dFXBERERERERERkRooICCAN998k48++ogZM2b4HO9wOFi8eDGhoaH069ePjz/+mH379hEREcHHH3/M2bNnq5xDv379SE5OLrdAe/PNN+N0Ojl27BgdO3b0tHfo0IEvvviC/Pz8KseUslTEFRERERERERERqaHq1atHRkYGr7/+utcVucU+/fRTfvjhB7788ktyc3PJzc1l8uTJpKam0r59e2JjY3nyySex1gKwb98+Vq5c6TP+8OHDeeKJJ4iMjCx3zDPPPFOmyFyvXj0efPBB/ud//of//Oc/ABw6dIilS5dW5rLlHEH+TkBERERERERERKSm2zN0j99iN27cmHfeeYfbb7+dpk2beh3jcDjo379/qbZ7772XxMREpk6dyqJFixg/fjwdOnSgXr16NGnShOeee85n7DZt2jB27NgKx8THx3ttf/rpp3n88ce5/vrrqVOnDvXr1+epp57yGVPKMsXV98tFbGys3b59u7/TuKRCJ63ya/zcOr/1a3xSTvg1fHancL/GX99tvl/jj15wp1/j503a5Nf4i+qs82v8lJQUv8YXERERERG5WmVnZxMe7t+agFxZvH2mjDE7rLWxvs7VSlwRERERERERqTH8vpDp2d5+jX+1mz9yvV/j+3shk0h5VMQVERERERERERG5Ch0/fpzu3buXaV+3bh1NmjTxQ0ZSHhVxRURERERERERErkJNmjQhKyvL32lIJaiIKyIiIiIiIiIigP+fC4OfnwsjUlMF+DsBERERERERERERESmfirgiIiIiIiIiIiIiNZiKuCIiIiIiIiIiIiI1WLUWcY0xPY0xnxpj9htjJnnpDzHGbDDG7DLG7DbG9KrOfERERERERERERM5Hdqfwi/qqjMDAQKKjo4mIiKBz5848//zznD17FoCNGzfSp08fABYvXkyzZs2Ijo6mU6dOvPDCCxXOm5KSwuzZs8uNd8MNNzBo0CBOnTrl6Vu+fDnGGD755JMK587NzcUYw9SpUz1tx44do1atWowZM8bT9tprr3HDDTcQERHB9ddf7zWfYg888AD16tUjPz/f0zZ27FiMMRw7dgwAYwxDhgzx9BcVFdGsWTPPPQJYs2YNsbGxhIeH06lTJ5KTkyu8lpqk2oq4xphAYD4QD1wPJBljrj9n2OPAm9baXwCJwEvVlY+IiIiIiIiIiMjlpG7dumRlZbF3714yMzNZvXo106ZN8zo2ISGBrKwsNm/ezPTp0zl48OB5x3M6nVxzzTUsWLDA0+dwOOjSpQupqak+5wkLCyMjI8NznJ6eTkREhOd4zZo1zJkzh/fee4+9e/eyc+dOGjZsWOGcHTp0YOXKlQCcPXuWDRs20Lp1a09//fr1cTqdnD59GoDMzMxS/U6nkzFjxrB06VKys7NxOp2EhYX5vJaaojpX4t4M7LfW5lhr/wOkAnefM8YCDdw/NwS+qsZ8RERERERERERELkvNmzdn4cKFzJs3D2ttueOaNGlChw4dOHTo0AXF69q1K/v37wfg5MmTbN68mVdeeaVSRdy6desSHh7O9u3bAUhLS2Pw4MGe/meeeYbZs2dz3XXXAVCnTh0eeuihCudMSkoiLS0NcK1C/uUvf0lQUFCpMfHx8axatQpwFZ2TkpI8fbNmzWLKlCl06tQJgKCgIEaNGuXzWmqK6izitgZKlvzz3G0lpQD3GWPygNXAH6sxHxERERERERERkctWWFgYZ8+e5ciRI+WO+eKLLygoKCAqKuq84xQVFbFmzRoiIyMBWLFiBT179qRjx440btyYnTt3+pwjMTGR1NRU8vLyCAwM9BRswbUq9sYbb6xSTj/72c84evQo3377LQ6Hg8TExHJjFhQUsHv3buLi4i4oZk1SnUVc46Xt3L8mSAIWW2vbAL2AJcaYMjkZY35vjNlujNl+9OjRakhVRERERERERESk5itvFW5aWhoRERGEhYUxduxY6tSpU+W5T58+TXR0NLGxsYSEhPDggw8ClCqaJiYm4nA4fM7Vs2dPMjMzcTgcJCQkVDkXbwYMGEBqaipbt26la9euZfqjoqLIzc3F4XDQq9eV9eitIN9Dzlse0LbEcRvKbpfwINATwFr7oTGmDtAUKPXXCdbahcBCgNjY2PLXi4uIiIiIiIiIiFyhcnJyCAwMpHnz5mRnZ5fqS0hIYN68eXz44Yf07t2b+Ph4WrZsWaX5i/fELen48eOsX78ep9OJMYYzZ85gjGHWrFkY420Np8s111zDjTfeyJ///Gf27t3L22+/7emLiIhgx44d3HnnnVXKLzExkZiYGIYOHUpAgPe1qf369SM5OZmNGzdy/PjxMjE7d+5cpZg1RXWuxP0I+Jkxpp0x5hpcDy77xzljvgC6AxhjwoE6gJbaioiIiIiIiIiIlHD06FFGjhzJmDFjKiye3nrrrQwZMoS5c+delLjLli3j/vvv58CBA+Tm5nLw4EHatWvHBx984PPc8ePHM3PmTJo0aVKqffLkyTz66KN8/fXXAPz444+8+OKLPucLCQlh+vTpFe5lO3z4cJ544gnPVhDFJkyYwIwZM/jss88A18PRnn/+eZ8xa4pqW4lrrS0yxowB3gUCgb9Za/caY54Ctltr/wGMB142xjyCa6uFB2xFOzOLiIiIiIiIiIj4Qfgn2b4HXWTF2xsUFhYSFBTEkCFDGDdunM/zJk6cSExMDI899hjBwcFexzz99NPMmTPHc5yXl+d1nMPhYNKkSaXa7r33Xt544w2vWxqUFBERQURERJn2Xr16cfjwYe666y6stRhjGD58uK/LAmDEiBEV9rdp04axY8eWaY+KimLOnDkkJSVx6tQpjDH07t27UjFrAnO51UxjY2Nt8ZPtrhahk1b5NX5und/6NT4pJ/waPrtTuF/jr+8236/xRy+o2lcbLra8SZv8Gn9RnXV+jZ+SkuLX+CIiIiIil5rffwd+9vIp6lQH/Q7839+Bs7OzCQ/37/2QK4u3z5QxZoe1NtbXudW5nYLQWjj5AAAgAElEQVSIiIiIiIiIiIiIXCCf2ykYYzoCfwVaWGtvMMZEAf2stU9Xe3YiIiIiIiIiIiJyXqZPn056enqptkGDBjFlypQLnnvPnj0MGTKkVFvt2rXZunXrec85evRoNm/eXKpt7NixDBs27LznvFJUZk/cl4EJwP8HYK3dbYx5A1ARV0REREREREREpIaaMmXKRSnYehMZGUlWVtZFnXP+fP9up1GTVWY7hXrW2m3ntBVVRzIiIiIiIiIiIiIiUlplirjHjDHtAQtgjBkIHKrWrEREREREREREREQEqNx2CqOBhUAnY8yXwOfAfdWalYiIiIiIiIiIiIgAlSjiWmtzgLuMMfWBAGttfvWnJSIiIiIiIiIiIiJQiSKuMWYGMMta+537uBEw3lr7eHUnJyIiIiIiIiIiUhPMH7n+os43esGdPscEBgYSGRlJYWEhQUFBDB06lIcffpiAgAA2btzI7NmzycjIYPHixUyYMIHWrVtTUFDAiBEjeOSRR8qdNyUlhWuvvZbk5GSv8YqKiggPD+fVV1+lXr16ACxfvpwBAwaQnZ1Np06dyp07NzeXdu3a8fjjj/OnP/0JgGPHjtGqVStGjBjBvHnzSElJYdq0aezbt48OHToA8MILLzBu3Dg++ugjYmNjvc4dGhpK27Zt2bRpk6ctOjqaoqIinE4nGzdu5I477mDRokU8+OCDAOzatYuYmBiee+45z/XOnj2bRYsWERQURGBgIOPHj+f+++/39Xb4VWX2xI0vLuACWGu/BXpVX0oiIiIiIiIiIiJSt25dsrKy2Lt3L5mZmaxevZpp06Z5HZuQkEBWVhabN29m+vTpHDx48LzjOZ1OrrnmGhYsWODpczgcdOnShdTUVJ/zhIWFkZGR4TlOT08nIiKi1JjIyMhScy1btozrr7/e59z5+fmea8vOzi7THxkZSVpamuc4NTWVzp07e44XLFhAZmYm27Ztw+l08v7772Ot9RnX3ypTxA00xtQuPjDG1AVqVzBeRERERERERERELqLmzZuzcOFC5s2bV2HRsUmTJnTo0IFDhw5dULyuXbuyf/9+AE6ePMnmzZt55ZVXKlXErVu3LuHh4Wzfvh2AtLQ0Bg8eXGrMPffcw8qVKwHIycmhYcOGNGvWzOfcgwcP9hRpHQ4HSUlJpfpDQkIoKCjg8OHDWGt55513iI+P9/TPmDGDl156iQYNGgDQsGFDhg4d6jOuv1WmiLsUWGeMedAYMxzIBF6t3rRERERERERERESkpLCwMM6ePcuRI0fKHfPFF19QUFBAVFTUeccpKipizZo1REZGArBixQp69uxJx44dady4MTt37vQ5R2JiIqmpqeTl5REYGMh1111Xqr9Bgwa0bdsWp9OJw+EgISGhUrkNHDiQt956C4C3336bvn37eh2Tnp7Oli1biImJoXZt13rU/Px88vPzad++faVi1SQ+i7jW2lnAdCAciAD+5G4TERERERERERGRS6i8VbhpaWlEREQQFhbG2LFjqVOnTpXnPn36NNHR0cTGxhISEuLZV9bhcJCYmAi4irMOh8PnXD179iQzM7PCAm1xoXfFihX079+/Ujk2btyYRo0akZqaSnh4uGfP3pIGDx5Menp6mZW61lqMMZWKU9P4fLAZgLV2DbCmmnMRERERERERERGRcuTk5BAYGEjz5s3L7AebkJDAvHnz+PDDD+nduzfx8fG0bNmySvMX74lb0vHjx1m/fj1OpxNjDGfOnMEYw6xZsyosiF5zzTXceOON/PnPf2bv3r28/fbbZcb07duXCRMmEBsb69neoDISEhIYPXo0ixcv9trfsmVLatWqRWZmJnPnzmXLli2Aa/Vv/fr1ycnJISwsrNLxagKfK3GNMQOMMfuMMSeMMd8bY/KNMd9fiuREREREREREREQEjh49ysiRIxkzZkyFxdNbb72VIUOGMHfu3IsSd9myZdx///0cOHCA3NxcDh48SLt27fjggw98njt+/HhmzpxJkyZNvPbXrVuXmTNnMmXKlCrl1L9/fx599FF69OhR7pinnnqKmTNnEhgYWKp98uTJjB49mu+/d5U3v//+exYuXFil+P5QmZW4s4C+1tqyj3sTERERERERERG5CoxecOclj1m8vUFhYSFBQUEMGTKEcePG+Txv4sSJxMTE8NhjjxEcHOx1zNNPP82cOXM8x3l5eV7HORwOJk2aVKrt3nvv5Y033qBr164V5hEREUFERESFY4q3aaiK4OBgJk6cWOGY2267zWv7H/7wB06ePMlNN91ErVq1qFWrFuPHj69yDpeaqehpdgDGmM3W2l9eonx8io2NtcVPtrtahE5a5df4uXV+69f4pJzwa/jsTuF+jb++23y/xvfH/6RKypu0ya/xF9VZ59f4KSkpfo0vIiIiInKp+f134Gd7+zW+v+l34P/+DpydnU14uH/vh1xZvH2mjDE7rLWxvs6tzErc7caYNGAF8GNxo7X2raomKiIiIiIiIiIiIiJVU5kibgPgFPCbEm0WUBFXRERERKSaaCXa1W3+yPV+je/vb2OJiMjFMX36dNLT00u1DRo0qMp70HqzZ88ehgwZUqqtdu3abN269YLnjouL48cffyzVtmTJEiIjIy947suVzyKutXbYpUhEpDyRr/r3X9A3/RpdREREREREROT8TJky5aIUbL2JjIwkKyurWua+GIXgK02ArwHGmI7GmHXGGKf7OMoY83j1pyYiIiIiIiIiIiIiPou4wMvAZKAQwFq7G6j6Y+NEREREREREREREpMoqU8StZ63ddk5bUXUkIyIiIiIiIiIiIiKlVaaIe8wY0x7Xw8wwxgwEDlVrViIiIiIiIiIiIiICVOLBZsBoYCHQyRjzJfA5cF+1ZiUiIiIiIiIiIlKD/Dmhz0Wdb3xahs8xgYGBREZGUlhYSFBQEEOHDuXhhx8mICCAjRs3Mnv2bDIyMli8eDETJkygdevWFBQUMGLECB555JFy501JSeHaa68lOTnZa7yioiLCw8N59dVXqVevHgDLly9nwIABZGdn06lTp3LnbteuHe+88w4///nPPW0PP/ww1113HTfffDN33HEHixYt4sEHHwRg165dxMTE8Nxzz5XJp9gDDzzAm2++yeHDhwkODgZg7NixvPjiixw9epSmTZtijOG+++5jyZIlABQVFdGqVSvi4uLIyHDd6zVr1jB16lR++OEHrLX06dOH2bNn+3obagSfK3GttTnW2ruAZkAna20Xa21utWcmIiIiIiIiIiJyFatbty5ZWVns3buXzMxMVq9ezbRp07yOTUhIICsri82bNzN9+nQOHjx43vGcTifXXHMNCxYs8PQ5HA66dOlCampqhXMkJiaWGnP27FmWLVtGQkICAJGRkaSlpXn6U1NT6dy5s8/cOnTowMqVKz1zbtiwgdatW3v669evj9Pp5PTp0wBkZmaW6nc6nYwZM4alS5eSnZ2N0+kkLCzMZ9yaotwirjFmXMkXMAJ4qMSxiIiIiIiIiIiIXALNmzdn4cKFzJs3D2ttueOaNGlChw4dOHTownZD7dq1K/v37wfg5MmTbN68mVdeecVnETcpKanUmPfff5/Q0FB++tOfAhASEkJBQQGHDx/GWss777xDfHy8z3ySkpI8xd+NGzfyy1/+kqCg0psMxMfHs2rVKsBVdE5KSvL0zZo1iylTpnhWEQcFBTFq1CifcWuKilbiBrtfscAfgNbu10jg+upPTURERERERERERIqFhYVx9uxZjhw5Uu6YL774goKCAqKios47TlFREWvWrCEyMhKAFStW0LNnTzp27Ejjxo3ZuXNnuedGRUUREBDAxx9/DLhW2pYspgIMHDiQ9PR0tmzZQkxMDLVr1/aZ089+9jOOHj3Kt99+i8PhIDExscyY4lXABQUF7N69m7i4OE+f0+nkxhtvrNT110TlFnGttdOstdOApkCMtXa8tXY8cCPQ5lIlKCIiIiIiIiIiIi7lrcJNS0sjIiKCsLAwxo4dS506dao89+nTp4mOjiY2NpaQkBDPvrUli6aJiYk4HI4K5ylejVtUVMTKlSsZNGhQqf7BgweTnp5eZrWsLwMGDCA1NZWtW7fStWvXMv1RUVHk5ubicDjo1atXpee9HFTmwWYhwH9KHP8HCK2WbERERERERERERMSrnJwcAgMDad68OdnZ2aX6EhISmDdvHh9++CG9e/cmPj6eli1bVmn+4j1xSzp+/Djr16/H6XRijOHMmTMYY5g1axbGGK/zJCUl8Zvf/IZf/epXREVF0bx581L9LVu2pFatWmRmZjJ37ly2bNlSqfwSExOJiYlh6NChBAR4X5var18/kpOT2bhxI8ePH/e0R0REsGPHjkrtv1sT+XywGbAE2GaMSTHGPAlsBV6r3rRERERERERERESk2NGjRxk5ciRjxowpt3gKcOuttzJkyBDmzp17UeIuW7aM+++/nwMHDpCbm8vBgwdp164dH3zwQbnntG/fniZNmjBp0qRyV9o+9dRTzJw5k8DAwErnEhISwvTp0yvcy3b48OE88cQTnq0gik2YMIEZM2bw2WefAa6Hoz3//POVju1vPlfiWmunG2PWAMVrlIdZa3dVb1oiIiIiIiIiIiI1x/i0jEses3h7g8LCQoKCghgyZAjjxo3zed7EiROJiYnhscceIzg42OuYp59+mjlz5niO8/LyvI5zOBxMmjSpVNu9997LG2+84XVLg2JJSUlMnjyZ/v37e+2/7bbbfF2GVyNGjKiwv02bNowdO7ZMe1RUFHPmzCEpKYlTp05hjKF3797nlYM/mPL20TDGNLDWfm+Maeyt31r7TbVmVo7Y2Fi7fft2f4T2m9BJq/waP7fOb/0aP7JdiF/jv/lMkV/jr+8236/xRy+406/x8yZt8mv8RXXW+TV+SkqKX+OLiIj/+P3PgM9ePr/UXInmj1zv1/j+/jOgXN303z//yu4U7tf4Nel34OzsbMLD/Xs/5Mri7TNljNlhrY31dW5FK3HfAPoAO4CSlV7jPg6reqoiIiIiIiIiIiIiUhUVFXGfdf8z3FpbcCmSERERERERqQn8vRINP69EExGRK8P06dNJT08v1TZo0CCmTJlywXPv2bOHIUOGlGqrXbs2W7duPe85R48ezebNm0u1jR07lmHDhp33nFeKioq4c4EbgS1AzKVJR0RERERERERERC6GKVOmXJSCrTeRkZFkZWVd1Dnnz9dfYpanoiJuoTHmf4E2xpgXz+201v5P9aUlIiIiIiIiIiIiIlBxEbcPcBdwJ659cUVERERERERERETkEiu3iGutPQakGmOyrbUfX8KcRERERERERERERMStopW4xb4yxjwGhJYcb60dXl1JiYiIiIiIiIiIiIhLZYq4K4FNwFrgTPWmIyIiIiIiIiLiRykN/Rz/hH/jS7nyJm26qPO1ebbrRZ2vMkJDQwkODgbgzJkzDBgwgKlTp1K7dm1yc3Pp06cPTqeTjRs3cvfddxMWFsbp06fp06cPs2fPLnfexYsXs337dubNm+c1XkBAAC1atOC1116jZcuWAOzatYuYmBjeeecdevToUWHexhjuu+8+lixZAkBRURGtWrUiLi6OjIwMANasWcPUqVP54YcfsNZWmHNKSgrTpk1j3759dOjQAYAXXniBcePG8dFHHxEbG0toaCht27Zl06b/vu/R0dEUFRXhdDoB2LZtG8nJyRw+fBhjDF26dOHFF1+kXr16FV7P+QioxJh61tqJ1to3rbV/L35d9ExERERERERERETkglhrOXv2bLn9GzZsYM+ePWzbto2cnBx+//vfex3XtWtXdu3axa5du8jIyGDz5s3nlc+GDRv4+OOPiY2NZcaMGZ52h8NBly5dcDgcPueoX78+TqeT06dPA5CZmUnr1q09/U6nkzFjxrB06VKys7NxOp2EhYVVOGdkZCSpqame42XLlnH99deXGpOfn8/BgwcByM7OLtV3+PBhBg0axMyZM/n000/Jzs6mZ8+e5Ofn+7ye81GZIm6GMaZXtUQXERERERERERGRMiZOnMhLL73kOS5ePdq9e3diYmKIjIxk5cqVAOTm5hIeHs6oUaOIiYnxFB4rcu2117JgwQJWrFjBN998U+64unXrEh0dzZdffnlB13P77bezf/9+wFVoXrZsGYsXL+a9996joKDA5/nx8fGsWrUKcBWAk5KSPH2zZs1iypQpdOrUCYCgoCBGjRpV4Xz33HOP5/7l5OTQsGFDmjVrVmrM4MGDSUtL8xpz/vz5DB06lFtvvRVwrRYeOHAgLVq08Hkt56MyRdyxuAq5BcaYfPfr+2rJRkREREREREREREhMTPQUEAHefPNNhg0bxvLly9m5cycbNmxg/PjxWGsB+PTTT7n//vvZtWsXP/3pTysVo0GDBrRr1459+/aVO+bbb79l37593H777Rd0PRkZGURGRgKwefNm2rVrR/v27enWrRurV6/2eX5iYiKpqakUFBSwe/du4uLiPH1Op5Mbb7yxSvk0aNCAtm3b4nQ6cTgcJCQklBkzcOBA3nrrLQDefvtt+vbte0ExL4TPIq61NthaG2CtreP+Odha2+BSJCciIiIiIiIiInI1+sUvfsGRI0f46quv+Pjjj2nUqBGtWrXiscceIyoqirvuuosvv/ySw4cPA/DTn/6UW265pcpxiovA59q0aRNRUVG0bNmSPn36ePayrao77riD6Ohovv/+eyZPngy4VrUmJiYCruJsZbZUiIqKIjc3F4fDQa9eF2fTgOLC8IoVK+jfv3+Z/saNG9OoUSNSU1MJDw+vlr1uK6syDzbDGNMPKC63b7TWZlTyvJ7AXCAQWGStfdbLmMFACmCBj621v63M3CIiIiIiUo30YB8RERG/GzhwIMuWLePrr78mMTGR119/naNHj7Jjxw5q1apFaGioZyuC+vXrV3n+/Px8cnNz6dixIydOlP5/b9euXcnIyOCzzz6jS5cu9O/fn+jo6CrH2LBhA02bNvUcnzlzhr///e/84x//YPr06VhrOX78OPn5+Z6HrpWnX79+JCcns3HjRo4fP+5pj4iIYMeOHXTu3LlKufXt25cJEyYQGxtLgwbe16wmJCQwevRoFi9eXKq9OObdd99dpZjny+dKXGPMs7i2VPiX+zXW3ebrvEBgPhAPXA8kGWOuP2fMz4DJwC+ttRHAw1W+AhERERERERERkStQ8UrRZcuWMXDgQE6cOEHz5s2pVasWGzZs4MCBA+c998mTJxk1ahT33HMPjRo1Kndcx44dmTx5MjNnzjzvWCWtXbuWzp07c/DgQXJzczlw4AD33nsvK1as8Hnu8OHDeeKJJzzbMhSbMGECM2bM4LPPPgPg7NmzPP/88z7nq1u3LjNnzmTKlCnljunfvz+PPvooPXr0KNU+ZswYXn31VbZu3eppW7p0KV9//bXPuOejMitxewHR1tqzAMaYV4FdwCQf590M7LfW5rjPSwXuxlUILvYQMN9a+y2AtfZI1dIXERERERERERGpfm2e7XrJY0ZERJCfn0/r1q1p1aoVv/vd7+jbty+xsbFER0d7HuRVFXfccQfWWs6ePUv//v2ZOnWqz3NGjhzJ7Nmz+fzzz2nXrp3XMYsXLy5ViP3nP//pdZzD4SizdcG9997LX//6V4YMGVJhHm3atGHs2LFl2qOiopgzZw5JSUmcOnUKYwy9e/f2dVkAnm0dyhMcHMzEiRPLtLdo0YLU1FSSk5M5cuQIAQEB3H777QwYMKBScauqUtspAD8Bih9TV9nvVbUGSj4KLw+IO2dMRwBjzGZcWy6kWGvfqeT8IiIiIiIiIiIiV7Q9e/Z4fm7atCkffvih13FOp9PnXLm5ueX2hYaG8t577/HVV1/RsWNHFi5cyFdffeXp/+ijjwBKtRX7zW9+41kFW9KWLVv4z3/+U+qcGTNmlJknNjaWV155pdzcTp48WaatW7dudOvWzXPcp08f+vTpU+4cJaWkpHht37hxo+dnb/cqNDS01H2+9dZb2bRpU6ViXqjKFHGfAXYZYzYABtfeuJMrcZ7x0nbuTslBwM+AbkAbYJMx5gZr7XelJjLm98DvAUJCQioRWkREREREREREROTK4LOIa611GGM2AjfhKsxOtNZWZnOHPKBtieM2wLml+jzgn9baQuBzY8ynuIq6H52Tw0JgIUBsbKz3R+aJiIiIiIiIiIgIcXFx/Pjjj6XalixZUmYv2apKS0tj0aJFpdpuuukmz+raC/HNN9/Qq1evMu3r1q2jSZMm5zXn9OnTSU9PL9U2aNCgCvfAral8FnGNMf2B9dbaf7iPf2KMucda62u34Y+Anxlj2gFfAonAb88ZswJIAhYbY5ri2l4hp4rXICIiIiIiIiIiIm4lH7Z1MSUkJJCQkFAtczdu3JisrKyLOueUKVMuy4KtNwGVGPOktfZE8YF7q4MnfZ1krS0CxgDvAtnAm9bavcaYp4wx/dzD3gWOG2P+BWwAJlhrj1f1IkRERERERERERESuVJXZE9dbobdSD0Sz1q4GVp/T9kSJny0wzv0SERERERERERERkXNUZiXudmPM88aY9saYMGPMC8CO6k5MRERERERERERERCq3ovaPwFQgzX38HvB4tWUkIiIiIiIiIiJSw6SkpNTo+SojNDSU4OBgAM6cOcOAAQOYOnUqtWvXJjc3l549e7J+/Xq2bNnC8OHDCQkJoaCggLvuuosnnnii3HnT0tLYvXs306dPL9UeFxfHtddeizGGZs2aMXfuXJo3bw6A0+mkR48evP7663Tr1q3CvI0x3HfffSxZsgSAoqIiWrVqRVxcHBkZGSxevJhhw4axdu1aunfvDsDy5csZMGAA6enpDBw40Ou83bp1IycnhwMHDmCMAeCee+5h7dq1nDx5ktzcXNq1a8fjjz/On/70JwCOHTtGq1atGDFiBPPmzQPgtddeY9asWVhrsdYyfPhwkpOTK7ymqvK5Etda+4O1dpK1Ntb9esxa+8NFzUJEREREREREREQumLWWs2fPltu/YcMG9uzZw7Zt28jJyeH3v/+913E333wz7733Hu+++y5r167lo48+Oq980tPTWbt2LVFRUfzlL3/xtK9YsYKbb76ZFStW+Jyjfv36OJ1OTp8+DUBmZiatW7cuNSYyMhKHw+E5Tk1NpXPnzj7n/slPfsLmzZsB+O677zh06FCp/rCwMDIyMkpdT0REhOd4zZo1zJkzh/fee4+9e/eyc+dOGjZs6DNuVVVmOwURERERERERERG5hCZOnMhLL73kOU5JSWHatGl0796dmJgYIiMjWblyJQC5ubmEh4czatQoYmJiOHjwoM/5r732WhYsWMCKFSv45ptvyh1Xt25dIiIiyhQ3q+qWW24hNzcXcBWaV61axQsvvMD7779PQUGBz/Pj4+NZtWoVAA6Hg6SkpFL9Xbt2Zdu2bRQWFnLy5En2799PdHS0z3kTExNJTU0F4K233mLAgAGl+uvWrUt4eDjbt28HXKuOBw8e7Ol/5plnmD17Ntdddx0AderU4aGHHvIZt6pUxBUREREREREREalhEhMTSUtL8xy/+eabDBs2jOXLl7Nz5042bNjA+PHjsdYC8On/3969R9tV1fcC//4MoaGKiPiiBkhKRSWGJpcYsMWqGK8RSzCKN0Evaml1dChCb6UW9RZT+sDn6JVRHFeLD/QqD7mCERFQIUhty0vCS6WCoEZaRVAMKvEC8/5xdnAnOY99EvY5+5zz+YxxRtaaa645f+cMmHvu3557rltuyWte85pcd9112WeffXrq47GPfWzmz5+fb3/72yPW+elPf5rbb789Bx988A79Pl/+8pfzjGc8I0ly9dVXZ6+99sq8efPynOc8J5deeumY929Ott5///254YYbctBBB21xvaqybNmyXHzxxfnc5z6XFStW9BTXC1/4wnz1q1/Ngw8+mLPOOiurVq0ase8NGzZk1qxZDydsk6FtIQ488MCe+toRkrgAAAAAMGAWL16cH/3oR7nzzjtz/fXXZ/fdd8+ee+6Zt7/97TnggAOybNmy/OAHP8gPf/jDJMk+++yzXYnWzUngrV111VVZtmxZFi9enGXLlj28l+14vfKVr8yLXvSibNy4Mccee2ySoa0UjjjiiCTJEUcc0dOWCgcccEDuuOOOnHnmmTnssMOGrbM52XrWWWdts1J3JLNmzcohhxySs88+O7/85S8zb968beosX748X/rSl3LmmWcOm+SdCGM+2Kyq5mfo4Wbzuuu31npLZwMAAAAA43bkkUfm3HPPzX/+539m9erV+dSnPpW77ror1157bWbPnp158+Y9vBXBox/96HG3v3Hjxtxxxx3Zb7/9cu+9925xbenSpfnEJz6R2267LStXrszy5cvzrGc9a9x9fOYzn8njH//4h88ffPDBXHjhhbnkkkty6qmnprWWn/zkJ7nvvvvGbGvFihU54YQTsm7dutx9993bXF+6dGluuumm7LLLLtlvv/16jnH16tVZuXLliA+b23nnnXPggQfm/e9/f26++eZ8/vOff/jaggULcu211+bQQw/tub/tMWYSN8n5ST6S5PNJRt4VGQAAAAB4xKxevTqvf/3r8+Mf/ziXX355zjnnnDzpSU/K7Nmzc9lll+W73/3udrd933335Y1vfGNe9rKXZffdd98mibvZvvvum2OPPTYf/OAHt9ijd3tdccUV2X///fPpT3/64bLjjz8+F1100ZiJ12OOOSa77bZbFi5cmHXr1g1b55RTTsmcOXPGFdNzn/vcvO1tbxt19e5b3vKWPO95z8see+yxRfnb3va2vPWtb80FF1yQpzzlKdm0aVM+9KEP5bjjjhtXDGPpJYl7f2vt1Ee0VwAAAACYQkZapdlPCxYsyMaNG/PUpz41e+65Z1796lfn8MMPz5IlS7Jo0aKH95gdjxe84AVpreWhhx7KypUr8zZdAaAAABjqSURBVFd/9Vdj3nP00UfnQx/6UL73ve9l7733HrbOOeeck4suuujh8+7Vqt3OP//8LF++fIuyl770pfnEJz4xZuJz7ty5Of7440et85KXvGTU68Opqpxwwgmj1lmwYEEWLFiwTflhhx2WH/7wh1m2bFlaa6mqHHPMMeOOYcwYR9r34uEKVa9K8rQklyTZtLm8tfb1RzyaHixZsqRtfhrcTDHvxC9Mav93zHnVpPa/cP7wg8NEOeeUBya1/0uff9qk9v+m/93frwOMZcOJV0xq/6fP+cqk9j8ZkwQABsNMnwNmzfCrgSbKN5/xzEntf6bPAZnZjH/Gv8nUPf5985vfzDOfObl/j4l25513Tmr/3Q8Mm46G+2+qqq5trS0Z695eVuIuTHJ0kkPz6+0UWuccAAAAAIA+6iWJuzLJb7fWftXvYAAAAACAHXPQQQdl06ZNW5R98pOfzMKFC3eo3bPPPjunn376FmXPfvaz8/d///c71G6S3HPPPTnssMO2Kf/KV76yzT6047Vy5crcfvvtW5S9+93vzotf/OIdanci9ZLEvT7J45L8qM+xAAAAAAA76Morr+xLu6tWrcqqVav60vbjH//4rF+/vi9tn3feeX1pdyL1ksR9cpJvVdXV2XJP3BV9iwoAMgD7ob3rpZPa/0x32p9eOtkh2BMSAGCG2/ygKthRYz2XbCy9JHHfuUM9AAAAAMAUM2fOnNx9993ZY489JHLZIa213H333ZkzZ852tzFmEre1dvl2tw4AAAAAU9DcuXOzYcOG3HXXXZMdyoT56U9/Oqn933vvvZPafz/NmTMnc+fO3e77x0ziVtXGJJvX++6cZHaSn7fWHrvdvQIAAADAAJs9e3bmz58/2WFMqDVr1szo/gdZLytxd+0+r6qXJVnat4gAAAAAAHjYo8Z7Q2vt/CSe8gEAAAAAMAF62U7h5V2nj0qyJL/eXgEAAAAAgD4aM4mb5PCu4weS3JHkiL5EAwAAAADAFnrZE/ePJiIQAAAAAAC21ct2CvOTvDnJvO76rbUV/QsLAAAAAICkt+0Uzk/ykSSfT/JQf8MBAAAAAKBbL0nc+1trp/Y9EgAAAAAAttFLEvcDVfXOJJck2bS5sLX29b5FBQAAAABAkt6SuAuTHJ3k0Px6O4XWOQcAAAAAoI96SeKuTPLbrbVf9TsYAAAAAAC29Kge6lyf5HH9DgQAAAAAgG31shL3yUm+VVVXZ8s9cVf0LSoAGARrdpvk/u+d3P4BAAAYCL0kcd/Z9ygAAAAAABjWmEnc1trlExEIAAAAAADbGjGJW1X/3Fo7pKo2Jmndl5K01tpj+x4dAAAAAMAMN2ISt7V2SOffXScuHAAAAAAAuj1qrApVtWyYstf2JxwAAAAAALr18mCzk6rqFUlOSPKYJKcn2ZTkjH4GBgAAAJNhzZo1M7p/AAbPmCtxkzwvyW1J1if55ySfbq0d2deoAAAAAABI0lsSd/ckB2UokbspyT5VVX2NCgAAAACAJL0lcf8tyRdba8uTPDvJbyX5Wl+jAgAAAAAgSW974i5rrX0vSVprv0xyXFX9QX/DAgAAAAAg6SGJ21r7XlXtnuRpSeb0PyQAAAAAADYbM4lbVX+S5PgkczP0cLODk/xrkkP7GxoAAAAAAL3siXt8hvbC/W5r7QVJFie5q69RAQAAAACQpLck7v2ttfuTpKp+o7X2rSRP76XxqlpeVbdU1a1VdeIo9Y6sqlZVS3oLGwAAAABgZujlwWYbqupxSc5P8qWq+kmSO8e6qapmJTktyYuSbEhydVWtba19Y6t6uyY5LsmV4w0eAACA6WfDiVdMbgCeBgPAgOnlwWYrO4drquqyJLsluaiHtpcmubW19p0kqaqzkhyR5Btb1fubJO9JckKvQQMAAAAAzBS9bKeQqtq9qg5IsjFDq2qf1cNtT03y/a7zDZ2y7nYXJ9mrtXZBb+ECAAAAAMwsY67Eraq/SfK6JN9J8lCnuCU5dKxbhylrXe0+Ksk/dNoeK4Y3JHlDkuy9995jVQcAAAAAmDZ62RP3vyXZt7X2q3G2vSHJXl3nc7PlXrq7ZmhF77qqSpKnJFlbVStaa9d0N9Ra+3CSDyfJkiVLWgAAAAAAZohekrg3JXlckh+Ns+2rkzytquYn+UGS1Uletflia+3eJE/YfF5V65KcsHUCFwCYHN98xjMnN4Dnnza5/QMAAAyIXpK4pyS5rqpuSrJpc2FrbcVoN7XWHqiqY5NcnGRWko+21m6uqpOTXNNaW7sDcQMAAAAAzAi9JHHPSPLuJDfm13vi9qS1dmGSC7cqO2mEus8fT9sAAAAAADNBL0ncH7fWTu17JAAAAAAAbKOXJO61VXVKkrXZcjuFr/ctKgAAAAAAkvSWxF3c+ffgrrKW5NBHPhwAAAAAALqNmcRtrb1gIgIBAAAAAGBbj5rsAAAAAAAAGJkkLgAAAADAAJPEBQAAAAAYYL082CxV9XtJ5nXXb619ok8xAQAAAADQMWYSt6o+mWTfJOuTPNgpbkkkcQEAAAAA+qyXlbhLkuzfWmv9DgYAAAAAgC31sifuTUme0u9AAAAAAADYVi8rcZ+Q5BtVdVWSTZsLW2sr+hYVAAAAAABJekvirul3EAAAAAAADG/MJG5r7fKJCAQAAAAAgG2NuSduVR1cVVdX1X1V9auqerCqfjYRwQEAAAAAzHS9PNjsH5McleTbSXZJ8iedMgAAAAAA+qyXPXHTWru1qma11h5M8rGq+pc+xwUAAAAAQHpL4v6iqnZOsr6q3pPkP5I8ur9hAQAAAACQ9LadwtGdescm+XmSvZK8op9BAQAAAAAwZMyVuK2171bVLkn2bK399QTEBAAAAABAx5grcavq8CTrk1zUOV9UVWv7HRgAAAAAAL3tibsmydIk65Kktba+qub1LSIAAGDGW3jGwknt/5xJ7R0AYEu97In7QGvt3r5HAgAAAADANnpZiXtTVb0qyayqelqS45L8S3/DAgAAAAAg6W0l7puTLEiyKcmZSX6W5M/6GRQAAAAAAEPGXInbWvtFknd0fgAAAAAAmEAjJnGrau1oN7bWVjzy4QAAAAAA0G20lbjPSfL9DG2hcGWSmpCIAAAAAAB42GhJ3KckeVGSo5K8KskXkpzZWrt5IgIDAAAAmGkWnrFwUvs/Z1J7B0Yy4oPNWmsPttYuaq29NsnBSW5Nsq6q3jxh0QEAAAAAzHCjPtisqn4jyUsztBp3XpJTk3y2/2EBAAAAAJCM/mCzM5I8K8kXk/x1a+2mCYsKAAAAAIAko6/EPTrJz5Psl+S4qoefa1ZJWmvtsX2ODQAAAABgxhsxidtaG3G/XAAAAAAAJoZELQAAAADAAJPEBQAAAAAYYKPtiQsATKKFZyyc1P7PmdTeAQAA2MxKXAAAAACAASaJCwAAAAAwwCRxAQAAAAAGmD1xAQAY1po1a2Z0/wAAMCisxAUAAAAAGGCSuAAAAAAAA0wSFwAAAABggPU1iVtVy6vqlqq6tapOHOb6n1fVN6rqhqr6SlXt0894AAAAAACmmr4lcatqVpLTkrwkyf5Jjqqq/beqdl2SJa21A5Kcm+Q9/YoHAAAAAGAq6udK3KVJbm2tfae19qskZyU5ortCa+2y1tovOqf/lmRuH+MBAAAAAJhy+pnEfWqS73edb+iUjeSPk3xxuAtV9YaquqaqrrnrrrsewRABAAAAAAZbP5O4NUxZG7Zi1X9PsiTJe4e73lr7cGttSWttyROf+MRHMEQAAAAAgMG2Ux/b3pBkr67zuUnu3LpSVS1L8o4kz2utbepjPAAAAAAAU04/V+JeneRpVTW/qnZOsjrJ2u4KVbU4yYeSrGit/aiPsQAAAAAATEl9S+K21h5IcmySi5N8M8k5rbWbq+rkqlrRqfbeJI9J8pmqWl9Va0doDgAAAABgRurndgpprV2Y5MKtyk7qOl7Wz/4BAKayDSdeMbkBzJnc7gEAgCH93E4BAAAAAIAdJIkLAAAAADDAJHEBAAAAAAaYJC4AAAAAwACTxAUAAAAAGGCSuAAAAAAAA0wSFwAAAABggEniAgAAAAAMMElcAAAAAIABJokLAAAAADDAJHEBAAAAAAaYJC4AAAAAwACTxAUAAAAAGGCSuAAAAAAAA0wSFwAAAABggEniAgAAAAAMMElcAAAAAIABJokLAAAAADDAJHEBAAAAAAaYJC4AAAAAwADbabIDAAbb+1f94aT2v2r+X05q/8DMZgwEZirjHwAMFklcAAAAAMjkf4j1lrMvmNT+GVy2UwAAAAAAGGCSuAAAAAAAA0wSFwAAAABggEniAgAAAAAMMElcAAAAAIABJokLAAAAADDAJHEBAAAAAAaYJC4AAAAAwACTxAUAAAAAGGCSuAAAAAAAA0wSFwAAAABggEniAgAAAAAMMElcAAAAAIABJokLAAAAADDAJHEBAAAAAAaYJC4AAAAAwACTxAUAAAAAGGCSuAAAAAAAA0wSFwAAAABggEniAgAAAAAMMElcAAAAAIAB1tckblUtr6pbqurWqjpxmOu/UVVnd65fWVXz+hkPAAAAAMBU07ckblXNSnJakpck2T/JUVW1/1bV/jjJT1prv5PkH5K8u1/xAAAAAABMRf1cibs0ya2tte+01n6V5KwkR2xV54gkZ3SOz03ywqqqPsYEAAAAADCl9DOJ+9Qk3+8639ApG7ZOa+2BJPcm2aOPMQEAAAAATCnVWutPw1WvTPLi1tqfdM6PTrK0tfbmrjo3d+ps6Jzf1qlz91ZtvSHJGzqnT09yS1+ChunpCUl+PNlBAEwSYyAwUxn/gJnK+MdUs09r7YljVdqpjwFsSLJX1/ncJHeOUGdDVe2UZLck92zdUGvtw0k+3Kc4YVqrqmtaa0smOw6AyWAMBGYq4x8wUxn/mK76uZ3C1UmeVlXzq2rnJKuTrN2qztokr+0cH5nk0tavpcEAAAAAAFNQ31bittYeqKpjk1ycZFaSj7bWbq6qk5Nc01pbm+QjST5ZVbdmaAXu6n7FAwAAAAAwFfVzO4W01i5McuFWZSd1Hd+f5JX9jAGwFQkwoxkDgZnK+AfMVMY/pqW+PdgMAAAAAIAd1889cQEAAAAA2EGSuAAAAAAAA0wSF/qoqu57BNp4flVd0DleUVUnjlJ3UVUdNkZ7r6uqVlUv7Cpb2Sk7snO+rqqu6bq+pKrWDRPPk6vqgqq6vqq+UVUXVtXCqlrf+bmnqm7vHH95h/4QwMCpqrlV9bmq+nZV3VZVH6iqnce45+3j7GNNVZ3QOT65qpaNUvdlVbX/GO19vGtcWl9Vx3XK76iqK7aqu76qbuocP78zTh7edf2Cqnp+53hdVS3pHB9TVTdW1Q1VdVNVHVFVp3Xa+0ZV/bKr/yPH8/cAJt5UHOvGa+s5pPkizBxTcYwzn2OmksSFKaS1tra19q5RqixKMmoSt+PGJEd1na9Ocv1WdZ5UVS8Zo52Tk3yptfa7rbX9k5zYWruxtbaotbYoydokf9E5H/GFGph6qqqSfDbJ+a21pyXZL8ljkvzdGLeOa9LfrbV2UmtttDf4L0vSS2Jj87i0qLV2alf5rlW1V5JU1TOHuW9DkneM1nBVze3UOaS1dkCSg5Pc0Fp7U2dcPCzJbV39n9tDvMAkmeJj3XgMN4c0X4RpboqPcdNmPldVO+3I/cwckrgwATqf+K2rqnOr6ltV9anOC+ZI9Zd36v1zkpd3lb+uqv6xc/zKzieC11fVVzuflp6cZFXn08BVo4R0RZKlVTW7qh6T5HeSrN+qznuT/M8xfrU9M/QimCRprd0wRn1g+jg0yf2ttY8lSWvtwST/I8kxVfXGzWNV8usVDlX1riS7dMaoT43UcFW9o6pu6azIenpX+ce7VoC9q7MK4oaqel9V/V6SFUne22l/3+34nc5JsnnsPCrJmVtdvz7JvVX1olHaeFKSjUnuS5LW2n2ttdu3IxZgMEzJsa4z73x3VV1VVf9eVc/tlM+pqo91VpddV1UvGGUOab4I09+UHOPGMCnzufGMu53y11XVZ6rq80ku6fxtL6+qczr3v6uqXt1p78bt/FswzUjiwsRZnOTPMvSp4m8n+f3hKlXVnCT/lOTwJM9N8pQR2jspyYtba7+bZEVr7VedsrM7nwaePUosLcmXk7w4yREZWgGxtX9Nsmnzi8wITkvykaq6rPMi/Vuj1AWmlwVJru0uaK39LMn3kgy7mqC1dmKSX3bGqFcPV6eqDszQaq/FGfoQ69nD1Hl8kpVJFnRWR/xta+1fsuVqrttGiX3zG4P1VbWwq/zc/PqDs8OTfH6Ye/82oycsrk/ywyS3dybsh49SFxh8U3ms26m1tjRD8893dsre1IlxYYaSG2dk6D3hcHNI80WY/qbyGDeI87mext3Oe/4keU6S17bWDu2c/26S45MsTHJ0kv067Z2e5M09xsA0JokLE+eq1tqG1tpDGVrFMG+Ees9Icntr7duttZbk/4xQ72tJPl5Vr08yazviOStDL6yrs+2nk5uN+uLWWrs4Qwnpf+rEfV1VPXE7YgGmnsrQG/xey3v13CTntdZ+0XkTMVzS4GdJ7k9yelW9PMkvxtlH99fvbuwqvyfJT6pqdZJvDtdua+2KJNm8umKY6w8mWZ7kyCT/nuQfqmrNOOMDBsdUHus+2/n32vx63nlIkk8mSWvtW0m+m6GvT4/EfBGmt6k8xg3ifG684+6XWmv3dN1/dWvtP1prm5LcluSSTvmNGTl/wAwiiQsTZ1PX8YMZ4ZPNjjFfMFtrf5qhCfNeSdZX1R7jCaa1dlWSZyV5Qmvt30eoc2mSORnaA2ikdu5prX26tXZ0kquT/MF44gCmrJuTLOkuqKrHZmhMujdbzjHmZHxGHQNbaw8kWZrk/2Zo37SLxtn+aM7O0KqxkZIVydA+cSPupdaGXNVaOyVDiY9XPILxARNrKo91m+ee3fPOEbfzGiEG80WY3qbyGDeayZrPjXfc/fkI9yfJQ13nD2X0/AEzhCQuDJ5vJZnftefNUcNVqqp9W2tXttZOSvLjDL3Qbkyy6zj6elvG3pT+75K8dYQYDq2q3+wc75pk3wx99QaY/r6S5Der6jVJUlWzkrw/yceTfCfJoqp6VA09WGJp133/r6pmj9LuV5OsrKpdOuPKNl9f6+zNuFtr7cIMfV1tUefSeMfA4ZyX5D1JLh6pQmvtkiS7Z+grb1vH9ltV9V+6ihZlaMUFMDVNt7Huq0le3Wl/vyR7J7lljDbNF2H6mm5j3GaDNJ8badyFcZPEhQHTWrs/yRuSfKGGHmw20ovFezsbnN+UoReG65NclmT/GvvBZpv7+mJr7bIx6lyY5K4RLh+Y5JqquiFDe6Kd3lq7eqx+gamvs93LyiSvrKpvZ+irZvdn6I3+15LcnqGvfr0vyde7bv1wkhtGehBGa+3rGVo9sT5DKzOuGKbarkku6Iw9l2foARzJ0Nd+/6Lz0IjtevhDa21ja+3dnX3GR/N3SeYOUz47yftq6OGU6zP0YI3jtycWYPJNw7Hug0lmVdWNnf5f1/na7ohzSPNFmL6m4Ri3uf9Bms+NNO7CuNXQ/7MAAAAAAAwiK3EBAAAAAAaYjZFhElXVeUnmb1X8l52n+O5o23+Ubb/y8bXW2pt2tG2AHdV5GONXhrn0wtba3Y9A+6cl+f2tij/QWvvYjrYN0CtjHTCdGeNgYtlOAQAAAABggNlOAQAAAABggEniAgAAAAAMMElcAACmlapqVfXJrvOdququqrpgjPsWVdVhXedrquqEHYhjh+4HAIDNJHEBAJhufp7kWVW1S+f8RUl+0MN9i5IcNmYtAACYYJK4AABMR19M8tLO8VFJztx8oaoeXVUfraqrq+q6qjqiqnZOcnKSVVW1vqpWdarvX1Xrquo7VXVcVxt/XlU3dX7+rKv8HVV1S1V9OcnT+/5bAgAwI0jiAgAwHZ2VZHVVzUlyQJIru669I8mlrbVnJ3lBkvcmmZ3kpCRnt9YWtdbO7tR9RpIXJ1ma5J1VNbuqDkzyR0kOSnJwktdX1eJO+eoki5O8PMmz+/1LAgAwM+w02QEAAMAjrbV2Q1XNy9Aq3Au3uvxfk6zo2q92TpK9R2jqC621TUk2VdWPkjw5ySFJzmut/TxJquqzSZ6boQUS57XWftEpX/vI/UYAAMxkkrgAAExXa5O8L8nzk+zRVV5JXtFau6W7clUdNEwbm7qOH8zQ/LlG6bNtV6QAADAK2ykAADBdfTTJya21G7cqvzjJm6uqkqSqFnfKNybZtYd2v5rkZVX1m1X16CQrk1zRKV9ZVbtU1a5JDn8kfgkAALASFwCAaam1tiHJB4a59DdJ/leSGzqJ3DuS/GGSy5KcWFXrk5wySrtfr6qPJ7mqU3R6a+26JKmqs5OsT/LdDCV2AQBgh1VrvvEFAAAAADCobKcAAAAAADDAJHEBAAAAAAaYJC4AAAAAwACTxAUAAAAAGGCSuAAAAAAAA0wSFwAAAABggEniAgAAAAAMMElcAAAAAIAB9v8B4YOrPLDU/iAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1728x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot for MMC\n",
    "labels=['In_dist_MNIST', 'Out_dist_FMNIST', 'Out_dist_notMNIST', 'Out_dist_FMNISTnorm']\n",
    "#labels = ['MAP', 'Diag', 'KFAC', 'DIR_LPA_MC', 'DIR_LPA_MM', 'DIR_LPA_VM', 'var_DIR_LPA_MC', 'var_DIR_LPA_MM']\n",
    "MAP_MMC = np.array([MMC_in_MAP, MMC_out_FMNIST_MAP, MMC_out_notMNIST_MAP, MMC_out_FMNISTn_MAP])\n",
    "Diag_MMC = np.array([MMC_in_D, MMC_out_FMNIST_D, MMC_out_notMNIST_D, MMC_out_FMNISTn_D])\n",
    "KFAC_MMC = np.array([MMC_in_KFAC, MMC_out_FMNIST_KFAC, MMC_out_notMNIST_KFAC, MMC_out_FMNISTn_KFAC])\n",
    "DIR_LPA_MC_MMC = np.array([MMC_in_DIR_LPA_MC, MMC_out_FMNIST_DIR_LPA_MC, MMC_out_notMNIST_DIR_LPA_MC, MMC_out_FMNISTn_DIR_LPA_MC])\n",
    "DIR_LPA_MM_MMC = np.array([MMC_in_DIR_LPA_MM, MMC_out_FMNIST_DIR_LPA_MM, MMC_out_notMNIST_DIR_LPA_MM, MMC_out_FMNISTn_DIR_LPA_MM])\n",
    "DIR_LPA_VM_MMC = np.array([MMC_in_DIR_LPA_VM, MMC_out_FMNIST_DIR_LPA_VM, MMC_out_notMNIST_DIR_LPA_VM, MMC_out_FMNISTn_DIR_LPA_VM])\n",
    "var_DIR_LPA_MC_MMC = np.array([MMC_in_var_DIR_LPA_MC, MMC_out_FMNIST_var_DIR_LPA_MC, MMC_out_notMNIST_var_DIR_LPA_MC, 0])\n",
    "var_DIR_LPA_MM_MMC = np.array([MMC_in_var_DIR_LPA_MM, MMC_out_FMNIST_var_DIR_LPA_MM, MMC_out_notMNIST_var_DIR_LPA_MM, MMC_out_FMNISTn_var_DIR_LPA_MM])\n",
    "\n",
    "X = np.vstack((MAP_MMC, Diag_MMC, KFAC_MMC , DIR_LPA_MC_MMC, DIR_LPA_MM_MMC, DIR_LPA_VM_MMC, var_DIR_LPA_MC_MMC, var_DIR_LPA_MM_MMC))\n",
    "In_dist_MMC = X[:,0]\n",
    "Out_dist_MMC_FMNIST = X[:,1]\n",
    "Out_dist_MMC_notMNIST = X[:,2]\n",
    "Out_dist_MMC_FMNISTnorm = X[:,3]\n",
    "\n",
    "width = 0.10  # the width of the bars\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 5))\n",
    "ax.bar(x - 4*width, MAP_MMC, width, label='MAP_MMC')\n",
    "ax.bar(x - 3*width, Diag_MMC, width, label='Diag_MMC')\n",
    "ax.bar(x - 2*width, KFAC_MMC, width, label='KFAC_MMC')\n",
    "ax.bar(x - 1*width, DIR_LPA_MC_MMC, width, label='DIR_LPA_MC_MMC')\n",
    "ax.bar(x, DIR_LPA_MM_MMC, width, label='DIR_LPA_MM_MMC')\n",
    "ax.bar(x + 1*width, DIR_LPA_VM_MMC, width, label='DIR_LPA_VM_MMC')\n",
    "ax.bar(x + 2*width, var_DIR_LPA_MC_MMC, width, label='var_DIR_LPA_MC_MMC')\n",
    "ax.bar(x + 3*width, var_DIR_LPA_MM_MMC, width, label='var_DIR_LPA_MM_MMC')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Mean maximum confidence')\n",
    "plt.title('MMCs of all methods in comparison')\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig('results_bar_plot.jpg')\n",
    "#plt.savefig('results_bar_plot.pdf')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot for entropy\n",
    "labels=['In_dist_MNIST', 'Out_dist_FMNIST', 'Out_dist_notMNIST', 'Out_dist_FMNISTnorm']\n",
    "#labels = ['MAP', 'Diag', 'KFAC', 'DIR_LPA_MC', 'DIR_LPA_MM', 'DIR_LPA_VM', 'var_DIR_LPA_MC', 'var_DIR_LPA_MM']\n",
    "MAP_ent = np.array([ent_in_MAP, ent_out_FMNIST_MAP, ent_out_notMNIST_MAP, ent_out_FMNISTn_MAP])\n",
    "Diag_ent = np.array([ent_in_D, ent_out_FMNIST_D, ent_out_notMNIST_D, ent_out_FMNISTn_D])\n",
    "KFAC_ent = np.array([ent_in_KFAC, ent_out_FMNIST_KFAC, ent_out_notMNIST_KFAC, ent_out_FMNISTn_KFAC])\n",
    "DIR_LPA_MC_ent = np.array([ent_in_DIR_LPA_MC, ent_out_FMNIST_DIR_LPA_MC, ent_out_notMNIST_DIR_LPA_MC, ent_out_FMNISTn_DIR_LPA_MC])\n",
    "DIR_LPA_MM_ent = np.array([ent_in_DIR_LPA_MM, ent_out_FMNIST_DIR_LPA_MM, ent_out_notMNIST_DIR_LPA_MM, ent_out_FMNISTn_DIR_LPA_MM])\n",
    "DIR_LPA_VM_ent = np.array([ent_in_DIR_LPA_VM, ent_out_FMNIST_DIR_LPA_VM, ent_out_notMNIST_DIR_LPA_VM, ent_out_FMNISTn_DIR_LPA_VM])\n",
    "var_DIR_LPA_MC_ent = np.array([ent_in_var_DIR_LPA_MC, ent_out_FMNIST_var_DIR_LPA_MC, ent_out_notMNIST_var_DIR_LPA_MC, 0])\n",
    "var_DIR_LPA_MM_ent = np.array([ent_in_var_DIR_LPA_MM, ent_out_FMNIST_var_DIR_LPA_MM, ent_out_notMNIST_var_DIR_LPA_MM, ent_out_FMNISTn_var_DIR_LPA_MM])\n",
    "\n",
    "X = np.vstack((MAP_ent, Diag_ent, KFAC_ent , DIR_LPA_MC_ent, DIR_LPA_MM_ent, DIR_LPA_VM_ent, var_DIR_LPA_MC_ent, var_DIR_LPA_MM_ent))\n",
    "In_dist_ent = X[:,0]\n",
    "Out_dist_ent_FMNIST = X[:,1]\n",
    "Out_dist_ent_notMNIST = X[:,2]\n",
    "Out_dist_ent_FMNISTnorm = X[:,3]\n",
    "\n",
    "width = 0.10  # the width of the bars\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 5))\n",
    "ax.bar(x - 4*width, MAP_ent, width, label='MAP_ent')\n",
    "ax.bar(x - 3*width, Diag_ent, width, label='Diag_ent')\n",
    "ax.bar(x - 2*width, KFAC_ent, width, label='KFAC_ent')\n",
    "ax.bar(x - 1*width, DIR_LPA_MC_ent, width, label='DIR_LPA_MC_ent')\n",
    "ax.bar(x, DIR_LPA_MM_ent, width, label='DIR_LPA_MM_ent')\n",
    "ax.bar(x + 1*width, DIR_LPA_VM_ent, width, label='DIR_LPA_VM_ent')\n",
    "ax.bar(x + 2*width, var_DIR_LPA_MC_ent, width, label='var_DIR_LPA_MC_ent')\n",
    "ax.bar(x + 3*width, var_DIR_LPA_MM_ent, width, label='var_DIR_LPA_MM_ent')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Entropy')\n",
    "plt.title('Entropy of all methods in comparison')\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig('results_bar_plot.jpg')\n",
    "#plt.savefig('results_bar_plot.pdf')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot for auroc\n",
    "labels=['Out_dist_FMNIST', 'Out_dist_notMNIST', 'Out_dist_FMNISTnorm']\n",
    "#labels = ['MAP', 'Diag', 'KFAC', 'DIR_LPA_MC', 'DIR_LPA_MM', 'DIR_LPA_VM', 'var_DIR_LPA_MC', 'var_DIR_LPA_MM']\n",
    "MAP_auroc = np.array([auroc_out_FMNIST_MAP, auroc_out_notMNIST_MAP, auroc_out_FMNISTn_MAP])\n",
    "Diag_auroc = np.array([auroc_out_FMNIST_D, auroc_out_notMNIST_D, auroc_out_FMNISTn_D])\n",
    "KFAC_auroc = np.array([auroc_out_FMNIST_KFAC, auroc_out_notMNIST_KFAC, auroc_out_FMNISTn_KFAC])\n",
    "DIR_LPA_MC_auroc = np.array([auroc_out_FMNIST_DIR_LPA_MC, auroc_out_notMNIST_DIR_LPA_MC, auroc_out_FMNISTn_DIR_LPA_MC])\n",
    "DIR_LPA_MM_auroc = np.array([auroc_out_FMNIST_DIR_LPA_MM, auroc_out_notMNIST_DIR_LPA_MM, auroc_out_FMNISTn_DIR_LPA_MM])\n",
    "DIR_LPA_VM_auroc = np.array([auroc_out_FMNIST_DIR_LPA_VM, auroc_out_notMNIST_DIR_LPA_VM, auroc_out_FMNISTn_DIR_LPA_VM])\n",
    "var_DIR_LPA_MC_auroc = np.array([auroc_out_FMNIST_var_DIR_LPA_MC, auroc_out_notMNIST_var_DIR_LPA_MC, 0])\n",
    "var_DIR_LPA_MM_auroc = np.array([auroc_out_FMNIST_var_DIR_LPA_MM, auroc_out_notMNIST_var_DIR_LPA_MM, auroc_out_FMNISTn_var_DIR_LPA_MM])\n",
    "\n",
    "X = np.vstack((MAP_auroc, Diag_auroc, KFAC_auroc , DIR_LPA_MC_auroc, DIR_LPA_MM_auroc, DIR_LPA_VM_auroc, var_DIR_LPA_MC_auroc, var_DIR_LPA_MM_auroc))\n",
    "Out_dist_ent_FMNIST = X[:,0]\n",
    "Out_dist_ent_notMNIST = X[:,1]\n",
    "Out_dist_ent_FMNISTnorm = X[:,2]\n",
    "\n",
    "width = 0.10  # the width of the bars\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 5))\n",
    "ax.bar(x - 4*width, MAP_auroc, width, label='MAP_auroc')\n",
    "ax.bar(x - 3*width, Diag_auroc, width, label='Diag_auroc')\n",
    "ax.bar(x - 2*width, KFAC_auroc, width, label='KFAC_auroc')\n",
    "ax.bar(x - 1*width, DIR_LPA_MC_auroc, width, label='DIR_LPA_MC_auroc')\n",
    "ax.bar(x, DIR_LPA_MM_auroc, width, label='DIR_LPA_MM_auroc')\n",
    "ax.bar(x + 1*width, DIR_LPA_VM_auroc, width, label='DIR_LPA_VM_auroc')\n",
    "ax.bar(x + 2*width, var_DIR_LPA_MC_auroc, width, label='var_DIR_LPA_MC_auroc')\n",
    "ax.bar(x + 3*width, var_DIR_LPA_MM_auroc, width, label='var_DIR_LPA_MM_auroc')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('AUROC')\n",
    "plt.title('AUROC of all methods in comparison')\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig('results_bar_plot.jpg')\n",
    "#plt.savefig('results_bar_plot.pdf')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot for prob @ correct\n",
    "labels=['In_dist_MNIST', 'Out_dist_FMNIST', 'Out_dist_notMNIST', 'Out_dist_FMNISTnorm']\n",
    "#labels = ['MAP', 'Diag', 'KFAC', 'DIR_LPA_MC', 'DIR_LPA_MM', 'DIR_LPA_VM', 'var_DIR_LPA_MC', 'var_DIR_LPA_MM']\n",
    "MAP_prob_correct = np.array([prob_correct_in_MAP, prob_correct_out_FMNIST_MAP, prob_correct_out_notMNIST_MAP, prob_correct_out_FMNISTn_MAP])\n",
    "Diag_prob_correct = np.array([prob_correct_in_D, prob_correct_out_FMNIST_D, prob_correct_out_notMNIST_D, prob_correct_out_FMNISTn_D])\n",
    "KFAC_prob_correct = np.array([prob_correct_in_KFAC, prob_correct_out_FMNIST_KFAC, prob_correct_out_notMNIST_KFAC, prob_correct_out_FMNISTn_KFAC])\n",
    "DIR_LPA_MC_prob_correct = np.array([prob_correct_in_DIR_LPA_MC, prob_correct_out_FMNIST_DIR_LPA_MC, prob_correct_out_notMNIST_DIR_LPA_MC, prob_correct_out_FMNISTn_DIR_LPA_MC])\n",
    "DIR_LPA_MM_prob_correct = np.array([prob_correct_in_DIR_LPA_MM, prob_correct_out_FMNIST_DIR_LPA_MM, prob_correct_out_notMNIST_DIR_LPA_MM, prob_correct_out_FMNISTn_DIR_LPA_MM])\n",
    "DIR_LPA_VM_prob_correct = np.array([prob_correct_in_DIR_LPA_VM, prob_correct_out_FMNIST_DIR_LPA_VM, prob_correct_out_notMNIST_DIR_LPA_VM, prob_correct_out_FMNISTn_DIR_LPA_VM])\n",
    "var_DIR_LPA_MC_prob_correct = np.array([prob_correct_in_var_DIR_LPA_MC, prob_correct_out_FMNIST_var_DIR_LPA_MC, prob_correct_out_notMNIST_var_DIR_LPA_MC, 0])\n",
    "var_DIR_LPA_MM_prob_correct = np.array([prob_correct_in_var_DIR_LPA_MM, prob_correct_out_FMNIST_var_DIR_LPA_MM, prob_correct_out_notMNIST_var_DIR_LPA_MM, prob_correct_out_FMNISTn_var_DIR_LPA_MM])\n",
    "\n",
    "X = np.vstack((MAP_prob_correct, Diag_prob_correct, KFAC_prob_correct , DIR_LPA_MC_prob_correct, DIR_LPA_MM_prob_correct, DIR_LPA_VM_prob_correct, var_DIR_LPA_MC_prob_correct, var_DIR_LPA_MM_prob_correct))\n",
    "In_dist_prob_correct = X[:,0]\n",
    "Out_dist_prob_correct_FMNIST = X[:,1]\n",
    "Out_dist_prob_correct_notMNIST = X[:,2]\n",
    "Out_dist_prob_correct_FMNISTnorm = X[:,3]\n",
    "\n",
    "width = 0.10  # the width of the bars\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 5))\n",
    "ax.bar(x - 4*width, MAP_prob_correct, width, label='MAP_prob_correct')\n",
    "ax.bar(x - 3*width, Diag_prob_correct, width, label='Diag_prob_correct')\n",
    "ax.bar(x - 2*width, KFAC_prob_correct, width, label='KFAC_prob_correct')\n",
    "ax.bar(x - 1*width, DIR_LPA_MC_prob_correct, width, label='DIR_LPA_MC_prob_correct')\n",
    "ax.bar(x, DIR_LPA_MM_prob_correct, width, label='DIR_LPA_MM_prob_correct')\n",
    "ax.bar(x + 1*width, DIR_LPA_VM_prob_correct, width, label='DIR_LPA_VM_prob_correct')\n",
    "ax.bar(x + 2*width, var_DIR_LPA_MC_prob_correct, width, label='var_DIR_LPA_MC_prob_correct')\n",
    "ax.bar(x + 3*width, var_DIR_LPA_MM_prob_correct, width, label='var_DIR_LPA_MM_prob_correct')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Prob @ Correct')\n",
    "plt.title('Prob @ Correct of all methods in comparison')\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig('results_bar_plot.jpg')\n",
    "#plt.savefig('results_bar_plot.pdf')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check out the scaled dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 12 21 32 45]\n",
      "[0.04347826 0.10434783 0.1826087  0.27826087 0.39130435]\n",
      "[0.52025766 0.21677431 0.12387109 0.08129042 0.05780652]\n",
      "[0.52025766 0.21677431 0.12387109 0.08129042 0.05780652]\n",
      "[0.06666667 0.13333333 0.2        0.26666667 0.33333333]\n",
      "[0.09913454 0.16522423 0.21243116 0.24783635 0.27537372]\n",
      "[0.04447238 0.12088846 0.19931138 0.27816143 0.35716635]\n",
      "[0.06739723 0.15267056 0.21575246 0.26346846 0.30071129]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import digamma\n",
    "#define all the functions\n",
    "\n",
    "#define closure function\n",
    "def pertubation(a,b):\n",
    "    return(a*b)\n",
    "\n",
    "def closure(x, k=1):\n",
    "    return(k*x/x.sum())\n",
    "\n",
    "def inverse_closure(x):\n",
    "    inv = 1/(x + 10e-8)\n",
    "    return(inv/inv.sum())\n",
    "\n",
    "def dirichlet_mode_a(alpha):\n",
    "    return(closure(alpha))\n",
    "\n",
    "def dirichlet_expected_a(alpha):\n",
    "    return(closure(np.exp(digamma(alpha))))\n",
    "\n",
    "def scaled_dirichlet_mode_a(alpha, beta):\n",
    "    return(closure(pertubation(inverse_closure(beta), dirichlet_mode_a(alpha))))\n",
    "    \n",
    "def scaled_dirichlet_expected_a(alpha, beta):\n",
    "    return(closure(pertubation(inverse_closure(beta), dirichlet_expected_a(alpha))))\n",
    "\n",
    "def scaled_dirichlet_mode_a_batch(alpha, beta):\n",
    "    r = []\n",
    "    for i in range(len(alpha)):\n",
    "        r.append(scaled_dirichlet_mode_a(alpha[i], beta[i]))\n",
    "        \n",
    "    return(torch.stack(r).numpy())\n",
    "\n",
    "def scaled_dirichlet_expected_a_batch(alpha, beta):\n",
    "    r = []\n",
    "    for i in range(len(alpha)):\n",
    "        r.append(scaled_dirichlet_expected_a(alpha[i], beta[i]))\n",
    "        \n",
    "    return(torch.stack(r).numpy())\n",
    "\n",
    "test_a = np.array(range(1,6))\n",
    "test_b = np.array(range(5, 10))\n",
    "test_p = pertubation(test_a, test_b)\n",
    "test_c = closure(test_p)\n",
    "test_inv_c = inverse_closure(test_c)\n",
    "test_c2 = closure(test_inv_c)\n",
    "           \n",
    "test_mode_a = dirichlet_mode_a(test_a)\n",
    "test_scaled_mode_a = scaled_dirichlet_mode_a(test_a, test_b)\n",
    "test_E_a = dirichlet_expected_a(test_a)\n",
    "test_scaled_E_a = scaled_dirichlet_expected_a(test_a, test_b)\n",
    "print(test_p)\n",
    "print(test_c)\n",
    "print(test_inv_c)\n",
    "print(test_c2)\n",
    "print(test_mode_a)\n",
    "print(test_scaled_mode_a)\n",
    "print(test_E_a)\n",
    "print(test_scaled_E_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on data\n",
    "#McKay\n",
    "alphas_var_mnist_in_DIR_LPA_MC = torch.Tensor(alphas_variance(mnist_test_in_DIR_LPA_MC))\n",
    "alphas_var_out_FMNIST_DIR_LPA_MC = torch.Tensor(alphas_variance(mnist_test_out_FMNIST_DIR_LPA_MC))\n",
    "alphas_var_out_notMNIST_DIR_LPA_MC = torch.Tensor(alphas_variance(mnist_test_out_notMNIST_DIR_LPA_MC))\n",
    "alphas_var_out_FMNIST_normalized_DIR_LPA_MC = torch.Tensor(alphas_variance(mnist_test_out_FMNIST_normalized_DIR_LPA_MC))\n",
    "\n",
    "in_mnist_test_scaled_MC = scaled_dirichlet_expected_a_batch(torch.Tensor(mnist_test_in_DIR_LPA_MC), alphas_var_mnist_in_DIR_LPA_MC)\n",
    "out_FMNIST_test_scaled_MC = scaled_dirichlet_expected_a_batch(torch.Tensor(mnist_test_out_FMNIST_DIR_LPA_MC), alphas_var_out_FMNIST_DIR_LPA_MC)\n",
    "out_notMNIST_test_scaled_MC = scaled_dirichlet_expected_a_batch(torch.Tensor(mnist_test_out_notMNIST_DIR_LPA_MC), alphas_var_out_notMNIST_DIR_LPA_MC)\n",
    "out_FMNIST_normalized_test_scaled_MC = scaled_dirichlet_expected_a_batch(torch.Tensor(mnist_test_out_FMNIST_normalized_DIR_LPA_MC), alphas_var_out_FMNIST_normalized_DIR_LPA_MC)\n",
    "\n",
    "in_mnist_test_scaled_mode_MC = scaled_dirichlet_mode_a_batch(torch.Tensor(mnist_test_in_DIR_LPA_MC), alphas_var_mnist_in_DIR_LPA_MC)\n",
    "out_FMNIST_test_scaled_mode_MC = scaled_dirichlet_mode_a_batch(torch.Tensor(mnist_test_out_FMNIST_DIR_LPA_MC), alphas_var_out_FMNIST_DIR_LPA_MC)\n",
    "out_notMNIST_test_scaled_mode_MC = scaled_dirichlet_mode_a_batch(torch.Tensor(mnist_test_out_notMNIST_DIR_LPA_MC), alphas_var_out_notMNIST_DIR_LPA_MC)\n",
    "out_FMNIST_normalized_test_scaled_mode_MC = scaled_dirichlet_mode_a_batch(torch.Tensor(mnist_test_out_FMNIST_normalized_DIR_LPA_MC), alphas_var_out_FMNIST_normalized_DIR_LPA_MC)\n",
    "#MAP match\n",
    "in_mnist_test_scaled_MM = get_norm_variance_alphas(mnist_test_in_DIR_LPA_MM)\n",
    "out_FMNIST_test_scaled_MM = get_norm_variance_alphas(mnist_test_out_FMNIST_DIR_LPA_MM)\n",
    "out_notMNIST_test_scaled_MM = get_norm_variance_alphas(mnist_test_out_notMNIST_DIR_LPA_MM)\n",
    "out_FMNIST_normalized_test_scaled_MM = get_norm_variance_alphas(mnist_test_out_FMNIST_normalized_DIR_LPA_MM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, var_DIR_LPA_MC, mnist] Accuracy: 0.989; average entropy: 0.056;     MMC: 0.983; Prob @ correct: 0.977\n",
      "[Out-fmnist, var_DIR_LPA_MC, mnist] Accuracy: 0.100; Average entropy: 0.599;    MMC: 0.781; AUROC: 0.907; Prob @ correct: 0.108\n",
      "[Out-notMNIST, var_DIR_LPA_MC, mnist] Accuracy: 0.131; Average entropy: 0.450;    MMC: 0.827; AUROC: 0.827; Prob @ correct: 0.130\n",
      "[Out-FMNISTn, var_DIR_LPA_MC, mnist] Accuracy: 0.089; Average entropy: 0.421;    MMC: 0.838; AUROC: 0.848; Prob @ correct: 0.094\n"
     ]
    }
   ],
   "source": [
    "acc_in_var_DIR_LPA_MC, prob_correct_in_var_DIR_LPA_MC, ent_in_var_DIR_LPA_MC, MMC_in_var_DIR_LPA_MC = get_in_dist_values(in_mnist_test_scaled_MC, targets)\n",
    "acc_out_FMNIST_var_DIR_LPA_MC, prob_correct_out_FMNIST_var_DIR_LPA_MC, ent_out_FMNIST_var_DIR_LPA_MC, MMC_out_FMNIST_var_DIR_LPA_MC, auroc_out_FMNIST_var_DIR_LPA_MC = get_out_dist_values(in_mnist_test_scaled_MC, out_FMNIST_test_scaled_MC, targets_FMNIST)\n",
    "acc_out_notMNIST_var_DIR_LPA_MC, prob_correct_out_notMNIST_var_DIR_LPA_MC, ent_out_notMNIST_var_DIR_LPA_MC, MMC_out_notMNIST_var_DIR_LPA_MC, auroc_out_notMNIST_var_DIR_LPA_MC = get_out_dist_values(in_mnist_test_scaled_MC, out_notMNIST_test_scaled_MC, targets_notMNIST)\n",
    "acc_out_FMNISTn_var_DIR_LPA_MC, prob_correct_out_FMNISTn_var_DIR_LPA_MC, ent_out_FMNISTn_var_DIR_LPA_MC, MMC_out_FMNISTn_var_DIR_LPA_MC, auroc_out_FMNISTn_var_DIR_LPA_MC = get_out_dist_values(in_mnist_test_scaled_MC, out_FMNIST_normalized_test_scaled_MC, targets_FMNIST_normalized)\n",
    "print_in_dist_values(acc_in_var_DIR_LPA_MC, prob_correct_in_var_DIR_LPA_MC, ent_in_var_DIR_LPA_MC, MMC_in_var_DIR_LPA_MC, 'mnist', 'var_DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_FMNIST_var_DIR_LPA_MC, prob_correct_out_FMNIST_var_DIR_LPA_MC, ent_out_FMNIST_var_DIR_LPA_MC, MMC_out_FMNIST_var_DIR_LPA_MC, auroc_out_FMNIST_var_DIR_LPA_MC, test='fmnist', method='var_DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_notMNIST_var_DIR_LPA_MC, prob_correct_out_notMNIST_var_DIR_LPA_MC, ent_out_notMNIST_var_DIR_LPA_MC, MMC_out_notMNIST_var_DIR_LPA_MC, auroc_out_notMNIST_var_DIR_LPA_MC, test='notMNIST', method='var_DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_FMNISTn_var_DIR_LPA_MC, prob_correct_out_FMNISTn_var_DIR_LPA_MC, ent_out_FMNISTn_var_DIR_LPA_MC, MMC_out_FMNISTn_var_DIR_LPA_MC, auroc_out_FMNISTn_var_DIR_LPA_MC, test='FMNISTn', method='var_DIR_LPA_MC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, var_DIR_LPA_MC, mnist] Accuracy: 0.989; average entropy: 0.107;     MMC: 0.971; Prob @ correct: 0.966\n",
      "[Out-fmnist, var_DIR_LPA_MC, mnist] Accuracy: 0.100; Average entropy: 2.227;    MMC: 0.182; AUROC: 0.997; Prob @ correct: 0.098\n",
      "[Out-notMNIST, var_DIR_LPA_MC, mnist] Accuracy: 0.131; Average entropy: 1.702;    MMC: 0.420; AUROC: 0.948; Prob @ correct: 0.109\n",
      "[Out-FMNISTn, var_DIR_LPA_MC, mnist] Accuracy: 0.089; Average entropy: 0.978;    MMC: 0.678; AUROC: 0.873; Prob @ correct: 0.094\n"
     ]
    }
   ],
   "source": [
    "acc_in_var_DIR_LPA_MC, prob_correct_in_var_DIR_LPA_MC, ent_in_var_DIR_LPA_MC, MMC_in_var_DIR_LPA_MC = get_in_dist_values(in_mnist_test_scaled_mode_MC, targets)\n",
    "acc_out_FMNIST_var_DIR_LPA_MC, prob_correct_out_FMNIST_var_DIR_LPA_MC, ent_out_FMNIST_var_DIR_LPA_MC, MMC_out_FMNIST_var_DIR_LPA_MC, auroc_out_FMNIST_var_DIR_LPA_MC = get_out_dist_values(in_mnist_test_scaled_mode_MC, out_FMNIST_test_scaled_mode_MC, targets_FMNIST)\n",
    "acc_out_notMNIST_var_DIR_LPA_MC, prob_correct_out_notMNIST_var_DIR_LPA_MC, ent_out_notMNIST_var_DIR_LPA_MC, MMC_out_notMNIST_var_DIR_LPA_MC, auroc_out_notMNIST_var_DIR_LPA_MC = get_out_dist_values(in_mnist_test_scaled_mode_MC, out_notMNIST_test_scaled_mode_MC, targets_notMNIST)\n",
    "acc_out_FMNISTn_var_DIR_LPA_MC, prob_correct_out_FMNISTn_var_DIR_LPA_MC, ent_out_FMNISTn_var_DIR_LPA_MC, MMC_out_FMNISTn_var_DIR_LPA_MC, auroc_out_FMNISTn_var_DIR_LPA_MC = get_out_dist_values(in_mnist_test_scaled_mode_MC, out_FMNIST_normalized_test_scaled_mode_MC, targets_FMNIST_normalized)\n",
    "print_in_dist_values(acc_in_var_DIR_LPA_MC, prob_correct_in_var_DIR_LPA_MC, ent_in_var_DIR_LPA_MC, MMC_in_var_DIR_LPA_MC, 'mnist', 'var_DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_FMNIST_var_DIR_LPA_MC, prob_correct_out_FMNIST_var_DIR_LPA_MC, ent_out_FMNIST_var_DIR_LPA_MC, MMC_out_FMNIST_var_DIR_LPA_MC, auroc_out_FMNIST_var_DIR_LPA_MC, test='fmnist', method='var_DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_notMNIST_var_DIR_LPA_MC, prob_correct_out_notMNIST_var_DIR_LPA_MC, ent_out_notMNIST_var_DIR_LPA_MC, MMC_out_notMNIST_var_DIR_LPA_MC, auroc_out_notMNIST_var_DIR_LPA_MC, test='notMNIST', method='var_DIR_LPA_MC')\n",
    "print_out_dist_values(acc_out_FMNISTn_var_DIR_LPA_MC, prob_correct_out_FMNISTn_var_DIR_LPA_MC, ent_out_FMNISTn_var_DIR_LPA_MC, MMC_out_FMNISTn_var_DIR_LPA_MC, auroc_out_FMNISTn_var_DIR_LPA_MC, test='FMNISTn', method='var_DIR_LPA_MC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[In, var_DIR_LPA_MC, mnist] Accuracy: 0.989; average entropy: 0.196;     MMC: 0.952; Prob @ correct: 0.948\n",
    "[Out-fmnist, var_DIR_LPA_MC, mnist] Accuracy: 0.100; Average entropy: 2.228;    MMC: 0.182; AUROC: 0.994; Prob @ correct: 0.098\n",
    "[Out-notMNIST, var_DIR_LPA_MC, mnist] Accuracy: 0.131; Average entropy: 1.729;    MMC: 0.413; AUROC: 0.938; Prob @ correct: 0.109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-545770523ba2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-545770523ba2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    num units in last hidden layer = 250\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "num units in last hidden layer = 250\n",
    "\n",
    "[In, LLLA-KF, mnist] Accuracy: 0.989; average entropy: 0.041; MMC: 0.988\n",
    "[Out-FMNIST, MAP, mnist] Average entropy: 1.363; MMC: 0.531; AUROC: 0.991\n",
    "[Out-notMNIST, MAP, mnist] Average entropy: 0.806; MMC: 0.718; AUROC: 0.934\n",
    "[Out-normalized FMNIST, MAP, mnist] Average entropy: 0.421; MMC: 0.841; AUROC: 0.880\n",
    "            \n",
    "[In, LLLA-KF, mnist] Accuracy: 0.988; average entropy: 0.309; MMC: 0.909\n",
    "[Out-FMNIST, Diag, mnist] Average entropy: 1.514; MMC: 0.470; AUROC: 0.972\n",
    "[Out-notMNIST, Diag, mnist] Average entropy: 1.094; MMC: 0.613; AUROC: 0.890\n",
    "[Out-normalized FMNIST, Diag, mnist] Average entropy: 0.955; MMC: 0.634; AUROC: 0.908\n",
    "            \n",
    "[In, LLLA-KF, mnist] Accuracy: 0.986; average entropy: 0.975; MMC: 0.720\n",
    "[Out-FMNIST, KFAC, mnist] Average entropy: 2.081; MMC: 0.246; AUROC: 0.991\n",
    "[Out-notMNIST, KFAC, mnist] Average entropy: 1.979; MMC: 0.302; AUROC: 0.961\n",
    "[Out-normalized FMNIST, KFAC, mnist] Average entropy: 2.008; MMC: 0.275; AUROC: 0.984\n",
    "            \n",
    "[In, LLLA-KF, mnist] Accuracy: 0.989; average entropy: 0.043; MMC: 0.987\n",
    "[Out-FMNIST, DIR_LPA, mnist] Average entropy: 1.774; MMC: 0.392; AUROC: 0.995\n",
    "[Out-notMNIST, DIR_LPA, mnist] Average entropy: 1.022; MMC: 0.657; AUROC: 0.938\n",
    "[Out-normalized FMNIST, DIR_LPA, mnist] Average entropy: 0.462; MMC: 0.832; AUROC: 0.878\n",
    "            \n",
    "[In, LLLA-KF, mnist] Accuracy: 0.987; average entropy: 0.048; MMC: 0.986\n",
    "[Out-FMNIST, DIR_LPA, mnist] Average entropy: 1.849; MMC: 0.347; AUROC: 0.996\n",
    "[Out-notMNIST, DIR_LPA, mnist] Average entropy: 1.113; MMC: 0.627; AUROC: 0.949\n",
    "[Out-normalized FMNIST, DIR_LPA, mnist] Average entropy: 0.516; MMC: 0.815; AUROC: 0.889\n",
    "            \n",
    "[In, LLLA-KF, mnist] Accuracy: 0.989; average entropy: 0.191; MMC: 0.953\n",
    "[Out-FMNIST, DIR_LPA, mnist] Average entropy: 2.181; MMC: 0.227; AUROC: 0.991\n",
    "[Out-notMNIST, DIR_LPA, mnist] Average entropy: 1.673; MMC: 0.447; AUROC: 0.934\n",
    "[Out-normalized FMNIST, DIR_LPA, mnist] Average entropy: 1.288; MMC: 0.602; AUROC: 0.873\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
