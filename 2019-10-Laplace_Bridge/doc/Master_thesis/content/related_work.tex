

In Bayesian neural networks, analytic approximations of posterior predictive distributions have attracted a great deal of research. In the binary classification case, for example, the probit approximation has been proposed already in the 1990s \citep{spiegelhalter1990sequential,mackay1992evidence}. However, while there exist some bounds \citep{michalis2016one} and approximations of the expected log-sum-exponent function \citep{ahmed2007tight,braun2010variational}, in the multi-class case, obtaining a good analytic approximation of the expected softmax function under a Gaussian measure is still considered an open problem. The Laplace Bridge is of interest in this domain, too, as the approximation of this integral can be analytically computed via \eqref{eq:dirichlet_mean}.

Approaches like \cite{DeterministicVI2018} and \cite{haussmann2019BEDLwithPAC} show the effectiveness of, and a general desire to provide sampling free methods for Bayesian Deep Learning. The Laplace Bridge furthers this trend since it is also sampling-free.

Recently, it has been proposed to model the distribution of softmax outputs of a network directly. Similar to the Laplace Bridge, \citet{NIPS2018PriorNetworks,NIPS2019PriorNetworks_improved,NIPS2018EvidentialDL} proposed to use the Dirichlet distribution to model the posterior predictive for non-Bayesian networks. They further proposed novel training techniques in order to directly learn the Dirichlet. In contrast, the Laplace bridge tackles the problem of approximating the distribution over the softmax outputs of the ubiquitous Gaussian-approximated Bayesian networks \citep[etc]{Graves2011VB,Blundell2015WeightUI,louizos_structured_2016,sun_learning_2017} without any additional training procedure. Further differences between the Laplace Bridge and  \cite{NIPS2018PriorNetworks,NIPS2019PriorNetworks_improved,NIPS2018EvidentialDL} include a) they require retraining of the network while ours can use pre-trained weights. The Laplace Bridge is, therefore, easier to apply to already-existing architectures and b) \cite{NIPS2018PriorNetworks,NIPS2019PriorNetworks_improved} require OOD samples during training while our method bases its uncertainty estimate solely on the information already included in the weights.


