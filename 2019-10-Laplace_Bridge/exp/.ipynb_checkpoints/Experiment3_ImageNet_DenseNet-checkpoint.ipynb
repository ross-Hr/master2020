{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe768a206d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "#import input_data\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from math import *\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import KFAC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from tqdm import tqdm, trange\n",
    "from backpack.core.layers import Flatten\n",
    "import pytest\n",
    "from DirLPA_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "#import ssms\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained Densenet for Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load imagenet\n",
    "imagenet_train_root = os.path.abspath('/media/marius/2e21db24-75e5-4af2-97c9-ad76aadaa247/marius/data/imagenet/ILSVRC2012_img_train')\n",
    "imagenet_val_root = os.path.abspath('/media/marius/2e21db24-75e5-4af2-97c9-ad76aadaa247/marius/data/imagenet/ILSVRC2012_img_val')\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform_imagenet_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "transform_imagenet_val =  transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "imagenet_train = datasets.ImageFolder(imagenet_train_root, transform=transform_imagenet_train)\n",
    "imagenet_val = datasets.ImageFolder(imagenet_val_root, transform=transform_imagenet_val)\n",
    "\n",
    "#print(imagenet_train[0])\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        imagenet_train,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        imagenet_val,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "from torch import Tensor\n",
    "#from torch.jit.annotations import List\n",
    "\n",
    "\n",
    "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\n",
    "\n",
    "model_urls = {\n",
    "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
    "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
    "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
    "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "        self.drop_rate = float(drop_rate)\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def bn_function(self, inputs):\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
    "        return bottleneck_output\n",
    "\n",
    "    # todo: rewrite when torchscript supports any\n",
    "    def any_requires_grad(self, input):\n",
    "        # type: (List[Tensor]) -> bool\n",
    "        for tensor in input:\n",
    "            if tensor.requires_grad:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    #@torch.jit.unused  # noqa: T484\n",
    "    def call_checkpoint_bottleneck(self, input):\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        def closure(*inputs):\n",
    "            return self.bn_function(*inputs)\n",
    "\n",
    "        return cp.checkpoint(closure, input)\n",
    "\n",
    "    #@torch.jit._overload_method  # noqa: F811\n",
    "    def forward(self, input):\n",
    "        # type: (List[Tensor]) -> (Tensor)\n",
    "        pass\n",
    "\n",
    "    #@torch.jit._overload_method  # noqa: F811\n",
    "    def forward(self, input):\n",
    "        # type: (Tensor) -> (Tensor)\n",
    "        pass\n",
    "\n",
    "    # torchscript does not yet support *args, so we overload method\n",
    "    # allowing it to take either a List[Tensor] or single Tensor\n",
    "    def forward(self, input):  # noqa: F811\n",
    "        if isinstance(input, Tensor):\n",
    "            prev_features = [input]\n",
    "        else:\n",
    "            prev_features = input\n",
    "\n",
    "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
    "            if torch.jit.is_scripting():\n",
    "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
    "\n",
    "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
    "        else:\n",
    "            bottleneck_output = self.bn_function(prev_features)\n",
    "\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.ModuleDict):\n",
    "    _version = 2\n",
    "    __constants__ = ['layers']\n",
    "\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = ['features']\n",
    "\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.phi(x)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "    def phi(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        \n",
    "        return(out)\n",
    "\n",
    "\n",
    "def _load_state_dict(model, model_url, progress):\n",
    "    # '.'s are no longer allowed in module names, but previous _DenseLayer\n",
    "    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
    "    # They are also in the checkpoints in model_urls. This pattern is used\n",
    "    # to find such keys.\n",
    "    pattern = re.compile(\n",
    "        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
    "\n",
    "    state_dict = load_state_dict_from_url(model_url, progress=progress)\n",
    "    for key in list(state_dict.keys()):\n",
    "        res = pattern.match(key)\n",
    "        if res:\n",
    "            new_key = res.group(1) + res.group(2)\n",
    "            state_dict[new_key] = state_dict[key]\n",
    "            del state_dict[key]\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress,\n",
    "              **kwargs):\n",
    "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
    "    if pretrained:\n",
    "        _load_state_dict(model, model_urls[arch], progress)\n",
    "    return model\n",
    "\n",
    "\n",
    "def densenet121(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Densenet-121 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\n",
    "                     **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load densenet\n",
    "\n",
    "densenet = densenet121(pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test inception on the training and testset and check accuracies\n",
    "\n",
    "def eval_imagenet():\n",
    "    \n",
    "    densenet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    max_idx = len(val_loader)\n",
    "\n",
    "    imagenet_MAP = []\n",
    "    for batch_idx, (x,y) in enumerate(val_loader):\n",
    "        x,y = x.cuda(), y.cuda()\n",
    "        pred_ = densenet(x).detach()\n",
    "        probs = torch.softmax(pred_, dim=1)\n",
    "        imagenet_MAP.append(probs)\n",
    "        prob_max, pred = probs.max(1)\n",
    "        correct += torch.sum(torch.eq(pred, y)).item()\n",
    "        total += len(y)\n",
    "        #print(prob, pred, y)\n",
    "        #print(correct, total)\n",
    "        print(\"batch {} of {}\".format(batch_idx, max_idx))\n",
    "\n",
    "    print(\"accuracy on the validation set: {:.03f}\".format(correct/total))\n",
    "    imagenet_MAP = torch.cat(imagenet_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_imagenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimagenet_MAP_dict = {\\n    'imagenet_MAP':imagenet_MAP\\n}\\n\\nIMAGENET_MAP_PATH = 'imagenet_MAP.pth'\\ntorch.save(imagenet_MAP_dict, IMAGENET_MAP_PATH)\\n#\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "imagenet_MAP_dict = {\n",
    "    'imagenet_MAP':imagenet_MAP\n",
    "}\n",
    "\n",
    "IMAGENET_MAP_PATH = 'imagenet_MAP.pth'\n",
    "torch.save(imagenet_MAP_dict, IMAGENET_MAP_PATH)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.74434"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGENET_MAP_PATH = 'imagenet_MAP.pth'\n",
    "imagenet_MAP_dict = torch.load(IMAGENET_MAP_PATH)\n",
    "imagenet_MAP = imagenet_MAP_dict['imagenet_MAP']\n",
    "print(imagenet_MAP.size())\n",
    "\n",
    "#check accuracy\n",
    "targets = np.array(imagenet_val.targets)\n",
    "prob_MAP, pred_MAP = imagenet_MAP.max(1)\n",
    "pred_MAP = pred_MAP.cpu().numpy()\n",
    "np.equal(targets, pred_MAP).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to approximate the diagonal Hessian of the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diag_second_order(model, train_loader, var0 = 10, device='cpu'):\n",
    "\n",
    "    W = list(model.parameters())[-2]\n",
    "    b = list(model.parameters())[-1]\n",
    "    m, n = W.shape\n",
    "    print(\"n: {} inputs to linear layer with m: {} classes\".format(n, m))\n",
    "    lossfunc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    tau = 1/var0\n",
    "\n",
    "    extend(lossfunc, debug=False)\n",
    "    extend(model.classifier, debug=False)\n",
    "\n",
    "    with backpack(DiagHessian()):\n",
    "\n",
    "        max_len = len(train_loader)\n",
    "        weights_cov = torch.zeros(m, n, device=device)\n",
    "        biases_cov = torch.zeros(m, device=device)\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "\n",
    "            if device == 'cuda':\n",
    "                x, y = x.float().cuda(), y.long().cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            lossfunc(model(x), y).backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Hessian of weight\n",
    "                W_ = W.diag_h\n",
    "                b_ = b.diag_h\n",
    "\n",
    "            rho = min(1-1/(batch_idx+1), 0.995)\n",
    "                \n",
    "            weights_cov = rho * weights_cov + (1-rho) * W_\n",
    "            biases_cov = rho * biases_cov + (1-rho) * b_\n",
    "\n",
    "            print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "\n",
    "            #print(\"Cov_M size: \", weights_cov.size())\n",
    "            #print(\"Cov_b size: \", biases_cov.size())\n",
    "\n",
    "    # Predictive distribution\n",
    "    with torch.no_grad():\n",
    "        M_W_post = W.t()\n",
    "        M_b_post = b\n",
    "\n",
    "        C_W_post = weights_cov\n",
    "        C_b_post = biases_cov\n",
    "    \n",
    "        print(\"M_W_post size: \", M_W_post.size())\n",
    "        print(\"M_b_post size: \", M_b_post.size())\n",
    "        print(\"C_W_post size: \", C_W_post.size())\n",
    "        print(\"C_b_post size: \", C_b_post.size())\n",
    "\n",
    "    return(M_W_post, M_b_post, C_W_post, C_b_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nM_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D = Diag_second_order(model=densenet,\\n                                                               train_loader=train_loader,\\n                                                               var0 = 10,\\n                                                               device='cuda')\\n#\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D = Diag_second_order(model=densenet,\n",
    "                                                               train_loader=train_loader,\n",
    "                                                               var0 = 10,\n",
    "                                                               device='cuda')\n",
    "#\"\"\"                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmatric_dict = {\\n    'M_W_post_D':M_W_post_D,\\n    'M_b_post_D':M_b_post_D,\\n    'C_W_post_D':C_W_post_D,\\n    'C_b_post_D':C_b_post_D\\n}\\n\\nMATRIX_PATH = 'matrices_diag.pth'\\ntorch.save(matric_dict, MATRIX_PATH)\\n#\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since calculating the matrices takes forever it is reasonable to save them\n",
    "\"\"\"\n",
    "matric_dict = {\n",
    "    'M_W_post_D':M_W_post_D,\n",
    "    'M_b_post_D':M_b_post_D,\n",
    "    'C_W_post_D':C_W_post_D,\n",
    "    'C_b_post_D':C_b_post_D\n",
    "}\n",
    "\n",
    "MATRIX_PATH = 'matrices_diag.pth'\n",
    "torch.save(matric_dict, MATRIX_PATH)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'matrices_diag.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-697f5ff0f616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# here you can load the matrices if you calculated them once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMATRIX_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'matrices_diag.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmatrix_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMATRIX_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mM_W_post_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_b_post_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'M_W_post_D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'M_b_post_D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mC_W_post_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_b_post_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'C_W_post_D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'C_b_post_D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'matrices_diag.pth'"
     ]
    }
   ],
   "source": [
    "# here you can load the matrices if you calculated them once\n",
    "MATRIX_PATH = 'matrices_diag.pth'\n",
    "matrix_dict = torch.load(MATRIX_PATH)\n",
    "M_W_post_D, M_b_post_D = matrix_dict['M_W_post_D'], matrix_dict['M_b_post_D']\n",
    "C_W_post_D, C_b_post_D = matrix_dict['C_W_post_D'], matrix_dict['C_b_post_D']\n",
    "print(\"M_W_post_D size: \", M_W_post_D.size())\n",
    "print(\"M_b_post_D size: \", M_b_post_D.size())\n",
    "print(\"C_W_post_D size: \", C_W_post_D.size())\n",
    "print(\"C_b_post_D size: \", C_b_post_D.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gaussian_output(x, mu_w, mu_b, sigma_w, sigma_b):\n",
    "    #get the distributions per class\n",
    "    batch_size = x.size(0)\n",
    "    num_classes = len(mu_b)\n",
    "    #print(\"batch_size, num_classes: \", batch_size, num_classes)\n",
    "    mu_batch = torch.zeros(batch_size, num_classes)\n",
    "    sigma_batch = torch.zeros(batch_size, num_classes, num_classes)\n",
    "    for i in range(batch_size):\n",
    "        per_class_sigmas = torch.zeros(num_classes)\n",
    "        for j in range(num_classes):\n",
    "            #create a diagonal Hessian\n",
    "            #print(sigma_w[j].size())\n",
    "            hess = torch.diag(sigma_w[j])\n",
    "            #b = x[i] @ hess @ x[i].t()\n",
    "            #a = sigma_b[i]\n",
    "            per_class_sigmas[j] = x[i] @ hess @ x[i].t() + sigma_b[j]\n",
    "\n",
    "        #print(\"sizes: \", mu_w.size(), x[i].size(), mu_b.size())\n",
    "        per_class_mus = x[i] @ mu_w + mu_b\n",
    "        mu_batch[i] = per_class_mus\n",
    "        sigma_batch[i] = torch.diag(per_class_sigmas)\n",
    "\n",
    "    return(mu_batch, sigma_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_diagonal_sampling(model, test_loader, M_W_post, M_b_post, C_W_post, C_b_post, n_samples, verbose=False, cuda=True):\n",
    "    py = []\n",
    "    max_len = len(test_loader)\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "\n",
    "        if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        phi = model.phi(x).detach()\n",
    "\n",
    "        mu, Sigma = get_Gaussian_output(phi, M_W_post, M_b_post, C_W_post, C_b_post)\n",
    "\n",
    "        post_pred = MultivariateNormal(mu, Sigma)\n",
    "\n",
    "        # MC-integral\n",
    "        py_ = 0\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            f_s = post_pred.rsample()\n",
    "            py_ += torch.softmax(f_s, 1)\n",
    "\n",
    "        py_ /= n_samples\n",
    "        py_ = py_.detach()\n",
    "\n",
    "        py.append(py_)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "\n",
    "    return torch.cat(py, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_Diag = predict_diagonal_sampling(densenet, val_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "imagenet_Diag_dict = {\n",
    "    'imagenet_Diag':imagenet_Diag\n",
    "}\n",
    "\n",
    "IMAGENET_DIAG_PATH = 'imagenet_Diag.pth'\n",
    "torch.save(imagenet_Diag_dict, IMAGENET_DIAG_PATH)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_DIAG_PATH = 'imagenet_Diag.pth'\n",
    "imagenet_Diag_dict = torch.load(IMAGENET_DIAG_PATH)\n",
    "imagenet_Diag = imagenet_Diag_dict['imagenet_Diag']\n",
    "print(imagenet_Diag.size())\n",
    "\n",
    "#check accuracy\n",
    "targets = np.array(imagenet_val.targets)\n",
    "prob_D, pred_D = imagenet_Diag.max(1)\n",
    "pred_D = pred_D.cpu().numpy()\n",
    "np.equal(targets, pred_D).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check out confusion matrix for MAP\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406])\n",
    "std = torch.Tensor([0.229, 0.224, 0.225])\n",
    "unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "#normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def imshow(img):\n",
    "    img = unnormalize(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "targets = np.array(imagenet_val.targets)\n",
    "_, pred = imagenet_MAP.max(1)\n",
    "pred = pred.cpu()\n",
    "cm = confusion_matrix(targets, pred)\n",
    "np.fill_diagonal(cm, 0)\n",
    "\n",
    "    \n",
    "#get max index\n",
    "max_idx = np.unravel_index(cm.argmax(), cm.shape)\n",
    "print(max_idx)\n",
    "print(cm[620, 681])\n",
    "print(cm[681, 620])\n",
    "idx_620 = np.where(targets==620)\n",
    "idx_681 = np.where(targets==681)\n",
    "#turns out that 620 is \"laptop, laptop computer\" and 681 is \"notebook, notebook computer\"\n",
    "print(\"idx 620: \", idx_620)\n",
    "print(\"idx 681: \", idx_681)\n",
    "imshow(imagenet_val[31003][0])\n",
    "top5_31003 = np.argpartition(imagenet_MAP[31003].cpu().numpy(), -5)[-5:]\n",
    "print(top5_31003)\n",
    "print(imagenet_MAP[31003][top5_31003])\n",
    "imshow(imagenet_val[34054][0])\n",
    "top5_34054 = np.argpartition(imagenet_MAP[34054].cpu().numpy(), -5)[-5:]\n",
    "print(top5_34054)\n",
    "print(imagenet_MAP[34054][top5_34054])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check out DIR_LPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_from_Normal(mu, Sigma):\n",
    "    batch_size, K = mu.size(0), mu.size(-1)\n",
    "    Sigma_d = torch.diagonal(Sigma, dim1=1, dim2=2)\n",
    "    sum_exp = torch.sum(torch.exp(-1*mu), dim=1).view(-1,1)\n",
    "    alpha = 1/Sigma_d * (1 - 2/K + torch.exp(mu)/K**2 * sum_exp)\n",
    "    \n",
    "    return(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_DIR_LPA(model, test_loader, M_W_post, M_b_post, C_W_post, C_b_post, verbose=False, cuda=False):\n",
    "    alphas = []\n",
    "\n",
    "    max_len = len(test_loader)\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        \n",
    "        if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        phi = model.phi(x).detach()\n",
    "\n",
    "        mu_pred, Cov_pred = get_Gaussian_output(phi, M_W_post, M_b_post, C_W_post, C_b_post)\n",
    "\n",
    "        alpha = get_alpha_from_Normal(mu_pred, Cov_pred).detach()\n",
    "        #alpha /= alpha.sum(dim=1).view(-1,1).detach()\n",
    "\n",
    "        alphas.append(alpha)\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "\n",
    "    return(torch.cat(alphas, dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagenet_DIR_LPA = predict_DIR_LPA(densenet, val_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=True, cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "imagenet_DIR_LPA_dict = {\n",
    "    'imagenet_DIR_LPA':imagenet_DIR_LPA\n",
    "}\n",
    "\n",
    "IMAGENET_DIR_LPA_PATH = 'imagenet_DIR_LPA.pth'\n",
    "torch.save(imagenet_DIR_LPA_dict, IMAGENET_DIR_LPA_PATH)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_DIR_LPA_PATH = 'imagenet_DIR_LPA.pth'\n",
    "imagenet_DIR_LPA_dict = torch.load(IMAGENET_DIR_LPA_PATH)\n",
    "imagenet_DIR_LPA = imagenet_DIR_LPA_dict['imagenet_DIR_LPA']\n",
    "print(imagenet_DIR_LPA.size())\n",
    "\n",
    "#check accuracy\n",
    "targets = np.array(imagenet_val.targets)\n",
    "prob_DIR_LPA, pred_DIR_LPA = imagenet_DIR_LPA.max(1)\n",
    "pred_DIR_LPA = pred_DIR_LPA.cpu().numpy()\n",
    "np.equal(targets, pred_DIR_LPA).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show how uncertainty differs depending on Dirichlet or Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5(pred):\n",
    "    return(np.argpartition(pred.cpu().numpy(), -5)[-5:])\n",
    "\n",
    "\n",
    "#11 & 12\n",
    "#13 & 9\n",
    "#15 & 6\n",
    "#21 is interesting \n",
    "i = 21\n",
    "top5_MAP_i = get_top5(imagenet_MAP[i])\n",
    "top5_DIR_LPA_i = get_top5(imagenet_DIR_LPA[i])\n",
    "\n",
    "print(top5_MAP_i)\n",
    "print(top5_DIR_LPA_i)\n",
    "print(imagenet_MAP[i][top5_MAP_i])\n",
    "print(imagenet_DIR_LPA[i][top5_DIR_LPA_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "a11 = imagenet_DIR_LPA[11][0]\n",
    "b11 = imagenet_DIR_LPA[11].sum() - a11\n",
    "print(\"a11, b11: \", a11, b11)\n",
    "\n",
    "top5_MAP_11 = get_top5(imagenet_MAP[11])\n",
    "top5_DIR_LPA_11 = get_top5(imagenet_DIR_LPA[11])\n",
    "\n",
    "print(top5_MAP_11)\n",
    "print(top5_DIR_LPA_11)\n",
    "print(imagenet_MAP[11][top5_MAP_11])\n",
    "print(imagenet_DIR_LPA[11][top5_DIR_LPA_11])\n",
    "\n",
    "x = np.linspace(0,1, 10000 + 1)\n",
    "beta_11_pdf = beta.pdf(x, a=a11, b=b11)\n",
    "plt.plot(x, beta_11_pdf)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "a12 = imagenet_DIR_LPA[12][0]\n",
    "b12 = imagenet_DIR_LPA[12].sum() - a12\n",
    "print(\"a12, b12: \", a12, b12)\n",
    "\n",
    "top5_MAP_12 = get_top5(imagenet_MAP[12])\n",
    "top5_DIR_LPA_12 = get_top5(imagenet_DIR_LPA[12])\n",
    "\n",
    "print(top5_MAP_12)\n",
    "print(top5_DIR_LPA_12)\n",
    "print(imagenet_MAP[12][top5_MAP_12])\n",
    "print(imagenet_DIR_LPA[12][top5_DIR_LPA_12])\n",
    "\n",
    "x = np.linspace(0,1, 10000 + 1)\n",
    "beta_12_pdf = beta.pdf(x, a=a12, b=b12)\n",
    "plt.plot(x, beta_12_pdf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a9 = imagenet_DIR_LPA[9][0]\n",
    "b9 = imagenet_DIR_LPA[9].sum() - a9\n",
    "\n",
    "x = np.linspace(0,1, 10000 + 1)\n",
    "beta_9_pdf = beta.pdf(x, a=a9, b=b9)\n",
    "plt.plot(x, beta_9_pdf)\n",
    "plt.show()\n",
    "\n",
    "top5_MAP_9 = get_top5(imagenet_MAP[9])\n",
    "top5_DIR_LPA_9 = get_top5(imagenet_DIR_LPA[9])\n",
    "\n",
    "print(top5_MAP_9)\n",
    "print(top5_DIR_LPA_9)\n",
    "print(imagenet_MAP[9][top5_MAP_9])\n",
    "print(imagenet_DIR_LPA[9][top5_DIR_LPA_9])\n",
    "\n",
    "a13 = imagenet_DIR_LPA[13][0]\n",
    "b13 = imagenet_DIR_LPA[13].sum() - a13\n",
    "\n",
    "top5_MAP_13 = get_top5(imagenet_MAP[13])\n",
    "top5_DIR_LPA_13 = get_top5(imagenet_DIR_LPA[13])\n",
    "\n",
    "print(top5_MAP_13)\n",
    "print(top5_DIR_LPA_13)\n",
    "print(imagenet_MAP[13][top5_MAP_13])\n",
    "print(imagenet_DIR_LPA[13][top5_DIR_LPA_13])\n",
    "\n",
    "x = np.linspace(0,1, 10000 + 1)\n",
    "beta_13_pdf = beta.pdf(x, a=a13, b=b13)\n",
    "plt.plot(x, beta_13_pdf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a21 = imagenet_DIR_LPA[21][0]\n",
    "b21 = imagenet_DIR_LPA[21].sum() - a21\n",
    "\n",
    "x = np.linspace(0,1, 10000 + 1)\n",
    "beta_21_pdf = beta.pdf(x, a=a21, b=b21)\n",
    "plt.plot(x, beta_21_pdf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check out class Laptop vs notebook to see how uncertainty is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top10(pred):\n",
    "    return(np.argpartition(pred.cpu().numpy(), -10)[-10:])\n",
    "\n",
    "i = 31036\n",
    "top5_MAP_i = get_top5(imagenet_MAP[i])\n",
    "top5_DIR_LPA_i = get_top5(imagenet_DIR_LPA[i])\n",
    "\n",
    "#examples for high uncertainty:\n",
    "#31000, 31001, 310\n",
    "\n",
    "#examples for low uncertainty for some classes:\n",
    "#31002, 31010\n",
    "\n",
    "#good examples:\n",
    "#31010, 31026, 31027, 31036\n",
    "\n",
    "\n",
    "print(top5_MAP_i)\n",
    "print(top5_DIR_LPA_i)\n",
    "print(imagenet_MAP[i][top5_MAP_i])\n",
    "print(imagenet_DIR_LPA[i][top5_DIR_LPA_i])\n",
    "\n",
    "imshow(imagenet_val[i][0])\n",
    "\n",
    "def plot_beta_top_5(top5_idx, i):\n",
    "    \n",
    "    x = np.linspace(0,1, 10000 + 1)\n",
    "    alpha_sum = imagenet_DIR_LPA[i].sum()\n",
    "    top5_alphas = imagenet_DIR_LPA[i][top5_idx]\n",
    "    for j, a_x in enumerate(top5_alphas):\n",
    "        b_x = alpha_sum - a_x \n",
    "        beta_x_pdf = beta.pdf(x, a=a_x, b=b_x)\n",
    "        plt.plot(x, beta_x_pdf, label='{}'.format(label_dict[top5_idx[j]]), alpha=0.8)\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_beta_top_5(top5_MAP_i, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.Tensor([0.485, 0.456, 0.406])\n",
    "std = torch.Tensor([0.229, 0.224, 0.225])\n",
    "unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "\n",
    "def imshow(img):\n",
    "    img = unnormalize(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def get_top5(pred):\n",
    "    return(np.argpartition(pred.cpu().numpy(), -5)[-5:])\n",
    "\n",
    "def make_combined_plot(idx1, idx2, filename, show_MAP=False):\n",
    "    \n",
    "    top5_MAP_idx1 = get_top5(imagenet_MAP[idx1])\n",
    "    top5_DIR_LPA_idx1 = get_top5(imagenet_DIR_LPA[idx1])\n",
    "    top5_MAP_idx2 = get_top5(imagenet_MAP[idx2])\n",
    "    top5_DIR_LPA_idx2 = get_top5(imagenet_DIR_LPA[idx2])\n",
    "    top5_MAP1 = imagenet_MAP[idx1][top5_MAP_idx1]\n",
    "    top5_MAP1_class = top5_MAP_idx1[top5_MAP1.argmax()]\n",
    "    top5_MAP2 = imagenet_MAP[idx2][top5_MAP_idx2]\n",
    "    top5_MAP2_class = top5_MAP_idx2[top5_MAP2.argmax()]\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, sharex=False, figsize=(15, 15))\n",
    "    \n",
    "    img_idx1 = unnormalize(imagenet_val[idx1][0])\n",
    "    img_idx2 = unnormalize(imagenet_val[idx2][0])\n",
    "    \n",
    "    axs[0][0].imshow(np.transpose(img_idx1.numpy(), (1, 2, 0)))\n",
    "    axs[0][0].axis('off')\n",
    "    axs[0][1].imshow(np.transpose(img_idx2.numpy(), (1, 2, 0)))\n",
    "    axs[0][1].axis('off')\n",
    "    \n",
    "    x = np.linspace(0,1, 10000 + 1)\n",
    "    #idx1 beta\n",
    "    alpha_sum1 = imagenet_DIR_LPA[idx1].sum()\n",
    "    top5_alphas1 = imagenet_DIR_LPA[idx1][top5_DIR_LPA_idx1]\n",
    "    max_beta1 = 0\n",
    "    for j, a_x in enumerate(top5_alphas1):\n",
    "        b_x = alpha_sum1 - a_x \n",
    "        beta_x_pdf1 = beta.pdf(x, a=a_x, b=b_x)\n",
    "        if max(beta_x_pdf1) > max_beta1:\n",
    "            max_beta1 = max(beta_x_pdf1)\n",
    "        axs[1][0].plot(x, beta_x_pdf1, label='{}'.format(label_dict[top5_DIR_LPA_idx1[j]]), alpha=0.8)\n",
    "\n",
    "    if show_MAP:  \n",
    "        axs[1][0].plot(2*[max(top5_MAP1)], [0, max_beta1], 'r--', label=\"MAP for {}\".format(label_dict[top5_MAP1_class]));\n",
    "    axs[1][0].legend(prop={'size': 13})    \n",
    "    \n",
    "    #idx2 beta\n",
    "    max_beta2 = 0\n",
    "    alpha_sum2 = imagenet_DIR_LPA[idx2].sum()\n",
    "    top5_alphas2 = imagenet_DIR_LPA[idx2][top5_DIR_LPA_idx2]\n",
    "    for j, a_x in enumerate(top5_alphas2):\n",
    "        b_x = alpha_sum2 - a_x \n",
    "        beta_x_pdf2 = beta.pdf(x, a=a_x, b=b_x)\n",
    "        if max(beta_x_pdf2) > max_beta2:\n",
    "            max_beta2 = max(beta_x_pdf2)\n",
    "        axs[1][1].plot(x, beta_x_pdf2, label='{}'.format(label_dict[top5_DIR_LPA_idx2[j]]), alpha=0.8)\n",
    "    \n",
    "    if show_MAP:\n",
    "        axs[1][1].plot(2*[max(top5_MAP2)], [0, max_beta2], 'r--', label=\"MAP for {}\".format(label_dict[top5_MAP2_class]));\n",
    "    axs[1][1].legend(prop={'size': 13})    \n",
    "       \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# big plot for same MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_combined_plot(11, 12, filename=\"figures/same_MAP.pdf\", show_MAP=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# big plot for new top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_combined_plot(31010, 31036, filename=\"figures/topk_replacement.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: 'tench, Tinca tinca',\n",
    " 1: 'goldfish, Carassius auratus',\n",
    " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
    " 3: 'tiger shark, Galeocerdo cuvieri',\n",
    " 4: 'hammerhead, hammerhead shark',\n",
    " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
    " 6: 'stingray',\n",
    " 7: 'cock',\n",
    " 8: 'hen',\n",
    " 9: 'ostrich, Struthio camelus',\n",
    " 10: 'brambling, Fringilla montifringilla',\n",
    " 11: 'goldfinch, Carduelis carduelis',\n",
    " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
    " 13: 'junco, snowbird',\n",
    " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
    " 15: 'robin, American robin, Turdus migratorius',\n",
    " 16: 'bulbul',\n",
    " 17: 'jay',\n",
    " 18: 'magpie',\n",
    " 19: 'chickadee',\n",
    " 20: 'water ouzel, dipper',\n",
    " 21: 'kite',\n",
    " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
    " 23: 'vulture',\n",
    " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
    " 25: 'European fire salamander, Salamandra salamandra',\n",
    " 26: 'common newt, Triturus vulgaris',\n",
    " 27: 'eft',\n",
    " 28: 'spotted salamander, Ambystoma maculatum',\n",
    " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
    " 30: 'bullfrog, Rana catesbeiana',\n",
    " 31: 'tree frog, tree-frog',\n",
    " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
    " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
    " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
    " 35: 'mud turtle',\n",
    " 36: 'terrapin',\n",
    " 37: 'box turtle, box tortoise',\n",
    " 38: 'banded gecko',\n",
    " 39: 'common iguana, iguana, Iguana iguana',\n",
    " 40: 'American chameleon, anole, Anolis carolinensis',\n",
    " 41: 'whiptail, whiptail lizard',\n",
    " 42: 'agama',\n",
    " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
    " 44: 'alligator lizard',\n",
    " 45: 'Gila monster, Heloderma suspectum',\n",
    " 46: 'green lizard, Lacerta viridis',\n",
    " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
    " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
    " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
    " 50: 'American alligator, Alligator mississipiensis',\n",
    " 51: 'triceratops',\n",
    " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
    " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
    " 54: 'hognose snake, puff adder, sand viper',\n",
    " 55: 'green snake, grass snake',\n",
    " 56: 'king snake, kingsnake',\n",
    " 57: 'garter snake, grass snake',\n",
    " 58: 'water snake',\n",
    " 59: 'vine snake',\n",
    " 60: 'night snake, Hypsiglena torquata',\n",
    " 61: 'boa constrictor, Constrictor constrictor',\n",
    " 62: 'rock python, rock snake, Python sebae',\n",
    " 63: 'Indian cobra, Naja naja',\n",
    " 64: 'green mamba',\n",
    " 65: 'sea snake',\n",
    " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
    " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
    " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
    " 69: 'trilobite',\n",
    " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
    " 71: 'scorpion',\n",
    " 72: 'black and gold garden spider, Argiope aurantia',\n",
    " 73: 'barn spider, Araneus cavaticus',\n",
    " 74: 'garden spider, Aranea diademata',\n",
    " 75: 'black widow, Latrodectus mactans',\n",
    " 76: 'tarantula',\n",
    " 77: 'wolf spider, hunting spider',\n",
    " 78: 'tick',\n",
    " 79: 'centipede',\n",
    " 80: 'black grouse',\n",
    " 81: 'ptarmigan',\n",
    " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
    " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
    " 84: 'peacock',\n",
    " 85: 'quail',\n",
    " 86: 'partridge',\n",
    " 87: 'African grey, African gray, Psittacus erithacus',\n",
    " 88: 'macaw',\n",
    " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
    " 90: 'lorikeet',\n",
    " 91: 'coucal',\n",
    " 92: 'bee eater',\n",
    " 93: 'hornbill',\n",
    " 94: 'hummingbird',\n",
    " 95: 'jacamar',\n",
    " 96: 'toucan',\n",
    " 97: 'drake',\n",
    " 98: 'red-breasted merganser, Mergus serrator',\n",
    " 99: 'goose',\n",
    " 100: 'black swan, Cygnus atratus',\n",
    " 101: 'tusker',\n",
    " 102: 'echidna, spiny anteater, anteater',\n",
    " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
    " 104: 'wallaby, brush kangaroo',\n",
    " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
    " 106: 'wombat',\n",
    " 107: 'jellyfish',\n",
    " 108: 'sea anemone, anemone',\n",
    " 109: 'brain coral',\n",
    " 110: 'flatworm, platyhelminth',\n",
    " 111: 'nematode, nematode worm, roundworm',\n",
    " 112: 'conch',\n",
    " 113: 'snail',\n",
    " 114: 'slug',\n",
    " 115: 'sea slug, nudibranch',\n",
    " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
    " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
    " 118: 'Dungeness crab, Cancer magister',\n",
    " 119: 'rock crab, Cancer irroratus',\n",
    " 120: 'fiddler crab',\n",
    " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
    " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
    " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
    " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
    " 125: 'hermit crab',\n",
    " 126: 'isopod',\n",
    " 127: 'white stork, Ciconia ciconia',\n",
    " 128: 'black stork, Ciconia nigra',\n",
    " 129: 'spoonbill',\n",
    " 130: 'flamingo',\n",
    " 131: 'little blue heron, Egretta caerulea',\n",
    " 132: 'American egret, great white heron, Egretta albus',\n",
    " 133: 'bittern',\n",
    " 134: 'crane',\n",
    " 135: 'limpkin, Aramus pictus',\n",
    " 136: 'European gallinule, Porphyrio porphyrio',\n",
    " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
    " 138: 'bustard',\n",
    " 139: 'ruddy turnstone, Arenaria interpres',\n",
    " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
    " 141: 'redshank, Tringa totanus',\n",
    " 142: 'dowitcher',\n",
    " 143: 'oystercatcher, oyster catcher',\n",
    " 144: 'pelican',\n",
    " 145: 'king penguin, Aptenodytes patagonica',\n",
    " 146: 'albatross, mollymawk',\n",
    " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
    " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
    " 149: 'dugong, Dugong dugon',\n",
    " 150: 'sea lion',\n",
    " 151: 'Chihuahua',\n",
    " 152: 'Japanese spaniel',\n",
    " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
    " 154: 'Pekinese, Pekingese, Peke',\n",
    " 155: 'Shih-Tzu',\n",
    " 156: 'Blenheim spaniel',\n",
    " 157: 'papillon',\n",
    " 158: 'toy terrier',\n",
    " 159: 'Rhodesian ridgeback',\n",
    " 160: 'Afghan hound, Afghan',\n",
    " 161: 'basset, basset hound',\n",
    " 162: 'beagle',\n",
    " 163: 'bloodhound, sleuthhound',\n",
    " 164: 'bluetick',\n",
    " 165: 'black-and-tan coonhound',\n",
    " 166: 'Walker hound, Walker foxhound',\n",
    " 167: 'English foxhound',\n",
    " 168: 'redbone',\n",
    " 169: 'borzoi, Russian wolfhound',\n",
    " 170: 'Irish wolfhound',\n",
    " 171: 'Italian greyhound',\n",
    " 172: 'whippet',\n",
    " 173: 'Ibizan hound, Ibizan Podenco',\n",
    " 174: 'Norwegian elkhound, elkhound',\n",
    " 175: 'otterhound, otter hound',\n",
    " 176: 'Saluki, gazelle hound',\n",
    " 177: 'Scottish deerhound, deerhound',\n",
    " 178: 'Weimaraner',\n",
    " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
    " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
    " 181: 'Bedlington terrier',\n",
    " 182: 'Border terrier',\n",
    " 183: 'Kerry blue terrier',\n",
    " 184: 'Irish terrier',\n",
    " 185: 'Norfolk terrier',\n",
    " 186: 'Norwich terrier',\n",
    " 187: 'Yorkshire terrier',\n",
    " 188: 'wire-haired fox terrier',\n",
    " 189: 'Lakeland terrier',\n",
    " 190: 'Sealyham terrier, Sealyham',\n",
    " 191: 'Airedale, Airedale terrier',\n",
    " 192: 'cairn, cairn terrier',\n",
    " 193: 'Australian terrier',\n",
    " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
    " 195: 'Boston bull, Boston terrier',\n",
    " 196: 'miniature schnauzer',\n",
    " 197: 'giant schnauzer',\n",
    " 198: 'standard schnauzer',\n",
    " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
    " 200: 'Tibetan terrier, chrysanthemum dog',\n",
    " 201: 'silky terrier, Sydney silky',\n",
    " 202: 'soft-coated wheaten terrier',\n",
    " 203: 'West Highland white terrier',\n",
    " 204: 'Lhasa, Lhasa apso',\n",
    " 205: 'flat-coated retriever',\n",
    " 206: 'curly-coated retriever',\n",
    " 207: 'golden retriever',\n",
    " 208: 'Labrador retriever',\n",
    " 209: 'Chesapeake Bay retriever',\n",
    " 210: 'German short-haired pointer',\n",
    " 211: 'vizsla, Hungarian pointer',\n",
    " 212: 'English setter',\n",
    " 213: 'Irish setter, red setter',\n",
    " 214: 'Gordon setter',\n",
    " 215: 'Brittany spaniel',\n",
    " 216: 'clumber, clumber spaniel',\n",
    " 217: 'English springer, English springer spaniel',\n",
    " 218: 'Welsh springer spaniel',\n",
    " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
    " 220: 'Sussex spaniel',\n",
    " 221: 'Irish water spaniel',\n",
    " 222: 'kuvasz',\n",
    " 223: 'schipperke',\n",
    " 224: 'groenendael',\n",
    " 225: 'malinois',\n",
    " 226: 'briard',\n",
    " 227: 'kelpie',\n",
    " 228: 'komondor',\n",
    " 229: 'Old English sheepdog, bobtail',\n",
    " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
    " 231: 'collie',\n",
    " 232: 'Border collie',\n",
    " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
    " 234: 'Rottweiler',\n",
    " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
    " 236: 'Doberman, Doberman pinscher',\n",
    " 237: 'miniature pinscher',\n",
    " 238: 'Greater Swiss Mountain dog',\n",
    " 239: 'Bernese mountain dog',\n",
    " 240: 'Appenzeller',\n",
    " 241: 'EntleBucher',\n",
    " 242: 'boxer',\n",
    " 243: 'bull mastiff',\n",
    " 244: 'Tibetan mastiff',\n",
    " 245: 'French bulldog',\n",
    " 246: 'Great Dane',\n",
    " 247: 'Saint Bernard, St Bernard',\n",
    " 248: 'Eskimo dog, husky',\n",
    " 249: 'malamute, malemute, Alaskan malamute',\n",
    " 250: 'Siberian husky',\n",
    " 251: 'dalmatian, coach dog, carriage dog',\n",
    " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
    " 253: 'basenji',\n",
    " 254: 'pug, pug-dog',\n",
    " 255: 'Leonberg',\n",
    " 256: 'Newfoundland, Newfoundland dog',\n",
    " 257: 'Great Pyrenees',\n",
    " 258: 'Samoyed, Samoyede',\n",
    " 259: 'Pomeranian',\n",
    " 260: 'chow, chow chow',\n",
    " 261: 'keeshond',\n",
    " 262: 'Brabancon griffon',\n",
    " 263: 'Pembroke, Pembroke Welsh corgi',\n",
    " 264: 'Cardigan, Cardigan Welsh corgi',\n",
    " 265: 'toy poodle',\n",
    " 266: 'miniature poodle',\n",
    " 267: 'standard poodle',\n",
    " 268: 'Mexican hairless',\n",
    " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
    " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
    " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
    " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
    " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
    " 274: 'dhole, Cuon alpinus',\n",
    " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
    " 276: 'hyena, hyaena',\n",
    " 277: 'red fox, Vulpes vulpes',\n",
    " 278: 'kit fox, Vulpes macrotis',\n",
    " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
    " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
    " 281: 'tabby, tabby cat',\n",
    " 282: 'tiger cat',\n",
    " 283: 'Persian cat',\n",
    " 284: 'Siamese cat, Siamese',\n",
    " 285: 'Egyptian cat',\n",
    " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
    " 287: 'lynx, catamount',\n",
    " 288: 'leopard, Panthera pardus',\n",
    " 289: 'snow leopard, ounce, Panthera uncia',\n",
    " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
    " 291: 'lion, king of beasts, Panthera leo',\n",
    " 292: 'tiger, Panthera tigris',\n",
    " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
    " 294: 'brown bear, bruin, Ursus arctos',\n",
    " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
    " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
    " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
    " 298: 'mongoose',\n",
    " 299: 'meerkat, mierkat',\n",
    " 300: 'tiger beetle',\n",
    " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
    " 302: 'ground beetle, carabid beetle',\n",
    " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
    " 304: 'leaf beetle, chrysomelid',\n",
    " 305: 'dung beetle',\n",
    " 306: 'rhinoceros beetle',\n",
    " 307: 'weevil',\n",
    " 308: 'fly',\n",
    " 309: 'bee',\n",
    " 310: 'ant, emmet, pismire',\n",
    " 311: 'grasshopper, hopper',\n",
    " 312: 'cricket',\n",
    " 313: 'walking stick, walkingstick, stick insect',\n",
    " 314: 'cockroach, roach',\n",
    " 315: 'mantis, mantid',\n",
    " 316: 'cicada, cicala',\n",
    " 317: 'leafhopper',\n",
    " 318: 'lacewing, lacewing fly',\n",
    " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
    " 320: 'damselfly',\n",
    " 321: 'admiral',\n",
    " 322: 'ringlet, ringlet butterfly',\n",
    " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
    " 324: 'cabbage butterfly',\n",
    " 325: 'sulphur butterfly, sulfur butterfly',\n",
    " 326: 'lycaenid, lycaenid butterfly',\n",
    " 327: 'starfish, sea star',\n",
    " 328: 'sea urchin',\n",
    " 329: 'sea cucumber, holothurian',\n",
    " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
    " 331: 'hare',\n",
    " 332: 'Angora, Angora rabbit',\n",
    " 333: 'hamster',\n",
    " 334: 'porcupine, hedgehog',\n",
    " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
    " 336: 'marmot',\n",
    " 337: 'beaver',\n",
    " 338: 'guinea pig, Cavia cobaya',\n",
    " 339: 'sorrel',\n",
    " 340: 'zebra',\n",
    " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
    " 342: 'wild boar, boar, Sus scrofa',\n",
    " 343: 'warthog',\n",
    " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
    " 345: 'ox',\n",
    " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
    " 347: 'bison',\n",
    " 348: 'ram, tup',\n",
    " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
    " 350: 'ibex, Capra ibex',\n",
    " 351: 'hartebeest',\n",
    " 352: 'impala, Aepyceros melampus',\n",
    " 353: 'gazelle',\n",
    " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
    " 355: 'llama',\n",
    " 356: 'weasel',\n",
    " 357: 'mink',\n",
    " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
    " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
    " 360: 'otter',\n",
    " 361: 'skunk, polecat, wood pussy',\n",
    " 362: 'badger',\n",
    " 363: 'armadillo',\n",
    " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
    " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
    " 366: 'gorilla, Gorilla gorilla',\n",
    " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
    " 368: 'gibbon, Hylobates lar',\n",
    " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
    " 370: 'guenon, guenon monkey',\n",
    " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
    " 372: 'baboon',\n",
    " 373: 'macaque',\n",
    " 374: 'langur',\n",
    " 375: 'colobus, colobus monkey',\n",
    " 376: 'proboscis monkey, Nasalis larvatus',\n",
    " 377: 'marmoset',\n",
    " 378: 'capuchin, ringtail, Cebus capucinus',\n",
    " 379: 'howler monkey, howler',\n",
    " 380: 'titi, titi monkey',\n",
    " 381: 'spider monkey, Ateles geoffroyi',\n",
    " 382: 'squirrel monkey, Saimiri sciureus',\n",
    " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
    " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
    " 385: 'Indian elephant, Elephas maximus',\n",
    " 386: 'African elephant, Loxodonta africana',\n",
    " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
    " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
    " 389: 'barracouta, snoek',\n",
    " 390: 'eel',\n",
    " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
    " 392: 'rock beauty, Holocanthus tricolor',\n",
    " 393: 'anemone fish',\n",
    " 394: 'sturgeon',\n",
    " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
    " 396: 'lionfish',\n",
    " 397: 'puffer, pufferfish, blowfish, globefish',\n",
    " 398: 'abacus',\n",
    " 399: 'abaya',\n",
    " 400: \"academic gown, academic robe, judge's robe\",\n",
    " 401: 'accordion, piano accordion, squeeze box',\n",
    " 402: 'acoustic guitar',\n",
    " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
    " 404: 'airliner',\n",
    " 405: 'airship, dirigible',\n",
    " 406: 'altar',\n",
    " 407: 'ambulance',\n",
    " 408: 'amphibian, amphibious vehicle',\n",
    " 409: 'analog clock',\n",
    " 410: 'apiary, bee house',\n",
    " 411: 'apron',\n",
    " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
    " 413: 'assault rifle, assault gun',\n",
    " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
    " 415: 'bakery, bakeshop, bakehouse',\n",
    " 416: 'balance beam, beam',\n",
    " 417: 'balloon',\n",
    " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
    " 419: 'Band Aid',\n",
    " 420: 'banjo',\n",
    " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
    " 422: 'barbell',\n",
    " 423: 'barber chair',\n",
    " 424: 'barbershop',\n",
    " 425: 'barn',\n",
    " 426: 'barometer',\n",
    " 427: 'barrel, cask',\n",
    " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
    " 429: 'baseball',\n",
    " 430: 'basketball',\n",
    " 431: 'bassinet',\n",
    " 432: 'bassoon',\n",
    " 433: 'bathing cap, swimming cap',\n",
    " 434: 'bath towel',\n",
    " 435: 'bathtub, bathing tub, bath, tub',\n",
    " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
    " 437: 'beacon, lighthouse, beacon light, pharos',\n",
    " 438: 'beaker',\n",
    " 439: 'bearskin, busby, shako',\n",
    " 440: 'beer bottle',\n",
    " 441: 'beer glass',\n",
    " 442: 'bell cote, bell cot',\n",
    " 443: 'bib',\n",
    " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
    " 445: 'bikini, two-piece',\n",
    " 446: 'binder, ring-binder',\n",
    " 447: 'binoculars, field glasses, opera glasses',\n",
    " 448: 'birdhouse',\n",
    " 449: 'boathouse',\n",
    " 450: 'bobsled, bobsleigh, bob',\n",
    " 451: 'bolo tie, bolo, bola tie, bola',\n",
    " 452: 'bonnet, poke bonnet',\n",
    " 453: 'bookcase',\n",
    " 454: 'bookshop, bookstore, bookstall',\n",
    " 455: 'bottlecap',\n",
    " 456: 'bow',\n",
    " 457: 'bow tie, bow-tie, bowtie',\n",
    " 458: 'brass, memorial tablet, plaque',\n",
    " 459: 'brassiere, bra, bandeau',\n",
    " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
    " 461: 'breastplate, aegis, egis',\n",
    " 462: 'broom',\n",
    " 463: 'bucket, pail',\n",
    " 464: 'buckle',\n",
    " 465: 'bulletproof vest',\n",
    " 466: 'bullet train, bullet',\n",
    " 467: 'butcher shop, meat market',\n",
    " 468: 'cab, hack, taxi, taxicab',\n",
    " 469: 'caldron, cauldron',\n",
    " 470: 'candle, taper, wax light',\n",
    " 471: 'cannon',\n",
    " 472: 'canoe',\n",
    " 473: 'can opener, tin opener',\n",
    " 474: 'cardigan',\n",
    " 475: 'car mirror',\n",
    " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
    " 477: \"carpenter's kit, tool kit\",\n",
    " 478: 'carton',\n",
    " 479: 'car wheel',\n",
    " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
    " 481: 'cassette',\n",
    " 482: 'cassette player',\n",
    " 483: 'castle',\n",
    " 484: 'catamaran',\n",
    " 485: 'CD player',\n",
    " 486: 'cello, violoncello',\n",
    " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
    " 488: 'chain',\n",
    " 489: 'chainlink fence',\n",
    " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
    " 491: 'chain saw, chainsaw',\n",
    " 492: 'chest',\n",
    " 493: 'chiffonier, commode',\n",
    " 494: 'chime, bell, gong',\n",
    " 495: 'china cabinet, china closet',\n",
    " 496: 'Christmas stocking',\n",
    " 497: 'church, church building',\n",
    " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
    " 499: 'cleaver, meat cleaver, chopper',\n",
    " 500: 'cliff dwelling',\n",
    " 501: 'cloak',\n",
    " 502: 'clog, geta, patten, sabot',\n",
    " 503: 'cocktail shaker',\n",
    " 504: 'coffee mug',\n",
    " 505: 'coffeepot',\n",
    " 506: 'coil, spiral, volute, whorl, helix',\n",
    " 507: 'combination lock',\n",
    " 508: 'computer keyboard, keypad',\n",
    " 509: 'confectionery, confectionary, candy store',\n",
    " 510: 'container ship, containership, container vessel',\n",
    " 511: 'convertible',\n",
    " 512: 'corkscrew, bottle screw',\n",
    " 513: 'cornet, horn, trumpet, trump',\n",
    " 514: 'cowboy boot',\n",
    " 515: 'cowboy hat, ten-gallon hat',\n",
    " 516: 'cradle',\n",
    " 517: 'crane',\n",
    " 518: 'crash helmet',\n",
    " 519: 'crate',\n",
    " 520: 'crib, cot',\n",
    " 521: 'Crock Pot',\n",
    " 522: 'croquet ball',\n",
    " 523: 'crutch',\n",
    " 524: 'cuirass',\n",
    " 525: 'dam, dike, dyke',\n",
    " 526: 'desk',\n",
    " 527: 'desktop computer',\n",
    " 528: 'dial telephone, dial phone',\n",
    " 529: 'diaper, nappy, napkin',\n",
    " 530: 'digital clock',\n",
    " 531: 'digital watch',\n",
    " 532: 'dining table, board',\n",
    " 533: 'dishrag, dishcloth',\n",
    " 534: 'dishwasher, dish washer, dishwashing machine',\n",
    " 535: 'disk brake, disc brake',\n",
    " 536: 'dock, dockage, docking facility',\n",
    " 537: 'dogsled, dog sled, dog sleigh',\n",
    " 538: 'dome',\n",
    " 539: 'doormat, welcome mat',\n",
    " 540: 'drilling platform, offshore rig',\n",
    " 541: 'drum, membranophone, tympan',\n",
    " 542: 'drumstick',\n",
    " 543: 'dumbbell',\n",
    " 544: 'Dutch oven',\n",
    " 545: 'electric fan, blower',\n",
    " 546: 'electric guitar',\n",
    " 547: 'electric locomotive',\n",
    " 548: 'entertainment center',\n",
    " 549: 'envelope',\n",
    " 550: 'espresso maker',\n",
    " 551: 'face powder',\n",
    " 552: 'feather boa, boa',\n",
    " 553: 'file, file cabinet, filing cabinet',\n",
    " 554: 'fireboat',\n",
    " 555: 'fire engine, fire truck',\n",
    " 556: 'fire screen, fireguard',\n",
    " 557: 'flagpole, flagstaff',\n",
    " 558: 'flute, transverse flute',\n",
    " 559: 'folding chair',\n",
    " 560: 'football helmet',\n",
    " 561: 'forklift',\n",
    " 562: 'fountain',\n",
    " 563: 'fountain pen',\n",
    " 564: 'four-poster',\n",
    " 565: 'freight car',\n",
    " 566: 'French horn, horn',\n",
    " 567: 'frying pan, frypan, skillet',\n",
    " 568: 'fur coat',\n",
    " 569: 'garbage truck, dustcart',\n",
    " 570: 'gasmask, respirator, gas helmet',\n",
    " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
    " 572: 'goblet',\n",
    " 573: 'go-kart',\n",
    " 574: 'golf ball',\n",
    " 575: 'golfcart, golf cart',\n",
    " 576: 'gondola',\n",
    " 577: 'gong, tam-tam',\n",
    " 578: 'gown',\n",
    " 579: 'grand piano, grand',\n",
    " 580: 'greenhouse, nursery, glasshouse',\n",
    " 581: 'grille, radiator grille',\n",
    " 582: 'grocery store, grocery, food market, market',\n",
    " 583: 'guillotine',\n",
    " 584: 'hair slide',\n",
    " 585: 'hair spray',\n",
    " 586: 'half track',\n",
    " 587: 'hammer',\n",
    " 588: 'hamper',\n",
    " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
    " 590: 'hand-held computer, hand-held microcomputer',\n",
    " 591: 'handkerchief, hankie, hanky, hankey',\n",
    " 592: 'hard disc, hard disk, fixed disk',\n",
    " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
    " 594: 'harp',\n",
    " 595: 'harvester, reaper',\n",
    " 596: 'hatchet',\n",
    " 597: 'holster',\n",
    " 598: 'home theater, home theatre',\n",
    " 599: 'honeycomb',\n",
    " 600: 'hook, claw',\n",
    " 601: 'hoopskirt, crinoline',\n",
    " 602: 'horizontal bar, high bar',\n",
    " 603: 'horse cart, horse-cart',\n",
    " 604: 'hourglass',\n",
    " 605: 'iPod',\n",
    " 606: 'iron, smoothing iron',\n",
    " 607: \"jack-o'-lantern\",\n",
    " 608: 'jean, blue jean, denim',\n",
    " 609: 'jeep, landrover',\n",
    " 610: 'jersey, T-shirt, tee shirt',\n",
    " 611: 'jigsaw puzzle',\n",
    " 612: 'jinrikisha, ricksha, rickshaw',\n",
    " 613: 'joystick',\n",
    " 614: 'kimono',\n",
    " 615: 'knee pad',\n",
    " 616: 'knot',\n",
    " 617: 'lab coat, laboratory coat',\n",
    " 618: 'ladle',\n",
    " 619: 'lampshade, lamp shade',\n",
    " 620: 'laptop, laptop computer',\n",
    " 621: 'lawn mower, mower',\n",
    " 622: 'lens cap, lens cover',\n",
    " 623: 'letter opener, paper knife, paperknife',\n",
    " 624: 'library',\n",
    " 625: 'lifeboat',\n",
    " 626: 'lighter, light, igniter, ignitor',\n",
    " 627: 'limousine, limo',\n",
    " 628: 'liner, ocean liner',\n",
    " 629: 'lipstick, lip rouge',\n",
    " 630: 'Loafer',\n",
    " 631: 'lotion',\n",
    " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
    " 633: \"loupe, jeweler's loupe\",\n",
    " 634: 'lumbermill, sawmill',\n",
    " 635: 'magnetic compass',\n",
    " 636: 'mailbag, postbag',\n",
    " 637: 'mailbox, letter box',\n",
    " 638: 'maillot',\n",
    " 639: 'maillot, tank suit',\n",
    " 640: 'manhole cover',\n",
    " 641: 'maraca',\n",
    " 642: 'marimba, xylophone',\n",
    " 643: 'mask',\n",
    " 644: 'matchstick',\n",
    " 645: 'maypole',\n",
    " 646: 'maze, labyrinth',\n",
    " 647: 'measuring cup',\n",
    " 648: 'medicine chest, medicine cabinet',\n",
    " 649: 'megalith, megalithic structure',\n",
    " 650: 'microphone, mike',\n",
    " 651: 'microwave, microwave oven',\n",
    " 652: 'military uniform',\n",
    " 653: 'milk can',\n",
    " 654: 'minibus',\n",
    " 655: 'miniskirt, mini',\n",
    " 656: 'minivan',\n",
    " 657: 'missile',\n",
    " 658: 'mitten',\n",
    " 659: 'mixing bowl',\n",
    " 660: 'mobile home, manufactured home',\n",
    " 661: 'Model T',\n",
    " 662: 'modem',\n",
    " 663: 'monastery',\n",
    " 664: 'monitor',\n",
    " 665: 'moped',\n",
    " 666: 'mortar',\n",
    " 667: 'mortarboard',\n",
    " 668: 'mosque',\n",
    " 669: 'mosquito net',\n",
    " 670: 'motor scooter, scooter',\n",
    " 671: 'mountain bike, all-terrain bike, off-roader',\n",
    " 672: 'mountain tent',\n",
    " 673: 'mouse, computer mouse',\n",
    " 674: 'mousetrap',\n",
    " 675: 'moving van',\n",
    " 676: 'muzzle',\n",
    " 677: 'nail',\n",
    " 678: 'neck brace',\n",
    " 679: 'necklace',\n",
    " 680: 'nipple',\n",
    " 681: 'notebook, notebook computer',\n",
    " 682: 'obelisk',\n",
    " 683: 'oboe, hautboy, hautbois',\n",
    " 684: 'ocarina, sweet potato',\n",
    " 685: 'odometer, hodometer, mileometer, milometer',\n",
    " 686: 'oil filter',\n",
    " 687: 'organ, pipe organ',\n",
    " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
    " 689: 'overskirt',\n",
    " 690: 'oxcart',\n",
    " 691: 'oxygen mask',\n",
    " 692: 'packet',\n",
    " 693: 'paddle, boat paddle',\n",
    " 694: 'paddlewheel, paddle wheel',\n",
    " 695: 'padlock',\n",
    " 696: 'paintbrush',\n",
    " 697: \"pajama, pyjama, pj's, jammies\",\n",
    " 698: 'palace',\n",
    " 699: 'panpipe, pandean pipe, syrinx',\n",
    " 700: 'paper towel',\n",
    " 701: 'parachute, chute',\n",
    " 702: 'parallel bars, bars',\n",
    " 703: 'park bench',\n",
    " 704: 'parking meter',\n",
    " 705: 'passenger car, coach, carriage',\n",
    " 706: 'patio, terrace',\n",
    " 707: 'pay-phone, pay-station',\n",
    " 708: 'pedestal, plinth, footstall',\n",
    " 709: 'pencil box, pencil case',\n",
    " 710: 'pencil sharpener',\n",
    " 711: 'perfume, essence',\n",
    " 712: 'Petri dish',\n",
    " 713: 'photocopier',\n",
    " 714: 'pick, plectrum, plectron',\n",
    " 715: 'pickelhaube',\n",
    " 716: 'picket fence, paling',\n",
    " 717: 'pickup, pickup truck',\n",
    " 718: 'pier',\n",
    " 719: 'piggy bank, penny bank',\n",
    " 720: 'pill bottle',\n",
    " 721: 'pillow',\n",
    " 722: 'ping-pong ball',\n",
    " 723: 'pinwheel',\n",
    " 724: 'pirate, pirate ship',\n",
    " 725: 'pitcher, ewer',\n",
    " 726: \"plane, carpenter's plane, woodworking plane\",\n",
    " 727: 'planetarium',\n",
    " 728: 'plastic bag',\n",
    " 729: 'plate rack',\n",
    " 730: 'plow, plough',\n",
    " 731: \"plunger, plumber's helper\",\n",
    " 732: 'Polaroid camera, Polaroid Land camera',\n",
    " 733: 'pole',\n",
    " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
    " 735: 'poncho',\n",
    " 736: 'pool table, billiard table, snooker table',\n",
    " 737: 'pop bottle, soda bottle',\n",
    " 738: 'pot, flowerpot',\n",
    " 739: \"potter's wheel\",\n",
    " 740: 'power drill',\n",
    " 741: 'prayer rug, prayer mat',\n",
    " 742: 'printer',\n",
    " 743: 'prison, prison house',\n",
    " 744: 'projectile, missile',\n",
    " 745: 'projector',\n",
    " 746: 'puck, hockey puck',\n",
    " 747: 'punching bag, punch bag, punching ball, punchball',\n",
    " 748: 'purse',\n",
    " 749: 'quill, quill pen',\n",
    " 750: 'quilt, comforter, comfort, puff',\n",
    " 751: 'racer, race car, racing car',\n",
    " 752: 'racket, racquet',\n",
    " 753: 'radiator',\n",
    " 754: 'radio, wireless',\n",
    " 755: 'radio telescope, radio reflector',\n",
    " 756: 'rain barrel',\n",
    " 757: 'recreational vehicle, RV, R.V.',\n",
    " 758: 'reel',\n",
    " 759: 'reflex camera',\n",
    " 760: 'refrigerator, icebox',\n",
    " 761: 'remote control, remote',\n",
    " 762: 'restaurant, eating house, eating place, eatery',\n",
    " 763: 'revolver, six-gun, six-shooter',\n",
    " 764: 'rifle',\n",
    " 765: 'rocking chair, rocker',\n",
    " 766: 'rotisserie',\n",
    " 767: 'rubber eraser, rubber, pencil eraser',\n",
    " 768: 'rugby ball',\n",
    " 769: 'rule, ruler',\n",
    " 770: 'running shoe',\n",
    " 771: 'safe',\n",
    " 772: 'safety pin',\n",
    " 773: 'saltshaker, salt shaker',\n",
    " 774: 'sandal',\n",
    " 775: 'sarong',\n",
    " 776: 'sax, saxophone',\n",
    " 777: 'scabbard',\n",
    " 778: 'scale, weighing machine',\n",
    " 779: 'school bus',\n",
    " 780: 'schooner',\n",
    " 781: 'scoreboard',\n",
    " 782: 'screen, CRT screen',\n",
    " 783: 'screw',\n",
    " 784: 'screwdriver',\n",
    " 785: 'seat belt, seatbelt',\n",
    " 786: 'sewing machine',\n",
    " 787: 'shield, buckler',\n",
    " 788: 'shoe shop, shoe-shop, shoe store',\n",
    " 789: 'shoji',\n",
    " 790: 'shopping basket',\n",
    " 791: 'shopping cart',\n",
    " 792: 'shovel',\n",
    " 793: 'shower cap',\n",
    " 794: 'shower curtain',\n",
    " 795: 'ski',\n",
    " 796: 'ski mask',\n",
    " 797: 'sleeping bag',\n",
    " 798: 'slide rule, slipstick',\n",
    " 799: 'sliding door',\n",
    " 800: 'slot, one-armed bandit',\n",
    " 801: 'snorkel',\n",
    " 802: 'snowmobile',\n",
    " 803: 'snowplow, snowplough',\n",
    " 804: 'soap dispenser',\n",
    " 805: 'soccer ball',\n",
    " 806: 'sock',\n",
    " 807: 'solar dish, solar collector, solar furnace',\n",
    " 808: 'sombrero',\n",
    " 809: 'soup bowl',\n",
    " 810: 'space bar',\n",
    " 811: 'space heater',\n",
    " 812: 'space shuttle',\n",
    " 813: 'spatula',\n",
    " 814: 'speedboat',\n",
    " 815: \"spider web, spider's web\",\n",
    " 816: 'spindle',\n",
    " 817: 'sports car, sport car',\n",
    " 818: 'spotlight, spot',\n",
    " 819: 'stage',\n",
    " 820: 'steam locomotive',\n",
    " 821: 'steel arch bridge',\n",
    " 822: 'steel drum',\n",
    " 823: 'stethoscope',\n",
    " 824: 'stole',\n",
    " 825: 'stone wall',\n",
    " 826: 'stopwatch, stop watch',\n",
    " 827: 'stove',\n",
    " 828: 'strainer',\n",
    " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
    " 830: 'stretcher',\n",
    " 831: 'studio couch, day bed',\n",
    " 832: 'stupa, tope',\n",
    " 833: 'submarine, pigboat, sub, U-boat',\n",
    " 834: 'suit, suit of clothes',\n",
    " 835: 'sundial',\n",
    " 836: 'sunglass',\n",
    " 837: 'sunglasses, dark glasses, shades',\n",
    " 838: 'sunscreen, sunblock, sun blocker',\n",
    " 839: 'suspension bridge',\n",
    " 840: 'swab, swob, mop',\n",
    " 841: 'sweatshirt',\n",
    " 842: 'swimming trunks, bathing trunks',\n",
    " 843: 'swing',\n",
    " 844: 'switch, electric switch, electrical switch',\n",
    " 845: 'syringe',\n",
    " 846: 'table lamp',\n",
    " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
    " 848: 'tape player',\n",
    " 849: 'teapot',\n",
    " 850: 'teddy, teddy bear',\n",
    " 851: 'television, television system',\n",
    " 852: 'tennis ball',\n",
    " 853: 'thatch, thatched roof',\n",
    " 854: 'theater curtain, theatre curtain',\n",
    " 855: 'thimble',\n",
    " 856: 'thresher, thrasher, threshing machine',\n",
    " 857: 'throne',\n",
    " 858: 'tile roof',\n",
    " 859: 'toaster',\n",
    " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
    " 861: 'toilet seat',\n",
    " 862: 'torch',\n",
    " 863: 'totem pole',\n",
    " 864: 'tow truck, tow car, wrecker',\n",
    " 865: 'toyshop',\n",
    " 866: 'tractor',\n",
    " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
    " 868: 'tray',\n",
    " 869: 'trench coat',\n",
    " 870: 'tricycle, trike, velocipede',\n",
    " 871: 'trimaran',\n",
    " 872: 'tripod',\n",
    " 873: 'triumphal arch',\n",
    " 874: 'trolleybus, trolley coach, trackless trolley',\n",
    " 875: 'trombone',\n",
    " 876: 'tub, vat',\n",
    " 877: 'turnstile',\n",
    " 878: 'typewriter keyboard',\n",
    " 879: 'umbrella',\n",
    " 880: 'unicycle, monocycle',\n",
    " 881: 'upright, upright piano',\n",
    " 882: 'vacuum, vacuum cleaner',\n",
    " 883: 'vase',\n",
    " 884: 'vault',\n",
    " 885: 'velvet',\n",
    " 886: 'vending machine',\n",
    " 887: 'vestment',\n",
    " 888: 'viaduct',\n",
    " 889: 'violin, fiddle',\n",
    " 890: 'volleyball',\n",
    " 891: 'waffle iron',\n",
    " 892: 'wall clock',\n",
    " 893: 'wallet, billfold, notecase, pocketbook',\n",
    " 894: 'wardrobe, closet, press',\n",
    " 895: 'warplane, military plane',\n",
    " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
    " 897: 'washer, automatic washer, washing machine',\n",
    " 898: 'water bottle',\n",
    " 899: 'water jug',\n",
    " 900: 'water tower',\n",
    " 901: 'whiskey jug',\n",
    " 902: 'whistle',\n",
    " 903: 'wig',\n",
    " 904: 'window screen',\n",
    " 905: 'window shade',\n",
    " 906: 'Windsor tie',\n",
    " 907: 'wine bottle',\n",
    " 908: 'wing',\n",
    " 909: 'wok',\n",
    " 910: 'wooden spoon',\n",
    " 911: 'wool, woolen, woollen',\n",
    " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
    " 913: 'wreck',\n",
    " 914: 'yawl',\n",
    " 915: 'yurt',\n",
    " 916: 'web site, website, internet site, site',\n",
    " 917: 'comic book',\n",
    " 918: 'crossword puzzle, crossword',\n",
    " 919: 'street sign',\n",
    " 920: 'traffic light, traffic signal, stoplight',\n",
    " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
    " 922: 'menu',\n",
    " 923: 'plate',\n",
    " 924: 'guacamole',\n",
    " 925: 'consomme',\n",
    " 926: 'hot pot, hotpot',\n",
    " 927: 'trifle',\n",
    " 928: 'ice cream, icecream',\n",
    " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
    " 930: 'French loaf',\n",
    " 931: 'bagel, beigel',\n",
    " 932: 'pretzel',\n",
    " 933: 'cheeseburger',\n",
    " 934: 'hotdog, hot dog, red hot',\n",
    " 935: 'mashed potato',\n",
    " 936: 'head cabbage',\n",
    " 937: 'broccoli',\n",
    " 938: 'cauliflower',\n",
    " 939: 'zucchini, courgette',\n",
    " 940: 'spaghetti squash',\n",
    " 941: 'acorn squash',\n",
    " 942: 'butternut squash',\n",
    " 943: 'cucumber, cuke',\n",
    " 944: 'artichoke, globe artichoke',\n",
    " 945: 'bell pepper',\n",
    " 946: 'cardoon',\n",
    " 947: 'mushroom',\n",
    " 948: 'Granny Smith',\n",
    " 949: 'strawberry',\n",
    " 950: 'orange',\n",
    " 951: 'lemon',\n",
    " 952: 'fig',\n",
    " 953: 'pineapple, ananas',\n",
    " 954: 'banana',\n",
    " 955: 'jackfruit, jak, jack',\n",
    " 956: 'custard apple',\n",
    " 957: 'pomegranate',\n",
    " 958: 'hay',\n",
    " 959: 'carbonara',\n",
    " 960: 'chocolate sauce, chocolate syrup',\n",
    " 961: 'dough',\n",
    " 962: 'meat loaf, meatloaf',\n",
    " 963: 'pizza, pizza pie',\n",
    " 964: 'potpie',\n",
    " 965: 'burrito',\n",
    " 966: 'red wine',\n",
    " 967: 'espresso',\n",
    " 968: 'cup',\n",
    " 969: 'eggnog',\n",
    " 970: 'alp',\n",
    " 971: 'bubble',\n",
    " 972: 'cliff, drop, drop-off',\n",
    " 973: 'coral reef',\n",
    " 974: 'geyser',\n",
    " 975: 'lakeside, lakeshore',\n",
    " 976: 'promontory, headland, head, foreland',\n",
    " 977: 'sandbar, sand bar',\n",
    " 978: 'seashore, coast, seacoast, sea-coast',\n",
    " 979: 'valley, vale',\n",
    " 980: 'volcano',\n",
    " 981: 'ballplayer, baseball player',\n",
    " 982: 'groom, bridegroom',\n",
    " 983: 'scuba diver',\n",
    " 984: 'rapeseed',\n",
    " 985: 'daisy',\n",
    " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
    " 987: 'corn',\n",
    " 988: 'acorn',\n",
    " 989: 'hip, rose hip, rosehip',\n",
    " 990: 'buckeye, horse chestnut, conker',\n",
    " 991: 'coral fungus',\n",
    " 992: 'agaric',\n",
    " 993: 'gyromitra',\n",
    " 994: 'stinkhorn, carrion fungus',\n",
    " 995: 'earthstar',\n",
    " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
    " 997: 'bolete',\n",
    " 998: 'ear, spike, capitulum',\n",
    " 999: 'toilet tissue, toilet paper, bathroom tissue'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
