{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version:  1.3.1\n",
      "cuda available:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import numpy as np\n",
    "#import input_data\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from math import *\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import KFAC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from tqdm import tqdm, trange\n",
    "import pytest\n",
    "import matplotlib.pyplot as plt\n",
    "from DirLPA_utils import * \n",
    "\n",
    "print(\"pytorch version: \", torch.__version__)\n",
    "print(\"cuda available: \", torch.cuda.is_available())\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cifar10 on Resnet32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TRAIN_CIFAR10 = 128\n",
    "BATCH_SIZE_TEST_CIFAR10 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, targets):\n",
    "    \"\"\"Helper function to print the accuracy\"\"\"\n",
    "    predictions = output.argmax(dim=1, keepdim=True).view_as(targets)\n",
    "    return predictions.eq(targets).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='~/data/cifar10', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "CIFAR10_train_dataset, CIFAR10_val_dataset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "CIFAR10_train_loader = torch.utils.data.DataLoader(CIFAR10_train_dataset, batch_size=BATCH_SIZE_TRAIN_CIFAR10,\n",
    "                                          shuffle=False)\n",
    "\n",
    "CIFAR10_val_loader = torch.utils.data.DataLoader(CIFAR10_val_dataset, batch_size=BATCH_SIZE_TRAIN_CIFAR10,\n",
    "                                          shuffle=False)\n",
    "\n",
    "CIFAR10_test = torchvision.datasets.CIFAR10(root='~/data/cifar10', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "CIFAR10_test_loader = torch.utils.data.DataLoader(CIFAR10_test, batch_size=BATCH_SIZE_TEST_CIFAR10,\n",
    "                                         shuffle=False)\n",
    "\n",
    "CIFAR10_classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(CIFAR10_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "nrow = int(BATCH_SIZE_TRAIN_CIFAR10/4)\n",
    "imshow(torchvision.utils.make_grid(images, nrow=nrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#load in CIFAR100\n",
    "BATCH_SIZE_TRAIN_CIFAR100 = 128\n",
    "BATCH_SIZE_TEST_CIFAR100 = 128\n",
    "\n",
    "CIFAR100_train = torchvision.datasets.CIFAR100(root='~/data/cifar100', train=True,\n",
    "                                       download=True, transform=transform_test)\n",
    "CIFAR100_train_loader = torch.utils.data.DataLoader(CIFAR100_train, batch_size=BATCH_SIZE_TRAIN_CIFAR100,\n",
    "                                         shuffle=False)\n",
    "\n",
    "CIFAR100_test = torchvision.datasets.CIFAR100(root='~/data/cifar100', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "CIFAR100_test_loader = torch.utils.data.DataLoader(CIFAR100_test, batch_size=BATCH_SIZE_TEST_CIFAR100,\n",
    "                                         shuffle=False)\n",
    "\n",
    "CIFAR100_classes = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
    "    'worm'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/marius/data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: /home/marius/data/SVHN/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "# load SVHN\n",
    "BATCH_SIZE_TRAIN_SVHN = 128\n",
    "BATCH_SIZE_TEST_SVHN = 128\n",
    "\n",
    "train_data_SVHN = torchvision.datasets.SVHN('~/data/SVHN', split='train',\n",
    "                             download=True, transform=transform_train)\n",
    "\n",
    "test_data_SVHN = torchvision.datasets.SVHN('~/data/SVHN', split='test',\n",
    "                             download=True, transform=transform_test)\n",
    "\n",
    "train_loader_SVHN = torch.utils.data.DataLoader(test_data_SVHN, batch_size=BATCH_SIZE_TRAIN_SVHN)\n",
    "test_loader_SVHN = torch.utils.data.DataLoader(test_data_SVHN, batch_size=BATCH_SIZE_TEST_SVHN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CIFAR10 on ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='~/data/cifar10', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE_TRAIN_CIFAR10, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='~/data/cifar10', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE_TEST_CIFAR10, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(CIFAR10_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "nrow = int(BATCH_SIZE_TRAIN_CIFAR10/4)\n",
    "imshow(torchvision.utils.make_grid(images, nrow=nrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def phi(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.phi(x)\n",
    "        out = self.fc(out)\n",
    "        return(out)\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CIFAR10_model = ResNet18().to(device)\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(CIFAR10_model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(net, epoch, optimizer, trainloader, filename):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    \n",
    "    print(\"train loss: \", train_loss)\n",
    "    print(\"train accuracy: \", correct/total)\n",
    "    print(\"saving model at: {}\".format(filename))\n",
    "    torch.save(net.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, epoch, testloader):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        print(\"test loss: \", test_loss)\n",
    "        print(\"test accuracy: \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all():\n",
    "    CIFAR10_path = 'weights/cifar10_resnet18_SGD.pth'\n",
    "    lr = 0.1\n",
    "    epoch = 0\n",
    "    for e in [100, 50, 50]:\n",
    "        print(\"current learning rate: \", lr)\n",
    "        for _ in range(e):\n",
    "            optimizer = optim.SGD(CIFAR10_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "            train(CIFAR10_model, epoch, optimizer, trainloader, CIFAR10_path)\n",
    "            test(CIFAR10_model, epoch, testloader)\n",
    "            epoch += 1\n",
    "        lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.041772682219744\n",
      "test accuracy:  0.9502\n"
     ]
    }
   ],
   "source": [
    "##### if you already have a trained model ##############\n",
    "CIFAR10_PATH = 'weights/cifar10_resnet18_SGD.pth'\n",
    "CIFAR10_model = ResNet18().to(device)\n",
    "print(\"loading model from: {}\".format(CIFAR10_PATH))\n",
    "CIFAR10_model.load_state_dict(torch.load(CIFAR10_PATH))#, map_location=torch.device('cpu')))\n",
    "#test the model\n",
    "test(CIFAR10_model, 0, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diag_second_order(model, train_loader, var0 = 10, device='cpu'):\n",
    "\n",
    "    W = list(model.parameters())[-2]\n",
    "    b = list(model.parameters())[-1]\n",
    "    m, n = W.shape\n",
    "    print(\"n: {} inputs to linear layer with m: {} classes\".format(n, m))\n",
    "    lossfunc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    tau = 1/var0\n",
    "\n",
    "    extend(lossfunc, debug=False)\n",
    "    extend(model.fc, debug=False)\n",
    "\n",
    "    with backpack(DiagHessian()):\n",
    "\n",
    "        max_len = len(train_loader)\n",
    "        weights_cov = torch.zeros(max_len, m, n, device=device)\n",
    "        biases_cov = torch.zeros(max_len, m, device=device)\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "\n",
    "            if device == 'cuda':\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            lossfunc(model(x), y).backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Hessian of weight\n",
    "                W_ = W.diag_h\n",
    "                b_ = b.diag_h\n",
    "\n",
    "                #add_prior: since it will be flattened later we can just add the prior like that\n",
    "                W_ += tau * torch.ones(W_.size(), device=device)\n",
    "                b_ += tau * torch.ones(b_.size(), device=device)\n",
    "\n",
    "\n",
    "            weights_cov[batch_idx] = W_\n",
    "            biases_cov[batch_idx] = b_\n",
    "\n",
    "            print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "\n",
    "        print(len(weights_cov))\n",
    "        C_W = torch.mean(weights_cov, dim=0)\n",
    "        C_b = torch.mean(biases_cov, dim=0)\n",
    "\n",
    "    # Predictive distribution\n",
    "    with torch.no_grad():\n",
    "        M_W_post = W.t()\n",
    "        M_b_post = b\n",
    "\n",
    "        C_W_post = C_W\n",
    "        C_b_post = C_b\n",
    "        \n",
    "    print(\"M_W_post size: \", M_W_post.size())\n",
    "    print(\"M_b_post size: \", M_b_post.size())\n",
    "    print(\"C_W_post size: \", C_W_post.size())\n",
    "    print(\"C_b_post size: \", C_b_post.size())\n",
    "\n",
    "    return(M_W_post, M_b_post, C_W_post, C_b_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 512 inputs to linear layer with m: 10 classes\n",
      "Batch: 0/391\n",
      "Batch: 1/391\n",
      "Batch: 2/391\n",
      "Batch: 3/391\n",
      "Batch: 4/391\n",
      "Batch: 5/391\n",
      "Batch: 6/391\n",
      "Batch: 7/391\n",
      "Batch: 8/391\n",
      "Batch: 9/391\n",
      "Batch: 10/391\n",
      "Batch: 11/391\n",
      "Batch: 12/391\n",
      "Batch: 13/391\n",
      "Batch: 14/391\n",
      "Batch: 15/391\n",
      "Batch: 16/391\n",
      "Batch: 17/391\n",
      "Batch: 18/391\n",
      "Batch: 19/391\n",
      "Batch: 20/391\n",
      "Batch: 21/391\n",
      "Batch: 22/391\n",
      "Batch: 23/391\n",
      "Batch: 24/391\n",
      "Batch: 25/391\n",
      "Batch: 26/391\n",
      "Batch: 27/391\n",
      "Batch: 28/391\n",
      "Batch: 29/391\n",
      "Batch: 30/391\n",
      "Batch: 31/391\n",
      "Batch: 32/391\n",
      "Batch: 33/391\n",
      "Batch: 34/391\n",
      "Batch: 35/391\n",
      "Batch: 36/391\n",
      "Batch: 37/391\n",
      "Batch: 38/391\n",
      "Batch: 39/391\n",
      "Batch: 40/391\n",
      "Batch: 41/391\n",
      "Batch: 42/391\n",
      "Batch: 43/391\n",
      "Batch: 44/391\n",
      "Batch: 45/391\n",
      "Batch: 46/391\n",
      "Batch: 47/391\n",
      "Batch: 48/391\n",
      "Batch: 49/391\n",
      "Batch: 50/391\n",
      "Batch: 51/391\n",
      "Batch: 52/391\n",
      "Batch: 53/391\n",
      "Batch: 54/391\n",
      "Batch: 55/391\n",
      "Batch: 56/391\n",
      "Batch: 57/391\n",
      "Batch: 58/391\n",
      "Batch: 59/391\n",
      "Batch: 60/391\n",
      "Batch: 61/391\n",
      "Batch: 62/391\n",
      "Batch: 63/391\n",
      "Batch: 64/391\n",
      "Batch: 65/391\n",
      "Batch: 66/391\n",
      "Batch: 67/391\n",
      "Batch: 68/391\n",
      "Batch: 69/391\n",
      "Batch: 70/391\n",
      "Batch: 71/391\n",
      "Batch: 72/391\n",
      "Batch: 73/391\n",
      "Batch: 74/391\n",
      "Batch: 75/391\n",
      "Batch: 76/391\n",
      "Batch: 77/391\n",
      "Batch: 78/391\n",
      "Batch: 79/391\n",
      "Batch: 80/391\n",
      "Batch: 81/391\n",
      "Batch: 82/391\n",
      "Batch: 83/391\n",
      "Batch: 84/391\n",
      "Batch: 85/391\n",
      "Batch: 86/391\n",
      "Batch: 87/391\n",
      "Batch: 88/391\n",
      "Batch: 89/391\n",
      "Batch: 90/391\n",
      "Batch: 91/391\n",
      "Batch: 92/391\n",
      "Batch: 93/391\n",
      "Batch: 94/391\n",
      "Batch: 95/391\n",
      "Batch: 96/391\n",
      "Batch: 97/391\n",
      "Batch: 98/391\n",
      "Batch: 99/391\n",
      "Batch: 100/391\n",
      "Batch: 101/391\n",
      "Batch: 102/391\n",
      "Batch: 103/391\n",
      "Batch: 104/391\n",
      "Batch: 105/391\n",
      "Batch: 106/391\n",
      "Batch: 107/391\n",
      "Batch: 108/391\n",
      "Batch: 109/391\n",
      "Batch: 110/391\n",
      "Batch: 111/391\n",
      "Batch: 112/391\n",
      "Batch: 113/391\n",
      "Batch: 114/391\n",
      "Batch: 115/391\n",
      "Batch: 116/391\n",
      "Batch: 117/391\n",
      "Batch: 118/391\n",
      "Batch: 119/391\n",
      "Batch: 120/391\n",
      "Batch: 121/391\n",
      "Batch: 122/391\n",
      "Batch: 123/391\n",
      "Batch: 124/391\n",
      "Batch: 125/391\n",
      "Batch: 126/391\n",
      "Batch: 127/391\n",
      "Batch: 128/391\n",
      "Batch: 129/391\n",
      "Batch: 130/391\n",
      "Batch: 131/391\n",
      "Batch: 132/391\n",
      "Batch: 133/391\n",
      "Batch: 134/391\n",
      "Batch: 135/391\n",
      "Batch: 136/391\n",
      "Batch: 137/391\n",
      "Batch: 138/391\n",
      "Batch: 139/391\n",
      "Batch: 140/391\n",
      "Batch: 141/391\n",
      "Batch: 142/391\n",
      "Batch: 143/391\n",
      "Batch: 144/391\n",
      "Batch: 145/391\n",
      "Batch: 146/391\n",
      "Batch: 147/391\n",
      "Batch: 148/391\n",
      "Batch: 149/391\n",
      "Batch: 150/391\n",
      "Batch: 151/391\n",
      "Batch: 152/391\n",
      "Batch: 153/391\n",
      "Batch: 154/391\n",
      "Batch: 155/391\n",
      "Batch: 156/391\n",
      "Batch: 157/391\n",
      "Batch: 158/391\n",
      "Batch: 159/391\n",
      "Batch: 160/391\n",
      "Batch: 161/391\n",
      "Batch: 162/391\n",
      "Batch: 163/391\n",
      "Batch: 164/391\n",
      "Batch: 165/391\n",
      "Batch: 166/391\n",
      "Batch: 167/391\n",
      "Batch: 168/391\n",
      "Batch: 169/391\n",
      "Batch: 170/391\n",
      "Batch: 171/391\n",
      "Batch: 172/391\n",
      "Batch: 173/391\n",
      "Batch: 174/391\n",
      "Batch: 175/391\n",
      "Batch: 176/391\n",
      "Batch: 177/391\n",
      "Batch: 178/391\n",
      "Batch: 179/391\n",
      "Batch: 180/391\n",
      "Batch: 181/391\n",
      "Batch: 182/391\n",
      "Batch: 183/391\n",
      "Batch: 184/391\n",
      "Batch: 185/391\n",
      "Batch: 186/391\n",
      "Batch: 187/391\n",
      "Batch: 188/391\n",
      "Batch: 189/391\n",
      "Batch: 190/391\n",
      "Batch: 191/391\n",
      "Batch: 192/391\n",
      "Batch: 193/391\n",
      "Batch: 194/391\n",
      "Batch: 195/391\n",
      "Batch: 196/391\n",
      "Batch: 197/391\n",
      "Batch: 198/391\n",
      "Batch: 199/391\n",
      "Batch: 200/391\n",
      "Batch: 201/391\n",
      "Batch: 202/391\n",
      "Batch: 203/391\n",
      "Batch: 204/391\n",
      "Batch: 205/391\n",
      "Batch: 206/391\n",
      "Batch: 207/391\n",
      "Batch: 208/391\n",
      "Batch: 209/391\n",
      "Batch: 210/391\n",
      "Batch: 211/391\n",
      "Batch: 212/391\n",
      "Batch: 213/391\n",
      "Batch: 214/391\n",
      "Batch: 215/391\n",
      "Batch: 216/391\n",
      "Batch: 217/391\n",
      "Batch: 218/391\n",
      "Batch: 219/391\n",
      "Batch: 220/391\n",
      "Batch: 221/391\n",
      "Batch: 222/391\n",
      "Batch: 223/391\n",
      "Batch: 224/391\n",
      "Batch: 225/391\n",
      "Batch: 226/391\n",
      "Batch: 227/391\n",
      "Batch: 228/391\n",
      "Batch: 229/391\n",
      "Batch: 230/391\n",
      "Batch: 231/391\n",
      "Batch: 232/391\n",
      "Batch: 233/391\n",
      "Batch: 234/391\n",
      "Batch: 235/391\n",
      "Batch: 236/391\n",
      "Batch: 237/391\n",
      "Batch: 238/391\n",
      "Batch: 239/391\n",
      "Batch: 240/391\n",
      "Batch: 241/391\n",
      "Batch: 242/391\n",
      "Batch: 243/391\n",
      "Batch: 244/391\n",
      "Batch: 245/391\n",
      "Batch: 246/391\n",
      "Batch: 247/391\n",
      "Batch: 248/391\n",
      "Batch: 249/391\n",
      "Batch: 250/391\n",
      "Batch: 251/391\n",
      "Batch: 252/391\n",
      "Batch: 253/391\n",
      "Batch: 254/391\n",
      "Batch: 255/391\n",
      "Batch: 256/391\n",
      "Batch: 257/391\n",
      "Batch: 258/391\n",
      "Batch: 259/391\n",
      "Batch: 260/391\n",
      "Batch: 261/391\n",
      "Batch: 262/391\n",
      "Batch: 263/391\n",
      "Batch: 264/391\n",
      "Batch: 265/391\n",
      "Batch: 266/391\n",
      "Batch: 267/391\n",
      "Batch: 268/391\n",
      "Batch: 269/391\n",
      "Batch: 270/391\n",
      "Batch: 271/391\n",
      "Batch: 272/391\n",
      "Batch: 273/391\n",
      "Batch: 274/391\n",
      "Batch: 275/391\n",
      "Batch: 276/391\n",
      "Batch: 277/391\n",
      "Batch: 278/391\n",
      "Batch: 279/391\n",
      "Batch: 280/391\n",
      "Batch: 281/391\n",
      "Batch: 282/391\n",
      "Batch: 283/391\n",
      "Batch: 284/391\n",
      "Batch: 285/391\n",
      "Batch: 286/391\n",
      "Batch: 287/391\n",
      "Batch: 288/391\n",
      "Batch: 289/391\n",
      "Batch: 290/391\n",
      "Batch: 291/391\n",
      "Batch: 292/391\n",
      "Batch: 293/391\n",
      "Batch: 294/391\n",
      "Batch: 295/391\n",
      "Batch: 296/391\n",
      "Batch: 297/391\n",
      "Batch: 298/391\n",
      "Batch: 299/391\n",
      "Batch: 300/391\n",
      "Batch: 301/391\n",
      "Batch: 302/391\n",
      "Batch: 303/391\n",
      "Batch: 304/391\n",
      "Batch: 305/391\n",
      "Batch: 306/391\n",
      "Batch: 307/391\n",
      "Batch: 308/391\n",
      "Batch: 309/391\n",
      "Batch: 310/391\n",
      "Batch: 311/391\n",
      "Batch: 312/391\n",
      "Batch: 313/391\n",
      "Batch: 314/391\n",
      "Batch: 315/391\n",
      "Batch: 316/391\n",
      "Batch: 317/391\n",
      "Batch: 318/391\n",
      "Batch: 319/391\n",
      "Batch: 320/391\n",
      "Batch: 321/391\n",
      "Batch: 322/391\n",
      "Batch: 323/391\n",
      "Batch: 324/391\n",
      "Batch: 325/391\n",
      "Batch: 326/391\n",
      "Batch: 327/391\n",
      "Batch: 328/391\n",
      "Batch: 329/391\n",
      "Batch: 330/391\n",
      "Batch: 331/391\n",
      "Batch: 332/391\n",
      "Batch: 333/391\n",
      "Batch: 334/391\n",
      "Batch: 335/391\n",
      "Batch: 336/391\n",
      "Batch: 337/391\n",
      "Batch: 338/391\n",
      "Batch: 339/391\n",
      "Batch: 340/391\n",
      "Batch: 341/391\n",
      "Batch: 342/391\n",
      "Batch: 343/391\n",
      "Batch: 344/391\n",
      "Batch: 345/391\n",
      "Batch: 346/391\n",
      "Batch: 347/391\n",
      "Batch: 348/391\n",
      "Batch: 349/391\n",
      "Batch: 350/391\n",
      "Batch: 351/391\n",
      "Batch: 352/391\n",
      "Batch: 353/391\n",
      "Batch: 354/391\n",
      "Batch: 355/391\n",
      "Batch: 356/391\n",
      "Batch: 357/391\n",
      "Batch: 358/391\n",
      "Batch: 359/391\n",
      "Batch: 360/391\n",
      "Batch: 361/391\n",
      "Batch: 362/391\n",
      "Batch: 363/391\n",
      "Batch: 364/391\n",
      "Batch: 365/391\n",
      "Batch: 366/391\n",
      "Batch: 367/391\n",
      "Batch: 368/391\n",
      "Batch: 369/391\n",
      "Batch: 370/391\n",
      "Batch: 371/391\n",
      "Batch: 372/391\n",
      "Batch: 373/391\n",
      "Batch: 374/391\n",
      "Batch: 375/391\n",
      "Batch: 376/391\n",
      "Batch: 377/391\n",
      "Batch: 378/391\n",
      "Batch: 379/391\n",
      "Batch: 380/391\n",
      "Batch: 381/391\n",
      "Batch: 382/391\n",
      "Batch: 383/391\n",
      "Batch: 384/391\n",
      "Batch: 385/391\n",
      "Batch: 386/391\n",
      "Batch: 387/391\n",
      "Batch: 388/391\n",
      "Batch: 389/391\n",
      "Batch: 390/391\n",
      "391\n",
      "M_W_post size:  torch.Size([512, 10])\n",
      "M_b_post size:  torch.Size([10])\n",
      "C_W_post size:  torch.Size([10, 512])\n",
      "C_b_post size:  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D = Diag_second_order(model=CIFAR10_model,\n",
    "                                                               train_loader=trainloader,\n",
    "                                                               var0 = 10,\n",
    "                                                               device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nM_W_post, M_b_post, U_post, V_post, B_post = KFLP_second_order(model=CIFAR10_model,\\n                                                               batch_size=128,\\n                                                               train_loader=trainloader,\\n                                                               var0 = 10,\\n                                                               device='cuda')\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "M_W_post, M_b_post, U_post, V_post, B_post = KFLP_second_order(model=CIFAR10_model,\n",
    "                                                               batch_size=128,\n",
    "                                                               train_loader=trainloader,\n",
    "                                                               var0 = 10,\n",
    "                                                               device='cuda')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_dist_values(py_in, targets):\n",
    "    acc_in = np.mean(np.argmax(py_in, 1) == targets)\n",
    "    #prob_correct = np.choose(targets, py_in.T).mean()\n",
    "    prob_correct = py_in[targets].mean()\n",
    "    average_entropy = -np.sum(py_in*np.log(py_in+1e-8), axis=1).mean()\n",
    "    MMC = py_in.max(1).mean()\n",
    "    return(acc_in, prob_correct, average_entropy, MMC)\n",
    "\n",
    "def get_out_dist_values(py_in, py_out, targets):\n",
    "    average_entropy = -np.sum(py_out*np.log(py_out+1e-8), axis=1).mean()\n",
    "    acc_out = np.mean(np.argmax(py_out, 1) == targets)\n",
    "    if max(targets) > len(py_in[0]):\n",
    "        targets = np.array(targets)\n",
    "        targets[targets >= len(py_in[0])] = 0\n",
    "    #prob_correct = np.choose(targets, py_out.T).mean()\n",
    "    prob_correct = py_out[targets].mean()\n",
    "    labels = np.zeros(len(py_in)+len(py_out), dtype='int32')\n",
    "    labels[:len(py_in)] = 1\n",
    "    examples = np.concatenate([py_in.max(1), py_out.max(1)])\n",
    "    auroc = roc_auc_score(labels, examples)\n",
    "    MMC = py_out.max(1).mean()\n",
    "    return(acc_out, prob_correct, average_entropy, MMC, auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gaussian_output_old(x, mu_w, mu_b, sigma_w, sigma_b):\n",
    "    #get the distributions per class\n",
    "    batch_size = x.size(0)\n",
    "    num_classes = len(mu_b)\n",
    "    #print(\"batch_size, num_classes: \", batch_size, num_classes)\n",
    "    mu_batch = torch.zeros(batch_size, num_classes)\n",
    "    sigma_batch = torch.zeros(batch_size, num_classes, num_classes)\n",
    "    for i in range(batch_size):\n",
    "        per_class_sigmas = torch.zeros(num_classes)\n",
    "        for j in range(num_classes):\n",
    "            #create a diagonal Hessian\n",
    "            hess = torch.diag(sigma_w[j])\n",
    "            #b = x[i] @ hess @ x[i].t()\n",
    "            #a = sigma_b[i]\n",
    "            per_class_sigmas[j] = x[i] @ hess @ x[i].t() + sigma_b[j]\n",
    "\n",
    "        #print(\"sizes: \", mu_w.size(), x[i].size(), mu_b.size())\n",
    "        per_class_mus = x[i] @ mu_w + mu_b\n",
    "        mu_batch[i] = per_class_mus\n",
    "        sigma_batch[i] = torch.diag(per_class_sigmas)\n",
    "\n",
    "    return(mu_batch, sigma_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gaussian_output(x, mu_w, mu_b, sigma_w, sigma_b):\n",
    "    #get the distributions per class\n",
    "    batch_size = x.size(0)\n",
    "    num_classes = mu_b.size(0)\n",
    "    \n",
    "    # get mu batch\n",
    "    mu_w_batch = mu_w.repeat(batch_size, 1, 1)\n",
    "    mu_b_batch = mu_b.repeat(batch_size, 1)\n",
    "    mu_batch = torch.bmm(x.view(batch_size, 1, -1), mu_w_batch).view(batch_size, -1) + mu_b_batch\n",
    "    \n",
    "    #get sigma batch\n",
    "    sigma_w_batch = sigma_w.repeat(batch_size, 1, 1)\n",
    "    sigma_b_batch = sigma_b.repeat(batch_size, 1)\n",
    "    sigmas_diag = torch.zeros(batch_size, num_classes, device='cuda')\n",
    "    for j in range(num_classes):\n",
    "        h1 = x * sigma_w_batch[:, j]\n",
    "        helper = torch.matmul(h1.view(batch_size, 1, -1), x.view(batch_size, -1, 1))\n",
    "        helper = helper.view(-1) + sigma_b_batch[:,j]\n",
    "        sigmas_diag[:,j] = helper\n",
    "        \n",
    "    sigma_batch = torch.stack([torch.diag(x) for x in sigmas_diag])\n",
    "\n",
    "    \n",
    "    return(mu_batch, sigma_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_diagonal_sampling(model, test_loader, M_W_post, M_b_post, C_W_post, C_b_post, n_samples, verbose=False, cuda=False, timing=False):\n",
    "    py = []\n",
    "    max_len = len(test_loader)\n",
    "    if timing:\n",
    "        time_sum = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "\n",
    "        if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        phi = model.phi(x)\n",
    "\n",
    "        mu, Sigma = get_Gaussian_output(phi, M_W_post, M_b_post, C_W_post, C_b_post)\n",
    "        #print(\"mu size: \", mu.size())\n",
    "        #print(\"sigma size: \", Sigma.size())\n",
    "\n",
    "        post_pred = MultivariateNormal(mu, Sigma)\n",
    "\n",
    "        # MC-integral\n",
    "        t0 = time.time()\n",
    "        py_ = 0\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            f_s = post_pred.rsample()\n",
    "            py_ += torch.softmax(f_s, 1)\n",
    "\n",
    "        py_ /= n_samples\n",
    "        py_ = py_.detach()\n",
    "\n",
    "        py.append(py_)\n",
    "        t1 = time.time()\n",
    "        if timing:\n",
    "            time_sum += (t1 - t0)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "\n",
    "    if timing: print(\"time used for sampling with {} samples: {}\".format(n_samples, time_sum))\n",
    "    \n",
    "    return torch.cat(py, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_CIFAR10 = testset.targets\n",
    "targets_CIFAR100 = CIFAR100_test.targets\n",
    "targets_SVHN = []\n",
    "for x,y in test_loader_SVHN:\n",
    "    targets_SVHN.append(y)\n",
    "targets_SVHN = torch.cat(targets_SVHN).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_test_in_MAP = predict_MAP(CIFAR10_model, testloader, cuda=True).cpu().numpy()\n",
    "CIFAR10_test_out_CIFAR100_MAP = predict_MAP(CIFAR10_model, CIFAR100_test_loader, cuda=True).cpu().numpy()\n",
    "CIFAR10_test_out_SVHN_MAP = predict_MAP(CIFAR10_model, test_loader_SVHN, cuda=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP = get_in_dist_values(CIFAR10_test_in_MAP, targets_CIFAR10)\n",
    "acc_out_CIFAR100_MAP, prob_correct_out_CIFAR100_MAP, ent_out_CIFAR100, MMC_out_CIFAR100_MAP, auroc_out_CIFAR100_MAP = get_out_dist_values(CIFAR10_test_in_MAP, CIFAR10_test_out_CIFAR100_MAP, targets_CIFAR100)\n",
    "acc_out_SVHN_MAP, prob_correct_out_SVHN_MAP, ent_out_SVHN, MMC_out_SVHN_MAP, auroc_out_SVHN_MAP = get_out_dist_values(CIFAR10_test_in_MAP, CIFAR10_test_out_SVHN_MAP, targets_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, MAP, CIFAR10] Accuracy: 0.950; average entropy: 0.066;     MMC: 0.979; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, CIFAR100] Accuracy: 0.009; Average entropy: 0.500;    MMC: 0.825; AUROC: 0.877; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, SVHN] Accuracy: 0.095; Average entropy: 0.658;    MMC: 0.770; AUROC: 0.927; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP, 'CIFAR10', 'MAP')\n",
    "print_out_dist_values(acc_out_CIFAR100_MAP, prob_correct_out_CIFAR100_MAP, ent_out_CIFAR100, MMC_out_CIFAR100_MAP, auroc_out_CIFAR100_MAP, 'CIFAR100', 'MAP')\n",
    "print_out_dist_values(acc_out_SVHN_MAP, prob_correct_out_SVHN_MAP, ent_out_SVHN, MMC_out_SVHN_MAP, auroc_out_SVHN_MAP, 'SVHN', 'MAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonal estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used for sampling with 1000 samples: 6.834805011749268\n",
      "time used for sampling with 1000 samples: 6.896717309951782\n",
      "time used for sampling with 1000 samples: 17.002918004989624\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_test_in_D = predict_diagonal_sampling(CIFAR10_model, testloader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR10_test_out_CIFAR100_D = predict_diagonal_sampling(CIFAR10_model, CIFAR100_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR10_test_out_SVHN_D = predict_diagonal_sampling(CIFAR10_model, test_loader_SVHN, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=1000, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D = get_in_dist_values(CIFAR10_test_in_D, targets_CIFAR10)\n",
    "acc_out_CIFAR100_D, prob_correct_out_CIFAR100_D, ent_out_CIFAR100_D, MMC_out_CIFAR100_D, auroc_out_CIFAR100_D = get_out_dist_values(CIFAR10_test_in_D, CIFAR10_test_out_CIFAR100_D, targets_CIFAR100)\n",
    "acc_out_SVHN_D, prob_correct_out_SVHN_D, ent_out_SVHN_D, MMC_out_SVHN_D, auroc_out_SVHN_D = get_out_dist_values(CIFAR10_test_in_D, CIFAR10_test_out_SVHN_D, targets_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, Diag, CIFAR10] Accuracy: 0.951; average entropy: 0.189;     MMC: 0.951; Prob @ correct: 0.100\n",
      "[Out-Diag, KFAC, CIFAR100] Accuracy: 0.009; Average entropy: 0.500;    MMC: 0.721; AUROC: 0.889; Prob @ correct: 0.100\n",
      "[Out-Diag, KFAC, SVHN] Accuracy: 0.096; Average entropy: 1.012;    MMC: 0.652; AUROC: 0.933; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D, 'CIFAR10', 'Diag')\n",
    "print_out_dist_values(acc_out_CIFAR100_D, prob_correct_out_CIFAR100_D, ent_out_CIFAR100, MMC_out_CIFAR100_D, auroc_out_CIFAR100_D, 'CIFAR100', 'Diag')\n",
    "print_out_dist_values(acc_out_SVHN_D, prob_correct_out_SVHN_D, ent_out_SVHN_D, MMC_out_SVHN_D, auroc_out_SVHN_D, 'SVHN', 'Diag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet Laplace estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_from_Normal(mu, Sigma):\n",
    "    batch_size, K = mu.size(0), mu.size(-1)\n",
    "    Sigma_d = torch.diagonal(Sigma, dim1=1, dim2=2)\n",
    "    sum_exp = torch.sum(torch.exp(-1*mu), dim=1).view(-1,1)\n",
    "    alpha = 1/Sigma_d * (1 - 2/K + torch.exp(mu)/K**2 * sum_exp)\n",
    "    \n",
    "    return(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "#U_post, V_post, B_post\n",
    "def predict_DIR_LPA(model, test_loader, M_W_post, M_b_post, C_W_post, C_b_post, verbose=False, cuda=False, timing=False):\n",
    "    alphas = []\n",
    "    if timing:\n",
    "        time_sum = 0\n",
    "\n",
    "    max_len = int(np.ceil(len(test_loader.dataset)/len(test_loader)))\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        \n",
    "        if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        phi = model.phi(x)\n",
    "\n",
    "        #mu_pred = phi @ M_W_post + M_b_post\n",
    "        #mu_pred -= mu_pred.mean(1).view(-1,1)\n",
    "        #Cov_pred = torch.diag(phi @ U_post @ phi.t()).reshape(-1, 1, 1) * V_post.unsqueeze(0) + B_post.unsqueeze(0)\n",
    "\n",
    "        mu_pred, Cov_pred = get_Gaussian_output(phi, M_W_post, M_b_post, C_W_post, C_b_post)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        alpha = get_alpha_from_Normal(mu_pred, Cov_pred).detach()\n",
    "        t1 = time.time()\n",
    "        if timing:\n",
    "            time_sum += (t1-t0)\n",
    "\n",
    "        alphas.append(alpha)\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "\n",
    "    if timing:\n",
    "        print(\"total time used for transform: {:.05f}\".format(time_sum))\n",
    "    \n",
    "    return(torch.cat(alphas, dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for transform: 0.01708\n",
      "total time used for transform: 0.01550\n",
      "total time used for transform: 0.04004\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_test_in_DIR_LPA = predict_DIR_LPA(CIFAR10_model, testloader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, cuda=True, verbose=False, timing=True).cpu().numpy()\n",
    "CIFAR10_test_out_CIFAR100_DIR_LPA = predict_DIR_LPA(CIFAR10_model, CIFAR100_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR10_test_out_SVHN_DIR_LPA = predict_DIR_LPA(CIFAR10_model, test_loader_SVHN, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize to get the MAP estimate (which is the mode) of the Dirichlet\n",
    "CIFAR10_test_in_DIR_LPAn = CIFAR10_test_in_DIR_LPA/CIFAR10_test_in_DIR_LPA.sum(1).reshape(-1,1)\n",
    "CIFAR10_test_out_CIFAR100_DIR_LPAn = CIFAR10_test_out_CIFAR100_DIR_LPA/CIFAR10_test_out_CIFAR100_DIR_LPA.sum(1).reshape(-1,1)\n",
    "CIFAR10_test_out_SVHN_DIR_LPAn = CIFAR10_test_out_SVHN_DIR_LPA/CIFAR10_test_out_SVHN_DIR_LPA.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA = get_in_dist_values(CIFAR10_test_in_DIR_LPAn, targets_CIFAR10)\n",
    "acc_out_CIFAR100_DIR_LPA, prob_correct_out_CIFAR100_DIR_LPA, ent_out_CIFAR100_DIR_LPA, MMC_out_CIFAR100_DIR_LPA, auroc_out_CIFAR100_DIR_LPA = get_out_dist_values(CIFAR10_test_in_DIR_LPAn, CIFAR10_test_out_CIFAR100_DIR_LPAn, targets_CIFAR100)\n",
    "acc_out_SVHN_DIR_LPA, prob_correct_out_SVHN_DIR_LPA, ent_out_SVHN_DIR_LPA, MMC_out_SVHN_DIR_LPA, auroc_out_SVHN_DIR_LPA = get_out_dist_values(CIFAR10_test_in_DIR_LPAn, CIFAR10_test_out_SVHN_DIR_LPAn, targets_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, DIR_LPA, CIFAR10] Accuracy: 0.950; average entropy: 0.118;     MMC: 0.970; Prob @ correct: 0.100\n",
      "[Out-DIR_LPA, KFAC, CIFAR100] Accuracy: 0.009; Average entropy: 0.761;    MMC: 0.769; AUROC: 0.863; Prob @ correct: 0.100\n",
      "[Out-DIR_LPA, KFAC, SVHN] Accuracy: 0.095; Average entropy: 1.019;    MMC: 0.690; AUROC: 0.925; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA, 'CIFAR10', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_CIFAR100_DIR_LPA, prob_correct_out_CIFAR100_DIR_LPA, ent_out_CIFAR100_DIR_LPA, MMC_out_CIFAR100_DIR_LPA, auroc_out_CIFAR100_DIR_LPA, 'CIFAR100', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_SVHN_DIR_LPA, prob_correct_out_SVHN_DIR_LPA, ent_out_SVHN_DIR_LPA, MMC_out_SVHN_DIR_LPA, auroc_out_SVHN_DIR_LPA, 'SVHN', 'DIR_LPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma, loggamma\n",
    "\n",
    "def beta_function(alpha):\n",
    "    return(np.exp(np.sum([loggamma(a_i) for a_i in alpha]) - loggamma(np.sum(alpha))))\n",
    "\n",
    "def alphas_norm(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    return(alphas/alphas.sum(axis=1).reshape(-1,1))\n",
    "\n",
    "def alphas_variance(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    norm = alphas_norm(alphas)\n",
    "    nom = norm * (1 - norm)\n",
    "    den = alphas.sum(axis=1).reshape(-1,1) + 1\n",
    "    return(nom/den)\n",
    "\n",
    "def log_beta_function(alpha):\n",
    "    return(np.sum([loggamma(a_i) for a_i in alpha]) - loggamma(np.sum(alpha)))\n",
    "\n",
    "def alphas_entropy(alphas):\n",
    "    K = len(alphas[0])\n",
    "    alphas = np.array(alphas)\n",
    "    entropy = []\n",
    "    for x in alphas:\n",
    "        B = log_beta_function(x)\n",
    "        alpha_0 = np.sum(x)\n",
    "        C = (alpha_0 - K)*digamma(alpha_0)\n",
    "        D = np.sum((x-1)*digamma(x))\n",
    "        entropy.append(B + C - D)\n",
    "    \n",
    "    return(np.array(entropy))\n",
    "        \n",
    "\n",
    "def alphas_log_prob(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    dig_sum = digamma(alphas.sum(axis=1).reshape(-1,1))\n",
    "    log_prob = digamma(alphas) - dig_sum\n",
    "    return(log_prob)\n",
    "\n",
    "def auroc_entropy(alphas_in, alphas_out):\n",
    "    \n",
    "    entropy_in = alphas_entropy(alphas_in)\n",
    "    entropy_out = alphas_entropy(alphas_out)\n",
    "    labels = np.zeros(len(entropy_in)+len(entropy_out), dtype='int32')\n",
    "    labels[:len(entropy_in)] = 1\n",
    "    examples = np.concatenate([entropy_in, entropy_out])\n",
    "    auroc_ent = roc_auc_score(labels, examples)\n",
    "    return(auroc_ent)\n",
    "\n",
    "def auroc_variance(alphas_in, alphas_out, method='mean'):\n",
    "    \n",
    "    if method=='mean':\n",
    "        variance_in = alphas_variance(alphas_in).mean(1)\n",
    "        variance_out = alphas_variance(alphas_out).mean(1)\n",
    "    elif method=='max':\n",
    "        variance_in = alphas_variance(alphas_in).max(1)\n",
    "        variance_out = alphas_variance(alphas_out).max(1)\n",
    "    labels = np.zeros(len(variance_in)+len(variance_out), dtype='int32')\n",
    "    labels[:len(variance_in)] = 1\n",
    "    examples = np.concatenate([variance_in, variance_out])\n",
    "    auroc_ent = roc_auc_score(labels, examples)\n",
    "    return(auroc_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc entropy: CIFAR10 in, CIFAR100 out:  0.857630105\n",
      "auroc entropy: CIFAR10 in, SVHN out:  0.9294787069760295\n"
     ]
    }
   ],
   "source": [
    "print(\"auroc entropy: CIFAR10 in, CIFAR100 out: \", 1-auroc_entropy(alphas_in=CIFAR10_test_in_DIR_LPA, alphas_out=CIFAR10_test_out_CIFAR100_DIR_LPA))\n",
    "print(\"auroc entropy: CIFAR10 in, SVHN out: \", 1-auroc_entropy(alphas_in=CIFAR10_test_in_DIR_LPA, alphas_out=CIFAR10_test_out_SVHN_DIR_LPA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc variance: CIFAR10 in, CIFAR100 out:  0.864432495\n",
      "auroc variance: CIFAR10 in, SVHN out:  0.9301324523663184\n"
     ]
    }
   ],
   "source": [
    "print(\"auroc variance: CIFAR10 in, CIFAR100 out: \", 1-auroc_variance(alphas_in=CIFAR10_test_in_DIR_LPA, alphas_out=CIFAR10_test_out_CIFAR100_DIR_LPA, method='mean'))\n",
    "print(\"auroc variance: CIFAR10 in, SVHN out: \", 1-auroc_variance(alphas_in=CIFAR10_test_in_DIR_LPA, alphas_out=CIFAR10_test_out_SVHN_DIR_LPA, method='mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train on SVHN test on CIFAR10 and CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SVHN_model = ResNet18(num_classes=10).to(device)\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_SVHN():\n",
    "    SVHN_path = 'weights/SVHN_resnet18_SGD.pth'\n",
    "    lr = 0.1\n",
    "    epoch = 0\n",
    "    for e in [100, 25, 25]:\n",
    "        print(\"current learning rate: \", lr)\n",
    "        for _ in range(e):\n",
    "            optimizer = optim.SGD(SVHN_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "            train(SVHN_model, epoch, optimizer, train_loader_SVHN, SVHN_path)\n",
    "            test(SVHN_model, epoch, test_loader_SVHN)\n",
    "            epoch += 1\n",
    "        lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_all_SVHN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2643543668091297\n",
      "test accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "##### if you already have a trained model ##############\n",
    "SVHN_PATH = 'weights/SVHN_resnet18_SGD.pth'\n",
    "SVHN_model = ResNet18().to(device)\n",
    "print(\"loading model from: {}\".format(SVHN_PATH))\n",
    "SVHN_model.load_state_dict(torch.load(SVHN_PATH))#, map_location=torch.device('cpu')))\n",
    "#test the model\n",
    "test(SVHN_model, 0, test_loader_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 512 inputs to linear layer with m: 10 classes\n",
      "Batch: 0/204\n",
      "Batch: 1/204\n",
      "Batch: 2/204\n",
      "Batch: 3/204\n",
      "Batch: 4/204\n",
      "Batch: 5/204\n",
      "Batch: 6/204\n",
      "Batch: 7/204\n",
      "Batch: 8/204\n",
      "Batch: 9/204\n",
      "Batch: 10/204\n",
      "Batch: 11/204\n",
      "Batch: 12/204\n",
      "Batch: 13/204\n",
      "Batch: 14/204\n",
      "Batch: 15/204\n",
      "Batch: 16/204\n",
      "Batch: 17/204\n",
      "Batch: 18/204\n",
      "Batch: 19/204\n",
      "Batch: 20/204\n",
      "Batch: 21/204\n",
      "Batch: 22/204\n",
      "Batch: 23/204\n",
      "Batch: 24/204\n",
      "Batch: 25/204\n",
      "Batch: 26/204\n",
      "Batch: 27/204\n",
      "Batch: 28/204\n",
      "Batch: 29/204\n",
      "Batch: 30/204\n",
      "Batch: 31/204\n",
      "Batch: 32/204\n",
      "Batch: 33/204\n",
      "Batch: 34/204\n",
      "Batch: 35/204\n",
      "Batch: 36/204\n",
      "Batch: 37/204\n",
      "Batch: 38/204\n",
      "Batch: 39/204\n",
      "Batch: 40/204\n",
      "Batch: 41/204\n",
      "Batch: 42/204\n",
      "Batch: 43/204\n",
      "Batch: 44/204\n",
      "Batch: 45/204\n",
      "Batch: 46/204\n",
      "Batch: 47/204\n",
      "Batch: 48/204\n",
      "Batch: 49/204\n",
      "Batch: 50/204\n",
      "Batch: 51/204\n",
      "Batch: 52/204\n",
      "Batch: 53/204\n",
      "Batch: 54/204\n",
      "Batch: 55/204\n",
      "Batch: 56/204\n",
      "Batch: 57/204\n",
      "Batch: 58/204\n",
      "Batch: 59/204\n",
      "Batch: 60/204\n",
      "Batch: 61/204\n",
      "Batch: 62/204\n",
      "Batch: 63/204\n",
      "Batch: 64/204\n",
      "Batch: 65/204\n",
      "Batch: 66/204\n",
      "Batch: 67/204\n",
      "Batch: 68/204\n",
      "Batch: 69/204\n",
      "Batch: 70/204\n",
      "Batch: 71/204\n",
      "Batch: 72/204\n",
      "Batch: 73/204\n",
      "Batch: 74/204\n",
      "Batch: 75/204\n",
      "Batch: 76/204\n",
      "Batch: 77/204\n",
      "Batch: 78/204\n",
      "Batch: 79/204\n",
      "Batch: 80/204\n",
      "Batch: 81/204\n",
      "Batch: 82/204\n",
      "Batch: 83/204\n",
      "Batch: 84/204\n",
      "Batch: 85/204\n",
      "Batch: 86/204\n",
      "Batch: 87/204\n",
      "Batch: 88/204\n",
      "Batch: 89/204\n",
      "Batch: 90/204\n",
      "Batch: 91/204\n",
      "Batch: 92/204\n",
      "Batch: 93/204\n",
      "Batch: 94/204\n",
      "Batch: 95/204\n",
      "Batch: 96/204\n",
      "Batch: 97/204\n",
      "Batch: 98/204\n",
      "Batch: 99/204\n",
      "Batch: 100/204\n",
      "Batch: 101/204\n",
      "Batch: 102/204\n",
      "Batch: 103/204\n",
      "Batch: 104/204\n",
      "Batch: 105/204\n",
      "Batch: 106/204\n",
      "Batch: 107/204\n",
      "Batch: 108/204\n",
      "Batch: 109/204\n",
      "Batch: 110/204\n",
      "Batch: 111/204\n",
      "Batch: 112/204\n",
      "Batch: 113/204\n",
      "Batch: 114/204\n",
      "Batch: 115/204\n",
      "Batch: 116/204\n",
      "Batch: 117/204\n",
      "Batch: 118/204\n",
      "Batch: 119/204\n",
      "Batch: 120/204\n",
      "Batch: 121/204\n",
      "Batch: 122/204\n",
      "Batch: 123/204\n",
      "Batch: 124/204\n",
      "Batch: 125/204\n",
      "Batch: 126/204\n",
      "Batch: 127/204\n",
      "Batch: 128/204\n",
      "Batch: 129/204\n",
      "Batch: 130/204\n",
      "Batch: 131/204\n",
      "Batch: 132/204\n",
      "Batch: 133/204\n",
      "Batch: 134/204\n",
      "Batch: 135/204\n",
      "Batch: 136/204\n",
      "Batch: 137/204\n",
      "Batch: 138/204\n",
      "Batch: 139/204\n",
      "Batch: 140/204\n",
      "Batch: 141/204\n",
      "Batch: 142/204\n",
      "Batch: 143/204\n",
      "Batch: 144/204\n",
      "Batch: 145/204\n",
      "Batch: 146/204\n",
      "Batch: 147/204\n",
      "Batch: 148/204\n",
      "Batch: 149/204\n",
      "Batch: 150/204\n",
      "Batch: 151/204\n",
      "Batch: 152/204\n",
      "Batch: 153/204\n",
      "Batch: 154/204\n",
      "Batch: 155/204\n",
      "Batch: 156/204\n",
      "Batch: 157/204\n",
      "Batch: 158/204\n",
      "Batch: 159/204\n",
      "Batch: 160/204\n",
      "Batch: 161/204\n",
      "Batch: 162/204\n",
      "Batch: 163/204\n",
      "Batch: 164/204\n",
      "Batch: 165/204\n",
      "Batch: 166/204\n",
      "Batch: 167/204\n",
      "Batch: 168/204\n",
      "Batch: 169/204\n",
      "Batch: 170/204\n",
      "Batch: 171/204\n",
      "Batch: 172/204\n",
      "Batch: 173/204\n",
      "Batch: 174/204\n",
      "Batch: 175/204\n",
      "Batch: 176/204\n",
      "Batch: 177/204\n",
      "Batch: 178/204\n",
      "Batch: 179/204\n",
      "Batch: 180/204\n",
      "Batch: 181/204\n",
      "Batch: 182/204\n",
      "Batch: 183/204\n",
      "Batch: 184/204\n",
      "Batch: 185/204\n",
      "Batch: 186/204\n",
      "Batch: 187/204\n",
      "Batch: 188/204\n",
      "Batch: 189/204\n",
      "Batch: 190/204\n",
      "Batch: 191/204\n",
      "Batch: 192/204\n",
      "Batch: 193/204\n",
      "Batch: 194/204\n",
      "Batch: 195/204\n",
      "Batch: 196/204\n",
      "Batch: 197/204\n",
      "Batch: 198/204\n",
      "Batch: 199/204\n",
      "Batch: 200/204\n",
      "Batch: 201/204\n",
      "Batch: 202/204\n",
      "Batch: 203/204\n",
      "204\n",
      "M_W_post size:  torch.Size([512, 10])\n",
      "M_b_post size:  torch.Size([10])\n",
      "C_W_post size:  torch.Size([10, 512])\n",
      "C_b_post size:  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN = Diag_second_order(model=SVHN_model,\n",
    "                                                               train_loader=train_loader_SVHN,\n",
    "                                                               var0 = 10,\n",
    "                                                               device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVHN_test_in_MAP = predict_MAP(SVHN_model, test_loader_SVHN, cuda=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR10_MAP = predict_MAP(SVHN_model, testloader, cuda=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR100_MAP = predict_MAP(SVHN_model, CIFAR100_test_loader, cuda=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP = get_in_dist_values(SVHN_test_in_MAP, targets_SVHN)\n",
    "acc_out_CIFAR10_MAP, prob_correct_out_CIFAR10_MAP, ent_out_CIFAR10, MMC_out_CIFAR10_MAP, auroc_out_CIFAR10_MAP = get_out_dist_values(SVHN_test_in_MAP, SVHN_test_out_CIFAR10_MAP, targets_CIFAR10)\n",
    "acc_out_CIFAR100_MAP, prob_correct_out_CIFAR100_MAP, ent_out_CIFAR100_MAP, MMC_out_CIFAR100_MAP, auroc_out_CIFAR100_MAP = get_out_dist_values(SVHN_test_in_MAP, SVHN_test_out_CIFAR100_MAP, targets_CIFAR100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, MAP, SVHN] Accuracy: 1.000; average entropy: 0.011;     MMC: 0.999; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, CIFAR10] Accuracy: 0.084; Average entropy: 1.161;    MMC: 0.625; AUROC: 0.996; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, CIFAR100] Accuracy: 0.012; Average entropy: 1.129;    MMC: 0.637; AUROC: 0.995; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP, 'SVHN', 'MAP')\n",
    "print_out_dist_values(acc_out_CIFAR10_MAP, prob_correct_out_CIFAR10_MAP, ent_out_CIFAR10, MMC_out_CIFAR10_MAP, auroc_out_CIFAR10_MAP, 'CIFAR10', 'MAP')\n",
    "print_out_dist_values(acc_out_CIFAR100_MAP, prob_correct_out_CIFAR100_MAP, ent_out_CIFAR100_MAP, MMC_out_CIFAR100_MAP, auroc_out_CIFAR100_MAP, 'CIFAR100', 'MAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diag sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used for sampling with 1000 samples: 17.187721729278564\n",
      "time used for sampling with 1000 samples: 6.691891431808472\n",
      "time used for sampling with 1000 samples: 6.595185041427612\n"
     ]
    }
   ],
   "source": [
    "SVHN_test_in_D = predict_diagonal_sampling(SVHN_model, test_loader_SVHN, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR10_D = predict_diagonal_sampling(SVHN_model, testloader, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR100_D = predict_diagonal_sampling(SVHN_model, CIFAR100_test_loader, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, n_samples=1000, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D = get_in_dist_values(SVHN_test_in_D, targets_SVHN)\n",
    "acc_out_CIFAR10_D, prob_correct_out_CIFAR10_D, ent_out_CIFAR10_D, MMC_out_CIFAR10_D, auroc_out_CIFAR10_D = get_out_dist_values(SVHN_test_in_D, SVHN_test_out_CIFAR10_D, targets_CIFAR10)\n",
    "acc_out_CIFAR100_D, prob_correct_out_CIFAR100_D, ent_out_CIFAR100_D, MMC_out_CIFAR100_D, auroc_out_CIFAR100_D = get_out_dist_values(SVHN_test_in_D, SVHN_test_out_CIFAR100_D, targets_CIFAR100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, Diag, SVHN] Accuracy: 1.000; average entropy: 0.096;     MMC: 0.986; Prob @ correct: 0.100\n",
      "[Out-Diag, KFAC, CIFAR10] Accuracy: 0.084; Average entropy: 1.379;    MMC: 0.551; AUROC: 0.994; Prob @ correct: 0.100\n",
      "[Out-Diag, KFAC, CIFAR100] Accuracy: 0.012; Average entropy: 1.349;    MMC: 0.563; AUROC: 0.993; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D, 'SVHN', 'Diag')\n",
    "print_out_dist_values(acc_out_CIFAR10_D, prob_correct_out_CIFAR10_D, ent_out_CIFAR10_D, MMC_out_CIFAR10_D, auroc_out_CIFAR10_D, 'CIFAR10', 'Diag')\n",
    "print_out_dist_values(acc_out_CIFAR100_D, prob_correct_out_CIFAR100_D, ent_out_CIFAR100_D, MMC_out_CIFAR100_D, auroc_out_CIFAR100_D, 'CIFAR100', 'Diag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for transform: 0.04083\n",
      "total time used for transform: 0.01693\n",
      "total time used for transform: 0.01568\n"
     ]
    }
   ],
   "source": [
    "SVHN_test_in_DIR_LPA = predict_DIR_LPA(SVHN_model, test_loader_SVHN, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, cuda=True, timing=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR10_DIR_LPA = predict_DIR_LPA(SVHN_model, testloader, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, cuda=True, timing=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR100_DIR_LPA = predict_DIR_LPA(SVHN_model, CIFAR100_test_loader, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize to get the MAP estimate (which is the mode) of the Dirichlet\n",
    "SVHN_test_in_DIR_LPAn = SVHN_test_in_DIR_LPA/SVHN_test_in_DIR_LPA.sum(1).reshape(-1,1)\n",
    "SVHN_test_out_CIFAR10_DIR_LPAn = SVHN_test_out_CIFAR10_DIR_LPA/SVHN_test_out_CIFAR10_DIR_LPA.sum(1).reshape(-1,1)\n",
    "SVHN_test_out_CIFAR100_DIR_LPAn = SVHN_test_out_CIFAR100_DIR_LPA/SVHN_test_out_CIFAR100_DIR_LPA.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA = get_in_dist_values(SVHN_test_in_DIR_LPAn, targets_SVHN)\n",
    "acc_out_CIFAR10_DIR_LPA, prob_correct_out_CIFAR10_DIR_LPA, ent_out_CIFAR10_DIR_LPA, MMC_out_CIFAR10_DIR_LPA, auroc_out_CIFAR10_DIR_LPA = get_out_dist_values(SVHN_test_in_DIR_LPAn, SVHN_test_out_CIFAR10_DIR_LPAn, targets_CIFAR10)\n",
    "acc_out_CIFAR100_DIR_LPA, prob_correct_out_CIFAR100_DIR_LPA, ent_out_CIFAR100_DIR_LPA, MMC_out_CIFAR100_DIR_LPA, auroc_out_CIFAR100_DIR_LPA = get_out_dist_values(SVHN_test_in_DIR_LPAn, SVHN_test_out_CIFAR100_DIR_LPAn, targets_CIFAR100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, DIR_LPA, SVHN] Accuracy: 1.000; average entropy: 0.064;     MMC: 0.991; Prob @ correct: 0.100\n",
      "[Out-DIR_LPA, KFAC, CIFAR10] Accuracy: 0.084; Average entropy: 1.853;    MMC: 0.391; AUROC: 0.996; Prob @ correct: 0.100\n",
      "[Out-DIR_LPA, KFAC, CIFAR100] Accuracy: 0.012; Average entropy: 1.818;    MMC: 0.405; AUROC: 0.995; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA, 'SVHN', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_CIFAR10_DIR_LPA, prob_correct_out_CIFAR10_DIR_LPA, ent_out_CIFAR10_DIR_LPA, MMC_out_CIFAR10_DIR_LPA, auroc_out_CIFAR10_DIR_LPA, 'CIFAR10', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_CIFAR100_DIR_LPA, prob_correct_out_CIFAR100_DIR_LPA, ent_out_CIFAR100_DIR_LPA, MMC_out_CIFAR100_DIR_LPA, auroc_out_CIFAR100_DIR_LPA, 'CIFAR100', 'DIR_LPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc variance: SVHN in, CIFAR10 out:  0.9959753572526122\n",
      "auroc variance: SVHN in, CIFAR100 out:  0.9946402619852489\n"
     ]
    }
   ],
   "source": [
    "print(\"auroc variance: SVHN in, CIFAR10 out: \", 1-auroc_variance(alphas_in=SVHN_test_in_DIR_LPA, alphas_out=SVHN_test_out_CIFAR10_DIR_LPA, method='mean'))\n",
    "print(\"auroc variance: SVHN in, CIFAR100 out: \", 1-auroc_variance(alphas_in=SVHN_test_in_DIR_LPA, alphas_out=SVHN_test_out_CIFAR100_DIR_LPA, method='mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train on CIFAR100 and test on CIFAR10, SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CIFAR100_model = ResNet18(num_classes=100).to(device)\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_CIFAR100():\n",
    "    CIFAR100_path = 'weights/CIFAR100_resnet18_SGD.pth'\n",
    "    lr = 0.1\n",
    "    epoch = 0\n",
    "    for e in [100, 50, 50]:\n",
    "        print(\"current learning rate: \", lr)\n",
    "        for _ in range(e):\n",
    "            optimizer = optim.SGD(CIFAR100_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "            train(CIFAR100_model, epoch, optimizer, CIFAR100_train_loader, CIFAR100_path)\n",
    "            test(CIFAR100_model, epoch, CIFAR100_test_loader)\n",
    "            epoch += 1\n",
    "        lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_all_CIFAR100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.42304813861847\n",
      "test accuracy:  0.5921\n"
     ]
    }
   ],
   "source": [
    "##### if you already have a trained model ##############\n",
    "CIFAR100_PATH = 'weights/CIFAR100_resnet18_SGD.pth'\n",
    "CIFAR100_model = ResNet18(num_classes=100).to(device)\n",
    "print(\"loading model from: {}\".format(CIFAR100_PATH))\n",
    "CIFAR100_model.load_state_dict(torch.load(CIFAR100_PATH))#, map_location=torch.device('cpu')))\n",
    "#test the model\n",
    "test(CIFAR100_model, 0, CIFAR100_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 512 inputs to linear layer with m: 100 classes\n",
      "Batch: 0/391\n",
      "Batch: 1/391\n",
      "Batch: 2/391\n",
      "Batch: 3/391\n",
      "Batch: 4/391\n",
      "Batch: 5/391\n",
      "Batch: 6/391\n",
      "Batch: 7/391\n",
      "Batch: 8/391\n",
      "Batch: 9/391\n",
      "Batch: 10/391\n",
      "Batch: 11/391\n",
      "Batch: 12/391\n",
      "Batch: 13/391\n",
      "Batch: 14/391\n",
      "Batch: 15/391\n",
      "Batch: 16/391\n",
      "Batch: 17/391\n",
      "Batch: 18/391\n",
      "Batch: 19/391\n",
      "Batch: 20/391\n",
      "Batch: 21/391\n",
      "Batch: 22/391\n",
      "Batch: 23/391\n",
      "Batch: 24/391\n",
      "Batch: 25/391\n",
      "Batch: 26/391\n",
      "Batch: 27/391\n",
      "Batch: 28/391\n",
      "Batch: 29/391\n",
      "Batch: 30/391\n",
      "Batch: 31/391\n",
      "Batch: 32/391\n",
      "Batch: 33/391\n",
      "Batch: 34/391\n",
      "Batch: 35/391\n",
      "Batch: 36/391\n",
      "Batch: 37/391\n",
      "Batch: 38/391\n",
      "Batch: 39/391\n",
      "Batch: 40/391\n",
      "Batch: 41/391\n",
      "Batch: 42/391\n",
      "Batch: 43/391\n",
      "Batch: 44/391\n",
      "Batch: 45/391\n",
      "Batch: 46/391\n",
      "Batch: 47/391\n",
      "Batch: 48/391\n",
      "Batch: 49/391\n",
      "Batch: 50/391\n",
      "Batch: 51/391\n",
      "Batch: 52/391\n",
      "Batch: 53/391\n",
      "Batch: 54/391\n",
      "Batch: 55/391\n",
      "Batch: 56/391\n",
      "Batch: 57/391\n",
      "Batch: 58/391\n",
      "Batch: 59/391\n",
      "Batch: 60/391\n",
      "Batch: 61/391\n",
      "Batch: 62/391\n",
      "Batch: 63/391\n",
      "Batch: 64/391\n",
      "Batch: 65/391\n",
      "Batch: 66/391\n",
      "Batch: 67/391\n",
      "Batch: 68/391\n",
      "Batch: 69/391\n",
      "Batch: 70/391\n",
      "Batch: 71/391\n",
      "Batch: 72/391\n",
      "Batch: 73/391\n",
      "Batch: 74/391\n",
      "Batch: 75/391\n",
      "Batch: 76/391\n",
      "Batch: 77/391\n",
      "Batch: 78/391\n",
      "Batch: 79/391\n",
      "Batch: 80/391\n",
      "Batch: 81/391\n",
      "Batch: 82/391\n",
      "Batch: 83/391\n",
      "Batch: 84/391\n",
      "Batch: 85/391\n",
      "Batch: 86/391\n",
      "Batch: 87/391\n",
      "Batch: 88/391\n",
      "Batch: 89/391\n",
      "Batch: 90/391\n",
      "Batch: 91/391\n",
      "Batch: 92/391\n",
      "Batch: 93/391\n",
      "Batch: 94/391\n",
      "Batch: 95/391\n",
      "Batch: 96/391\n",
      "Batch: 97/391\n",
      "Batch: 98/391\n",
      "Batch: 99/391\n",
      "Batch: 100/391\n",
      "Batch: 101/391\n",
      "Batch: 102/391\n",
      "Batch: 103/391\n",
      "Batch: 104/391\n",
      "Batch: 105/391\n",
      "Batch: 106/391\n",
      "Batch: 107/391\n",
      "Batch: 108/391\n",
      "Batch: 109/391\n",
      "Batch: 110/391\n",
      "Batch: 111/391\n",
      "Batch: 112/391\n",
      "Batch: 113/391\n",
      "Batch: 114/391\n",
      "Batch: 115/391\n",
      "Batch: 116/391\n",
      "Batch: 117/391\n",
      "Batch: 118/391\n",
      "Batch: 119/391\n",
      "Batch: 120/391\n",
      "Batch: 121/391\n",
      "Batch: 122/391\n",
      "Batch: 123/391\n",
      "Batch: 124/391\n",
      "Batch: 125/391\n",
      "Batch: 126/391\n",
      "Batch: 127/391\n",
      "Batch: 128/391\n",
      "Batch: 129/391\n",
      "Batch: 130/391\n",
      "Batch: 131/391\n",
      "Batch: 132/391\n",
      "Batch: 133/391\n",
      "Batch: 134/391\n",
      "Batch: 135/391\n",
      "Batch: 136/391\n",
      "Batch: 137/391\n",
      "Batch: 138/391\n",
      "Batch: 139/391\n",
      "Batch: 140/391\n",
      "Batch: 141/391\n",
      "Batch: 142/391\n",
      "Batch: 143/391\n",
      "Batch: 144/391\n",
      "Batch: 145/391\n",
      "Batch: 146/391\n",
      "Batch: 147/391\n",
      "Batch: 148/391\n",
      "Batch: 149/391\n",
      "Batch: 150/391\n",
      "Batch: 151/391\n",
      "Batch: 152/391\n",
      "Batch: 153/391\n",
      "Batch: 154/391\n",
      "Batch: 155/391\n",
      "Batch: 156/391\n",
      "Batch: 157/391\n",
      "Batch: 158/391\n",
      "Batch: 159/391\n",
      "Batch: 160/391\n",
      "Batch: 161/391\n",
      "Batch: 162/391\n",
      "Batch: 163/391\n",
      "Batch: 164/391\n",
      "Batch: 165/391\n",
      "Batch: 166/391\n",
      "Batch: 167/391\n",
      "Batch: 168/391\n",
      "Batch: 169/391\n",
      "Batch: 170/391\n",
      "Batch: 171/391\n",
      "Batch: 172/391\n",
      "Batch: 173/391\n",
      "Batch: 174/391\n",
      "Batch: 175/391\n",
      "Batch: 176/391\n",
      "Batch: 177/391\n",
      "Batch: 178/391\n",
      "Batch: 179/391\n",
      "Batch: 180/391\n",
      "Batch: 181/391\n",
      "Batch: 182/391\n",
      "Batch: 183/391\n",
      "Batch: 184/391\n",
      "Batch: 185/391\n",
      "Batch: 186/391\n",
      "Batch: 187/391\n",
      "Batch: 188/391\n",
      "Batch: 189/391\n",
      "Batch: 190/391\n",
      "Batch: 191/391\n",
      "Batch: 192/391\n",
      "Batch: 193/391\n",
      "Batch: 194/391\n",
      "Batch: 195/391\n",
      "Batch: 196/391\n",
      "Batch: 197/391\n",
      "Batch: 198/391\n",
      "Batch: 199/391\n",
      "Batch: 200/391\n",
      "Batch: 201/391\n",
      "Batch: 202/391\n",
      "Batch: 203/391\n",
      "Batch: 204/391\n",
      "Batch: 205/391\n",
      "Batch: 206/391\n",
      "Batch: 207/391\n",
      "Batch: 208/391\n",
      "Batch: 209/391\n",
      "Batch: 210/391\n",
      "Batch: 211/391\n",
      "Batch: 212/391\n",
      "Batch: 213/391\n",
      "Batch: 214/391\n",
      "Batch: 215/391\n",
      "Batch: 216/391\n",
      "Batch: 217/391\n",
      "Batch: 218/391\n",
      "Batch: 219/391\n",
      "Batch: 220/391\n",
      "Batch: 221/391\n",
      "Batch: 222/391\n",
      "Batch: 223/391\n",
      "Batch: 224/391\n",
      "Batch: 225/391\n",
      "Batch: 226/391\n",
      "Batch: 227/391\n",
      "Batch: 228/391\n",
      "Batch: 229/391\n",
      "Batch: 230/391\n",
      "Batch: 231/391\n",
      "Batch: 232/391\n",
      "Batch: 233/391\n",
      "Batch: 234/391\n",
      "Batch: 235/391\n",
      "Batch: 236/391\n",
      "Batch: 237/391\n",
      "Batch: 238/391\n",
      "Batch: 239/391\n",
      "Batch: 240/391\n",
      "Batch: 241/391\n",
      "Batch: 242/391\n",
      "Batch: 243/391\n",
      "Batch: 244/391\n",
      "Batch: 245/391\n",
      "Batch: 246/391\n",
      "Batch: 247/391\n",
      "Batch: 248/391\n",
      "Batch: 249/391\n",
      "Batch: 250/391\n",
      "Batch: 251/391\n",
      "Batch: 252/391\n",
      "Batch: 253/391\n",
      "Batch: 254/391\n",
      "Batch: 255/391\n",
      "Batch: 256/391\n",
      "Batch: 257/391\n",
      "Batch: 258/391\n",
      "Batch: 259/391\n",
      "Batch: 260/391\n",
      "Batch: 261/391\n",
      "Batch: 262/391\n",
      "Batch: 263/391\n",
      "Batch: 264/391\n",
      "Batch: 265/391\n",
      "Batch: 266/391\n",
      "Batch: 267/391\n",
      "Batch: 268/391\n",
      "Batch: 269/391\n",
      "Batch: 270/391\n",
      "Batch: 271/391\n",
      "Batch: 272/391\n",
      "Batch: 273/391\n",
      "Batch: 274/391\n",
      "Batch: 275/391\n",
      "Batch: 276/391\n",
      "Batch: 277/391\n",
      "Batch: 278/391\n",
      "Batch: 279/391\n",
      "Batch: 280/391\n",
      "Batch: 281/391\n",
      "Batch: 282/391\n",
      "Batch: 283/391\n",
      "Batch: 284/391\n",
      "Batch: 285/391\n",
      "Batch: 286/391\n",
      "Batch: 287/391\n",
      "Batch: 288/391\n",
      "Batch: 289/391\n",
      "Batch: 290/391\n",
      "Batch: 291/391\n",
      "Batch: 292/391\n",
      "Batch: 293/391\n",
      "Batch: 294/391\n",
      "Batch: 295/391\n",
      "Batch: 296/391\n",
      "Batch: 297/391\n",
      "Batch: 298/391\n",
      "Batch: 299/391\n",
      "Batch: 300/391\n",
      "Batch: 301/391\n",
      "Batch: 302/391\n",
      "Batch: 303/391\n",
      "Batch: 304/391\n",
      "Batch: 305/391\n",
      "Batch: 306/391\n",
      "Batch: 307/391\n",
      "Batch: 308/391\n",
      "Batch: 309/391\n",
      "Batch: 310/391\n",
      "Batch: 311/391\n",
      "Batch: 312/391\n",
      "Batch: 313/391\n",
      "Batch: 314/391\n",
      "Batch: 315/391\n",
      "Batch: 316/391\n",
      "Batch: 317/391\n",
      "Batch: 318/391\n",
      "Batch: 319/391\n",
      "Batch: 320/391\n",
      "Batch: 321/391\n",
      "Batch: 322/391\n",
      "Batch: 323/391\n",
      "Batch: 324/391\n",
      "Batch: 325/391\n",
      "Batch: 326/391\n",
      "Batch: 327/391\n",
      "Batch: 328/391\n",
      "Batch: 329/391\n",
      "Batch: 330/391\n",
      "Batch: 331/391\n",
      "Batch: 332/391\n",
      "Batch: 333/391\n",
      "Batch: 334/391\n",
      "Batch: 335/391\n",
      "Batch: 336/391\n",
      "Batch: 337/391\n",
      "Batch: 338/391\n",
      "Batch: 339/391\n",
      "Batch: 340/391\n",
      "Batch: 341/391\n",
      "Batch: 342/391\n",
      "Batch: 343/391\n",
      "Batch: 344/391\n",
      "Batch: 345/391\n",
      "Batch: 346/391\n",
      "Batch: 347/391\n",
      "Batch: 348/391\n",
      "Batch: 349/391\n",
      "Batch: 350/391\n",
      "Batch: 351/391\n",
      "Batch: 352/391\n",
      "Batch: 353/391\n",
      "Batch: 354/391\n",
      "Batch: 355/391\n",
      "Batch: 356/391\n",
      "Batch: 357/391\n",
      "Batch: 358/391\n",
      "Batch: 359/391\n",
      "Batch: 360/391\n",
      "Batch: 361/391\n",
      "Batch: 362/391\n",
      "Batch: 363/391\n",
      "Batch: 364/391\n",
      "Batch: 365/391\n",
      "Batch: 366/391\n",
      "Batch: 367/391\n",
      "Batch: 368/391\n",
      "Batch: 369/391\n",
      "Batch: 370/391\n",
      "Batch: 371/391\n",
      "Batch: 372/391\n",
      "Batch: 373/391\n",
      "Batch: 374/391\n",
      "Batch: 375/391\n",
      "Batch: 376/391\n",
      "Batch: 377/391\n",
      "Batch: 378/391\n",
      "Batch: 379/391\n",
      "Batch: 380/391\n",
      "Batch: 381/391\n",
      "Batch: 382/391\n",
      "Batch: 383/391\n",
      "Batch: 384/391\n",
      "Batch: 385/391\n",
      "Batch: 386/391\n",
      "Batch: 387/391\n",
      "Batch: 388/391\n",
      "Batch: 389/391\n",
      "Batch: 390/391\n",
      "391\n",
      "M_W_post size:  torch.Size([512, 100])\n",
      "M_b_post size:  torch.Size([100])\n",
      "C_W_post size:  torch.Size([100, 512])\n",
      "C_b_post size:  torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100 = Diag_second_order(model=CIFAR100_model,\n",
    "                                                               train_loader=CIFAR100_train_loader,\n",
    "                                                               var0 = 75,\n",
    "                                                               device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP estimate for CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR100_test_in_MAP = predict_MAP(CIFAR100_model, CIFAR100_test_loader, cuda=True).cpu().numpy()\n",
    "CIFAR100_test_out_CIFAR10_MAP = predict_MAP(CIFAR100_model, testloader, cuda=True).cpu().numpy()\n",
    "CIFAR100_test_out_SVHN_MAP = predict_MAP(CIFAR100_model, test_loader_SVHN, cuda=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP = get_in_dist_values(CIFAR100_test_in_MAP, targets_CIFAR100)\n",
    "acc_out_CIFAR10_MAP, prob_correct_out_CIFAR10_MAP, ent_out_CIFAR10, MMC_out_CIFAR10_MAP, auroc_out_CIFAR10_MAP = get_out_dist_values(CIFAR100_test_in_MAP, CIFAR100_test_out_CIFAR10_MAP, targets_CIFAR10)\n",
    "acc_out_SVHN_MAP, prob_correct_out_SVHN_MAP, ent_out_SVHN_MAP, MMC_out_SVHN_MAP, auroc_out_SVHN_MAP = get_out_dist_values(CIFAR100_test_in_MAP, CIFAR100_test_out_SVHN_MAP, targets_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, MAP, CIFAR100] Accuracy: 0.592; average entropy: 2.159;     MMC: 0.556; Prob @ correct: 0.010\n",
      "[Out-MAP, KFAC, CIFAR10] Accuracy: 0.007; Average entropy: 3.419;    MMC: 0.287; AUROC: 0.715; Prob @ correct: 0.010\n",
      "[Out-MAP, KFAC, SVHN] Accuracy: 0.003; Average entropy: 3.323;    MMC: 0.317; AUROC: 0.687; Prob @ correct: 0.010\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP, 'CIFAR100', 'MAP')\n",
    "print_out_dist_values(acc_out_CIFAR10_MAP, prob_correct_out_CIFAR10_MAP, ent_out_CIFAR10, MMC_out_CIFAR10_MAP, auroc_out_CIFAR10_MAP, 'CIFAR10', 'MAP')\n",
    "print_out_dist_values(acc_out_SVHN_MAP, prob_correct_out_SVHN_MAP, ent_out_SVHN_MAP, MMC_out_SVHN_MAP, auroc_out_SVHN_MAP, 'SVHN', 'MAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diag sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used for sampling with 1000 samples: 6.725818872451782\n",
      "time used for sampling with 1000 samples: 6.7555623054504395\n",
      "time used for sampling with 1000 samples: 17.338218927383423\n"
     ]
    }
   ],
   "source": [
    "CIFAR100_test_in_D = predict_diagonal_sampling(CIFAR100_model, CIFAR100_test_loader, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR100_test_out_CIFAR10_D = predict_diagonal_sampling(CIFAR100_model, testloader, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR100_test_out_SVHN_D = predict_diagonal_sampling(CIFAR100_model, test_loader_SVHN, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, n_samples=1000, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marius/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D = get_in_dist_values(CIFAR100_test_in_D, targets_CIFAR100)\n",
    "acc_out_CIFAR10_D, prob_correct_out_CIFAR10_D, ent_out_CIFAR10_D, MMC_out_CIFAR10_D, auroc_out_CIFAR10_D = get_out_dist_values(CIFAR100_test_in_D, CIFAR100_test_out_CIFAR10_D, targets_CIFAR10)\n",
    "acc_out_SVHN_D, prob_correct_out_SVHN_D, ent_out_SVHN_D, MMC_out_SVHN_D, auroc_out_SVHN_D = get_out_dist_values(CIFAR100_test_in_D, CIFAR100_test_out_SVHN_D, targets_CIFAR100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, Diag, CIFAR100] Accuracy: 0.591; average entropy: 2.295;     MMC: 0.528; Prob @ correct: 0.010\n",
      "[Out-Diag, KFAC, CIFAR10] Accuracy: 0.007; Average entropy: 3.509;    MMC: 0.267; AUROC: 0.716; Prob @ correct: 0.010\n",
      "[Out-Diag, KFAC, SVHN] Accuracy: 0.000; Average entropy: 3.424;    MMC: 0.295; AUROC: 0.686; Prob @ correct: 0.010\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D, 'CIFAR100', 'Diag')\n",
    "print_out_dist_values(acc_out_CIFAR10_D, prob_correct_out_CIFAR10_D, ent_out_CIFAR10_D, MMC_out_CIFAR10_D, auroc_out_CIFAR10_D, 'CIFAR10', 'Diag')\n",
    "print_out_dist_values(acc_out_SVHN_D, prob_correct_out_SVHN_D, ent_out_SVHN_D, MMC_out_SVHN_D, auroc_out_SVHN_D, 'SVHN', 'Diag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for transform: 0.01617\n",
      "total time used for transform: 0.01767\n",
      "total time used for transform: 0.04027\n"
     ]
    }
   ],
   "source": [
    "CIFAR100_test_in_DIR_LPA = predict_DIR_LPA(CIFAR100_model, CIFAR100_test_loader, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR100_test_out_CIFAR10_DIR_LPA = predict_DIR_LPA(CIFAR100_model, testloader, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR100_test_out_SVHN_DIR_LPA = predict_DIR_LPA(CIFAR100_model, test_loader_SVHN, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize to get the MAP estimate (which is the mode) of the Dirichlet\n",
    "CIFAR100_test_in_DIR_LPAn = CIFAR100_test_in_DIR_LPA/CIFAR100_test_in_DIR_LPA.sum(1).reshape(-1,1)\n",
    "CIFAR100_test_out_CIFAR10_DIR_LPAn = CIFAR100_test_out_CIFAR10_DIR_LPA/CIFAR100_test_out_CIFAR10_DIR_LPA.sum(1).reshape(-1,1)\n",
    "CIFAR100_test_out_SVHN_DIR_LPAn = CIFAR100_test_out_SVHN_DIR_LPA/CIFAR100_test_out_SVHN_DIR_LPA.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA = get_in_dist_values(CIFAR100_test_in_DIR_LPAn, targets_CIFAR100)\n",
    "acc_out_CIFAR10_DIR_LPA, prob_correct_out_CIFAR10_DIR_LPA, ent_out_CIFAR10_DIR_LPA, MMC_out_CIFAR10_DIR_LPA, auroc_out_CIFAR10_DIR_LPA = get_out_dist_values(CIFAR100_test_in_DIR_LPAn, CIFAR100_test_out_CIFAR10_DIR_LPAn, targets_CIFAR10)\n",
    "acc_out_SVHN_DIR_LPA, prob_correct_out_SVHN_DIR_LPA, ent_out_SVHN_DIR_LPA, MMC_out_SVHN_DIR_LPA, auroc_out_SVHN_DIR_LPA = get_out_dist_values(CIFAR100_test_in_DIR_LPAn, CIFAR100_test_out_SVHN_DIR_LPAn, targets_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, DIR_LPA, CIFAR100] Accuracy: 0.592; average entropy: 3.581;     MMC: 0.264; Prob @ correct: 0.010\n",
      "[Out-DIR_LPA, KFAC, CIFAR10] Accuracy: 0.007; Average entropy: 4.420;    MMC: 0.064; AUROC: 0.714; Prob @ correct: 0.010\n",
      "[Out-DIR_LPA, KFAC, SVHN] Accuracy: 0.003; Average entropy: 4.485;    MMC: 0.052; AUROC: 0.694; Prob @ correct: 0.010\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA, 'CIFAR100', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_CIFAR10_DIR_LPA, prob_correct_out_CIFAR10_DIR_LPA, ent_out_CIFAR10_DIR_LPA, MMC_out_CIFAR10_DIR_LPA, auroc_out_CIFAR10_DIR_LPA, 'CIFAR10', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_SVHN_DIR_LPA, prob_correct_out_SVHN_DIR_LPA, ent_out_SVHN_DIR_LPA, MMC_out_SVHN_DIR_LPA, auroc_out_SVHN_DIR_LPA, 'SVHN', 'DIR_LPA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.ones(32, 10).cuda()\n",
    "Sigma = torch.stack(32 * [torch.eye(10)]).cuda()\n",
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.ones(32, 100).cuda()\n",
    "Sigma = torch.stack(32 * [torch.eye(100)]).cuda()\n",
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8539102077484131\n"
     ]
    }
   ],
   "source": [
    "post_pred = MultivariateNormal(mu, Sigma)\n",
    "\n",
    "py =[]\n",
    "# MC-integral\n",
    "t0 = time.time()\n",
    "py_ = 0\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    f_s = post_pred.rsample()\n",
    "    py_ += torch.softmax(f_s, 1)\n",
    "\n",
    "py_ /= n_samples\n",
    "py_ = py_.detach()\n",
    "\n",
    "py.append(py_)\n",
    "t1 = time.time()\n",
    "\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
