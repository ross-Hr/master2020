{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version:  1.2.0\n",
      "cuda available:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from math import *\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import KFAC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from tqdm import tqdm, trange\n",
    "import pytest\n",
    "import matplotlib.pyplot as plt\n",
    "from DirLPA_utils import * \n",
    "\n",
    "print(\"pytorch version: \", torch.__version__)\n",
    "print(\"cuda available: \", torch.cuda.is_available())\n",
    "\n",
    "s = 127\n",
    "np.random.seed(s)\n",
    "torch.manual_seed(s)\n",
    "torch.cuda.manual_seed(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cifar10 on Resnet32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TRAIN_CIFAR10 = 128\n",
    "BATCH_SIZE_TEST_CIFAR10 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, targets):\n",
    "    \"\"\"Helper function to print the accuracy\"\"\"\n",
    "    predictions = output.argmax(dim=1, keepdim=True).view_as(targets)\n",
    "    return predictions.eq(targets).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "transform_base = [transforms.ToTensor()]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "    ] + transform_base)\n",
    "\n",
    "transform_test = transforms.Compose(transform_base)\n",
    "transform_train = transforms.RandomChoice([transform_train, transform_test])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='~/data/cifar10', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "CIFAR10_train_dataset, CIFAR10_val_dataset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "CIFAR10_train_loader = torch.utils.data.DataLoader(CIFAR10_train_dataset, batch_size=BATCH_SIZE_TRAIN_CIFAR10,\n",
    "                                          shuffle=False)\n",
    "\n",
    "CIFAR10_val_loader = torch.utils.data.DataLoader(CIFAR10_val_dataset, batch_size=BATCH_SIZE_TRAIN_CIFAR10,\n",
    "                                          shuffle=False)\n",
    "\n",
    "CIFAR10_test = torchvision.datasets.CIFAR10(root='~/data/cifar10', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "CIFAR10_test_loader = torch.utils.data.DataLoader(CIFAR10_test, batch_size=BATCH_SIZE_TEST_CIFAR10,\n",
    "                                         shuffle=False)\n",
    "\n",
    "CIFAR10_classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(CIFAR10_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "nrow = int(BATCH_SIZE_TRAIN_CIFAR10/4)\n",
    "imshow(torchvision.utils.make_grid(images, nrow=nrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#load in CIFAR100\n",
    "BATCH_SIZE_TRAIN_CIFAR100 = 128\n",
    "BATCH_SIZE_TEST_CIFAR100 = 128\n",
    "\n",
    "CIFAR100_train = torchvision.datasets.CIFAR100(root='~/data/cifar100', train=True,\n",
    "                                       download=True, transform=transform_test)\n",
    "CIFAR100_train_loader = torch.utils.data.DataLoader(CIFAR100_train, batch_size=BATCH_SIZE_TRAIN_CIFAR100,\n",
    "                                         shuffle=False)\n",
    "\n",
    "CIFAR100_test = torchvision.datasets.CIFAR100(root='~/data/cifar100', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "CIFAR100_test_loader = torch.utils.data.DataLoader(CIFAR100_test, batch_size=BATCH_SIZE_TEST_CIFAR100,\n",
    "                                         shuffle=False)\n",
    "\n",
    "CIFAR100_classes = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
    "    'worm'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to /home/marius/data/SVHN/train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 25944064/182040794 [00:07<00:44, 3471080.90it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5e4e6bec6f95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m train_data_SVHN = torchvision.datasets.SVHN('~/data/SVHN', split='train',\n\u001b[0;32m----> 6\u001b[0;31m                              download=True, transform=transform_train)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m test_data_SVHN = torchvision.datasets.SVHN('~/data/SVHN', split='test',\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/svhn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/svhn.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mmd5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     82\u001b[0m             urllib.request.urlretrieve(\n\u001b[1;32m     83\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_bar_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             )\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load SVHN\n",
    "BATCH_SIZE_TRAIN_SVHN = 128\n",
    "BATCH_SIZE_TEST_SVHN = 128\n",
    "\n",
    "train_data_SVHN = torchvision.datasets.SVHN('~/data/SVHN', split='train',\n",
    "                             download=True, transform=transform_train)\n",
    "\n",
    "test_data_SVHN = torchvision.datasets.SVHN('~/data/SVHN', split='test',\n",
    "                             download=True, transform=transform_test)\n",
    "\n",
    "train_loader_SVHN = torch.utils.data.DataLoader(test_data_SVHN, batch_size=BATCH_SIZE_TRAIN_SVHN)\n",
    "test_loader_SVHN = torch.utils.data.DataLoader(test_data_SVHN, batch_size=BATCH_SIZE_TEST_SVHN)\n",
    "print(\"everything loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CIFAR10 on ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "(3, 138, 1090)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='~/data/cifar10', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE_TRAIN_CIFAR10, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='~/data/cifar10', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE_TEST_CIFAR10, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    print(npimg.shape)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(CIFAR10_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "nrow = int(BATCH_SIZE_TRAIN_CIFAR10/4)\n",
    "print(images.size())\n",
    "imshow(torchvision.utils.make_grid(images, nrow=nrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def phi(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.phi(x)\n",
    "        out = self.fc(out)\n",
    "        return(out)\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CIFAR10_model = ResNet18().to(device)\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(CIFAR10_model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(net, epoch, optimizer, trainloader, filename):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    \n",
    "    print(\"train loss: \", train_loss)\n",
    "    print(\"train accuracy: \", correct/total)\n",
    "    print(\"saving model at: {}\".format(filename))\n",
    "    torch.save(net.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, epoch, testloader):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        print(\"test loss: \", test_loss)\n",
    "        print(\"test accuracy: \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all():\n",
    "    CIFAR10_path = 'weights/cifar10_resnet18_SGD.pth'\n",
    "    lr = 0.1\n",
    "    epoch = 0\n",
    "    for e in [100, 50, 50]:\n",
    "        print(\"current learning rate: \", lr)\n",
    "        for _ in range(e):\n",
    "            optimizer = optim.SGD(CIFAR10_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "            train(CIFAR10_model, epoch, optimizer, trainloader, CIFAR10_path)\n",
    "            test(CIFAR10_model, epoch, testloader)\n",
    "            epoch += 1\n",
    "        lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current learning rate:  0.1\n",
      "\n",
      "Epoch: 0\n",
      "train loss:  735.2782756090164\n",
      "train accuracy:  0.32482\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  122.92022550106049\n",
      "test accuracy:  0.4207\n",
      "\n",
      "Epoch: 1\n",
      "train loss:  549.5818940401077\n",
      "train accuracy:  0.48396\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  102.8718353509903\n",
      "test accuracy:  0.5241\n",
      "\n",
      "Epoch: 2\n",
      "train loss:  441.67920756340027\n",
      "train accuracy:  0.59368\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  120.26511776447296\n",
      "test accuracy:  0.5313\n",
      "\n",
      "Epoch: 3\n",
      "train loss:  363.2937567830086\n",
      "train accuracy:  0.67208\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  73.24164694547653\n",
      "test accuracy:  0.6793\n",
      "\n",
      "Epoch: 4\n",
      "train loss:  307.59092301130295\n",
      "train accuracy:  0.72514\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  62.32203954458237\n",
      "test accuracy:  0.7245\n",
      "\n",
      "Epoch: 5\n",
      "train loss:  259.6400728225708\n",
      "train accuracy:  0.7697\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  63.78632813692093\n",
      "test accuracy:  0.7294\n",
      "\n",
      "Epoch: 6\n",
      "train loss:  234.68083626031876\n",
      "train accuracy:  0.79464\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  54.20434129238129\n",
      "test accuracy:  0.7699\n",
      "\n",
      "Epoch: 7\n",
      "train loss:  220.6452583372593\n",
      "train accuracy:  0.80546\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  74.70105791091919\n",
      "test accuracy:  0.6883\n",
      "\n",
      "Epoch: 8\n",
      "train loss:  208.96894630789757\n",
      "train accuracy:  0.81706\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  52.42300161719322\n",
      "test accuracy:  0.7751\n",
      "\n",
      "Epoch: 9\n",
      "train loss:  197.9608652293682\n",
      "train accuracy:  0.82674\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  43.445687621831894\n",
      "test accuracy:  0.8084\n",
      "\n",
      "Epoch: 10\n",
      "train loss:  191.92938047647476\n",
      "train accuracy:  0.8313\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  45.472194850444794\n",
      "test accuracy:  0.8047\n",
      "\n",
      "Epoch: 11\n",
      "train loss:  183.22814339399338\n",
      "train accuracy:  0.83848\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  49.94606575369835\n",
      "test accuracy:  0.7937\n",
      "\n",
      "Epoch: 12\n",
      "train loss:  177.1351764947176\n",
      "train accuracy:  0.84428\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  41.601422727108\n",
      "test accuracy:  0.8241\n",
      "\n",
      "Epoch: 13\n",
      "train loss:  173.64965426921844\n",
      "train accuracy:  0.84808\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  50.47848695516586\n",
      "test accuracy:  0.7832\n",
      "\n",
      "Epoch: 14\n",
      "train loss:  168.9003986865282\n",
      "train accuracy:  0.85272\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  42.03575322031975\n",
      "test accuracy:  0.8201\n",
      "\n",
      "Epoch: 15\n",
      "train loss:  166.36166819930077\n",
      "train accuracy:  0.85418\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  48.62107786536217\n",
      "test accuracy:  0.7997\n",
      "\n",
      "Epoch: 16\n",
      "train loss:  161.1516668200493\n",
      "train accuracy:  0.86122\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  37.85386994481087\n",
      "test accuracy:  0.8381\n",
      "\n",
      "Epoch: 17\n",
      "train loss:  159.48876737058163\n",
      "train accuracy:  0.86262\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  45.891616344451904\n",
      "test accuracy:  0.8059\n",
      "\n",
      "Epoch: 18\n",
      "train loss:  158.6100970953703\n",
      "train accuracy:  0.86168\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  45.009265810251236\n",
      "test accuracy:  0.8092\n",
      "\n",
      "Epoch: 19\n",
      "train loss:  153.41581417620182\n",
      "train accuracy:  0.86586\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  62.399791836738586\n",
      "test accuracy:  0.7732\n",
      "\n",
      "Epoch: 20\n",
      "train loss:  153.22674678266048\n",
      "train accuracy:  0.867\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  38.46659451723099\n",
      "test accuracy:  0.8337\n",
      "\n",
      "Epoch: 21\n",
      "train loss:  151.41149884462357\n",
      "train accuracy:  0.86652\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  39.3250874876976\n",
      "test accuracy:  0.8308\n",
      "\n",
      "Epoch: 22\n",
      "train loss:  147.56377239525318\n",
      "train accuracy:  0.87076\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  38.83779215812683\n",
      "test accuracy:  0.834\n",
      "\n",
      "Epoch: 23\n",
      "train loss:  146.43692187964916\n",
      "train accuracy:  0.87116\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  37.48514437675476\n",
      "test accuracy:  0.8429\n",
      "\n",
      "Epoch: 24\n",
      "train loss:  146.25257217884064\n",
      "train accuracy:  0.87154\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  39.27301988005638\n",
      "test accuracy:  0.8346\n",
      "\n",
      "Epoch: 25\n",
      "train loss:  144.00774157047272\n",
      "train accuracy:  0.874\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  40.15346199274063\n",
      "test accuracy:  0.8299\n",
      "\n",
      "Epoch: 26\n",
      "train loss:  145.0283734947443\n",
      "train accuracy:  0.87166\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  51.259469747543335\n",
      "test accuracy:  0.7954\n",
      "\n",
      "Epoch: 27\n",
      "train loss:  141.95609113574028\n",
      "train accuracy:  0.87572\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  42.2141595184803\n",
      "test accuracy:  0.8266\n",
      "\n",
      "Epoch: 28\n",
      "train loss:  141.04627867043018\n",
      "train accuracy:  0.87708\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  34.367194443941116\n",
      "test accuracy:  0.8554\n",
      "\n",
      "Epoch: 29\n",
      "train loss:  138.92403322458267\n",
      "train accuracy:  0.87816\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  45.50177337229252\n",
      "test accuracy:  0.81\n",
      "\n",
      "Epoch: 30\n",
      "train loss:  138.70095413923264\n",
      "train accuracy:  0.87938\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  41.137366861104965\n",
      "test accuracy:  0.8265\n",
      "\n",
      "Epoch: 31\n",
      "train loss:  137.76476934552193\n",
      "train accuracy:  0.88044\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  40.11628299951553\n",
      "test accuracy:  0.834\n",
      "\n",
      "Epoch: 32\n",
      "train loss:  136.8565664589405\n",
      "train accuracy:  0.8822\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  37.93354448676109\n",
      "test accuracy:  0.8399\n",
      "\n",
      "Epoch: 33\n",
      "train loss:  135.8086083829403\n",
      "train accuracy:  0.88248\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  42.32056511938572\n",
      "test accuracy:  0.8285\n",
      "\n",
      "Epoch: 34\n",
      "train loss:  136.14403495192528\n",
      "train accuracy:  0.88194\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  37.620365619659424\n",
      "test accuracy:  0.8398\n",
      "\n",
      "Epoch: 35\n",
      "train loss:  134.57910558581352\n",
      "train accuracy:  0.88208\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  49.844798535108566\n",
      "test accuracy:  0.7975\n",
      "\n",
      "Epoch: 36\n",
      "train loss:  133.8311863988638\n",
      "train accuracy:  0.88232\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  48.578622192144394\n",
      "test accuracy:  0.7996\n",
      "\n",
      "Epoch: 37\n",
      "train loss:  133.12614685297012\n",
      "train accuracy:  0.88412\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  37.03795041143894\n",
      "test accuracy:  0.837\n",
      "\n",
      "Epoch: 38\n",
      "train loss:  133.39302895963192\n",
      "train accuracy:  0.88262\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  46.30499216914177\n",
      "test accuracy:  0.8189\n",
      "\n",
      "Epoch: 39\n",
      "train loss:  132.67175567150116\n",
      "train accuracy:  0.8845\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  37.93428233265877\n",
      "test accuracy:  0.8364\n",
      "\n",
      "Epoch: 40\n",
      "train loss:  130.46961656212807\n",
      "train accuracy:  0.88686\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  40.968283891677856\n",
      "test accuracy:  0.8349\n",
      "\n",
      "Epoch: 41\n",
      "train loss:  135.82418631017208\n",
      "train accuracy:  0.88212\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  43.09888792037964\n",
      "test accuracy:  0.8224\n",
      "\n",
      "Epoch: 42\n",
      "train loss:  131.3018907904625\n",
      "train accuracy:  0.8863\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  40.08290043473244\n",
      "test accuracy:  0.8286\n",
      "\n",
      "Epoch: 43\n",
      "train loss:  130.1330772191286\n",
      "train accuracy:  0.8865\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  49.13764998316765\n",
      "test accuracy:  0.8061\n",
      "\n",
      "Epoch: 44\n",
      "train loss:  130.8436713963747\n",
      "train accuracy:  0.886\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  34.32694208621979\n",
      "test accuracy:  0.8571\n",
      "\n",
      "Epoch: 45\n",
      "train loss:  129.93266320228577\n",
      "train accuracy:  0.88732\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  45.229412376880646\n",
      "test accuracy:  0.8103\n",
      "\n",
      "Epoch: 46\n",
      "train loss:  129.0844328701496\n",
      "train accuracy:  0.8864\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  38.058562591671944\n",
      "test accuracy:  0.8445\n",
      "\n",
      "Epoch: 47\n",
      "train loss:  129.09183740615845\n",
      "train accuracy:  0.88742\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  44.743534207344055\n",
      "test accuracy:  0.8233\n",
      "\n",
      "Epoch: 48\n",
      "train loss:  126.61730471253395\n",
      "train accuracy:  0.89024\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  47.30108866095543\n",
      "test accuracy:  0.8138\n",
      "\n",
      "Epoch: 49\n",
      "train loss:  129.40179263055325\n",
      "train accuracy:  0.88776\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  35.689236000180244\n",
      "test accuracy:  0.8549\n",
      "\n",
      "Epoch: 50\n",
      "train loss:  128.40078946948051\n",
      "train accuracy:  0.88714\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  34.13819809257984\n",
      "test accuracy:  0.8544\n",
      "\n",
      "Epoch: 51\n",
      "train loss:  128.55023358762264\n",
      "train accuracy:  0.8883\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  39.56062787771225\n",
      "test accuracy:  0.8426\n",
      "\n",
      "Epoch: 52\n",
      "train loss:  129.28291402757168\n",
      "train accuracy:  0.88804\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  41.26557415723801\n",
      "test accuracy:  0.8313\n",
      "\n",
      "Epoch: 53\n",
      "train loss:  128.0588103532791\n",
      "train accuracy:  0.88908\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  43.67113682627678\n",
      "test accuracy:  0.8236\n",
      "\n",
      "Epoch: 54\n",
      "train loss:  125.51014702022076\n",
      "train accuracy:  0.89082\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  33.68338191509247\n",
      "test accuracy:  0.8545\n",
      "\n",
      "Epoch: 55\n",
      "train loss:  126.34681706130505\n",
      "train accuracy:  0.88976\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  38.31580209732056\n",
      "test accuracy:  0.8419\n",
      "\n",
      "Epoch: 56\n",
      "train loss:  125.7595289349556\n",
      "train accuracy:  0.89168\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  45.115991294384\n",
      "test accuracy:  0.813\n",
      "\n",
      "Epoch: 57\n",
      "train loss:  127.52363722026348\n",
      "train accuracy:  0.88882\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  42.10357782244682\n",
      "test accuracy:  0.8248\n",
      "\n",
      "Epoch: 58\n",
      "train loss:  125.7120059132576\n",
      "train accuracy:  0.89096\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  46.74441596865654\n",
      "test accuracy:  0.8151\n",
      "\n",
      "Epoch: 59\n",
      "train loss:  127.77247519791126\n",
      "train accuracy:  0.88984\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  38.64498269557953\n",
      "test accuracy:  0.8426\n",
      "\n",
      "Epoch: 60\n",
      "train loss:  124.45226323604584\n",
      "train accuracy:  0.89198\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  39.392548114061356\n",
      "test accuracy:  0.8387\n",
      "\n",
      "Epoch: 61\n",
      "train loss:  125.59126272797585\n",
      "train accuracy:  0.8907\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  32.47063934803009\n",
      "test accuracy:  0.8686\n",
      "\n",
      "Epoch: 62\n",
      "train loss:  125.90072256326675\n",
      "train accuracy:  0.89018\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  43.99706721305847\n",
      "test accuracy:  0.8185\n",
      "\n",
      "Epoch: 63\n",
      "train loss:  125.19762627780437\n",
      "train accuracy:  0.89066\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  37.37919861078262\n",
      "test accuracy:  0.844\n",
      "\n",
      "Epoch: 64\n",
      "train loss:  125.96083442866802\n",
      "train accuracy:  0.89044\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  30.64273777604103\n",
      "test accuracy:  0.8708\n",
      "\n",
      "Epoch: 65\n",
      "train loss:  126.20467455685139\n",
      "train accuracy:  0.89048\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  35.959047973155975\n",
      "test accuracy:  0.8469\n",
      "\n",
      "Epoch: 66\n",
      "train loss:  123.76024435460567\n",
      "train accuracy:  0.89338\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  41.27850419282913\n",
      "test accuracy:  0.8357\n",
      "\n",
      "Epoch: 67\n",
      "train loss:  123.94511878490448\n",
      "train accuracy:  0.89196\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  56.05960103869438\n",
      "test accuracy:  0.7863\n",
      "\n",
      "Epoch: 68\n",
      "train loss:  125.56964348256588\n",
      "train accuracy:  0.89008\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  35.01720058917999\n",
      "test accuracy:  0.8539\n",
      "\n",
      "Epoch: 69\n",
      "train loss:  123.3416563719511\n",
      "train accuracy:  0.89284\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  36.56856954097748\n",
      "test accuracy:  0.8383\n",
      "\n",
      "Epoch: 70\n",
      "train loss:  123.20300368964672\n",
      "train accuracy:  0.8928\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  43.83230763673782\n",
      "test accuracy:  0.8334\n",
      "\n",
      "Epoch: 71\n",
      "train loss:  125.02663416415453\n",
      "train accuracy:  0.89016\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  40.62821885943413\n",
      "test accuracy:  0.8325\n",
      "\n",
      "Epoch: 72\n",
      "train loss:  123.82918241620064\n",
      "train accuracy:  0.89312\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  35.473335951566696\n",
      "test accuracy:  0.8445\n",
      "\n",
      "Epoch: 73\n",
      "train loss:  123.05767947435379\n",
      "train accuracy:  0.89386\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  51.53806033730507\n",
      "test accuracy:  0.8042\n",
      "\n",
      "Epoch: 74\n",
      "train loss:  124.45881925523281\n",
      "train accuracy:  0.89266\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  32.9300282895565\n",
      "test accuracy:  0.8571\n",
      "\n",
      "Epoch: 75\n",
      "train loss:  121.84680977463722\n",
      "train accuracy:  0.89498\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  31.890491262078285\n",
      "test accuracy:  0.868\n",
      "\n",
      "Epoch: 76\n",
      "train loss:  121.34215394407511\n",
      "train accuracy:  0.89492\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  36.555488616228104\n",
      "test accuracy:  0.8491\n",
      "\n",
      "Epoch: 77\n",
      "train loss:  124.32656449079514\n",
      "train accuracy:  0.89188\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  37.01793244481087\n",
      "test accuracy:  0.8431\n",
      "\n",
      "Epoch: 78\n",
      "train loss:  123.377555757761\n",
      "train accuracy:  0.8918\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  35.10664168000221\n",
      "test accuracy:  0.8546\n",
      "\n",
      "Epoch: 79\n",
      "train loss:  123.32625532150269\n",
      "train accuracy:  0.89284\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  40.586500614881516\n",
      "test accuracy:  0.8329\n",
      "\n",
      "Epoch: 80\n",
      "train loss:  123.50754772126675\n",
      "train accuracy:  0.89278\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  34.117792159318924\n",
      "test accuracy:  0.8556\n",
      "\n",
      "Epoch: 81\n",
      "train loss:  124.8787127584219\n",
      "train accuracy:  0.88956\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  41.20957365632057\n",
      "test accuracy:  0.8277\n",
      "\n",
      "Epoch: 82\n",
      "train loss:  122.90561197698116\n",
      "train accuracy:  0.89204\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  31.41836841404438\n",
      "test accuracy:  0.8653\n",
      "\n",
      "Epoch: 83\n",
      "train loss:  122.96219101548195\n",
      "train accuracy:  0.895\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  60.00528208911419\n",
      "test accuracy:  0.7797\n",
      "\n",
      "Epoch: 84\n",
      "train loss:  122.0858574360609\n",
      "train accuracy:  0.8944\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  32.588352501392365\n",
      "test accuracy:  0.8636\n",
      "\n",
      "Epoch: 85\n",
      "train loss:  122.35585448145866\n",
      "train accuracy:  0.89312\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  36.52269667387009\n",
      "test accuracy:  0.8459\n",
      "\n",
      "Epoch: 86\n",
      "train loss:  121.53147161006927\n",
      "train accuracy:  0.89566\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  39.48566557466984\n",
      "test accuracy:  0.8422\n",
      "\n",
      "Epoch: 87\n",
      "train loss:  121.92821857333183\n",
      "train accuracy:  0.89448\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  30.239537075161934\n",
      "test accuracy:  0.873\n",
      "\n",
      "Epoch: 88\n",
      "train loss:  120.6724538654089\n",
      "train accuracy:  0.8962\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  34.99002377688885\n",
      "test accuracy:  0.853\n",
      "\n",
      "Epoch: 89\n",
      "train loss:  122.08653259277344\n",
      "train accuracy:  0.89488\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  38.423144683241844\n",
      "test accuracy:  0.8449\n",
      "\n",
      "Epoch: 90\n",
      "train loss:  124.4649215489626\n",
      "train accuracy:  0.89296\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  34.10139709711075\n",
      "test accuracy:  0.8545\n",
      "\n",
      "Epoch: 91\n",
      "train loss:  122.59224581718445\n",
      "train accuracy:  0.89296\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  33.311301693320274\n",
      "test accuracy:  0.8593\n",
      "\n",
      "Epoch: 92\n",
      "train loss:  121.5404092669487\n",
      "train accuracy:  0.89424\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  37.98545742034912\n",
      "test accuracy:  0.8481\n",
      "\n",
      "Epoch: 93\n",
      "train loss:  121.17199827730656\n",
      "train accuracy:  0.89592\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  44.41413041949272\n",
      "test accuracy:  0.826\n",
      "\n",
      "Epoch: 94\n",
      "train loss:  122.08380080759525\n",
      "train accuracy:  0.89422\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  34.55571573972702\n",
      "test accuracy:  0.8531\n",
      "\n",
      "Epoch: 95\n",
      "train loss:  121.88795667886734\n",
      "train accuracy:  0.89542\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  39.3612824678421\n",
      "test accuracy:  0.8422\n",
      "\n",
      "Epoch: 96\n",
      "train loss:  122.20713892579079\n",
      "train accuracy:  0.89412\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  40.09773635864258\n",
      "test accuracy:  0.833\n",
      "\n",
      "Epoch: 97\n",
      "train loss:  120.82792742550373\n",
      "train accuracy:  0.89372\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  35.29389515519142\n",
      "test accuracy:  0.8521\n",
      "\n",
      "Epoch: 98\n",
      "train loss:  122.62900297343731\n",
      "train accuracy:  0.8935\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  40.32561373710632\n",
      "test accuracy:  0.8361\n",
      "\n",
      "Epoch: 99\n",
      "train loss:  122.49335362017155\n",
      "train accuracy:  0.89298\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  61.7334361076355\n",
      "test accuracy:  0.7739\n",
      "current learning rate:  0.01\n",
      "\n",
      "Epoch: 100\n",
      "train loss:  66.39613330364227\n",
      "train accuracy:  0.94336\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  16.767706099897623\n",
      "test accuracy:  0.9286\n",
      "\n",
      "Epoch: 101\n",
      "train loss:  45.67028550803661\n",
      "train accuracy:  0.96096\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.523209258913994\n",
      "test accuracy:  0.937\n",
      "\n",
      "Epoch: 102\n",
      "train loss:  37.35347820259631\n",
      "train accuracy:  0.9689\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.072382349520922\n",
      "test accuracy:  0.9376\n",
      "\n",
      "Epoch: 103\n",
      "train loss:  32.16447488591075\n",
      "train accuracy:  0.97248\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.161146376281977\n",
      "test accuracy:  0.9371\n",
      "\n",
      "Epoch: 104\n",
      "train loss:  29.654664343222976\n",
      "train accuracy:  0.97526\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.101696774363518\n",
      "test accuracy:  0.9393\n",
      "\n",
      "Epoch: 105\n",
      "train loss:  25.574491364881396\n",
      "train accuracy:  0.9782\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  14.955663822591305\n",
      "test accuracy:  0.9422\n",
      "\n",
      "Epoch: 106\n",
      "train loss:  22.286871498450637\n",
      "train accuracy:  0.98114\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.465611312538385\n",
      "test accuracy:  0.94\n",
      "\n",
      "Epoch: 107\n",
      "train loss:  20.753661155700684\n",
      "train accuracy:  0.98314\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  16.050706814974546\n",
      "test accuracy:  0.9405\n",
      "\n",
      "Epoch: 108\n",
      "train loss:  19.22425278648734\n",
      "train accuracy:  0.98358\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  16.422849263995886\n",
      "test accuracy:  0.9382\n",
      "\n",
      "Epoch: 109\n",
      "train loss:  17.365010015666485\n",
      "train accuracy:  0.9858\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  16.082497771829367\n",
      "test accuracy:  0.9396\n",
      "\n",
      "Epoch: 110\n",
      "train loss:  14.989303315756842\n",
      "train accuracy:  0.98766\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  16.440493308007717\n",
      "test accuracy:  0.9382\n",
      "\n",
      "Epoch: 111\n",
      "train loss:  14.387911669909954\n",
      "train accuracy:  0.98852\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  16.694645207375288\n",
      "test accuracy:  0.9403\n",
      "\n",
      "Epoch: 112\n",
      "train loss:  14.674033211544156\n",
      "train accuracy:  0.9878\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  16.33341544494033\n",
      "test accuracy:  0.9391\n",
      "\n",
      "Epoch: 113\n",
      "train loss:  13.091193426400423\n",
      "train accuracy:  0.98878\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  17.34790463745594\n",
      "test accuracy:  0.937\n",
      "\n",
      "Epoch: 114\n",
      "train loss:  12.482844000682235\n",
      "train accuracy:  0.98946\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  16.5622566267848\n",
      "test accuracy:  0.9406\n",
      "\n",
      "Epoch: 115\n",
      "train loss:  12.456603622063994\n",
      "train accuracy:  0.99012\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  16.51494330354035\n",
      "test accuracy:  0.9428\n",
      "\n",
      "Epoch: 116\n",
      "train loss:  11.322378072887659\n",
      "train accuracy:  0.99074\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  18.955997474491596\n",
      "test accuracy:  0.9347\n",
      "\n",
      "Epoch: 117\n",
      "train loss:  12.735555151477456\n",
      "train accuracy:  0.98942\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  19.594983752816916\n",
      "test accuracy:  0.9321\n",
      "\n",
      "Epoch: 118\n",
      "train loss:  11.942582977935672\n",
      "train accuracy:  0.99006\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  18.061766851693392\n",
      "test accuracy:  0.9384\n",
      "\n",
      "Epoch: 119\n",
      "train loss:  11.169333131983876\n",
      "train accuracy:  0.9915\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  17.555693432688713\n",
      "test accuracy:  0.9398\n",
      "\n",
      "Epoch: 120\n",
      "train loss:  10.17173426784575\n",
      "train accuracy:  0.9918\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  18.84408687800169\n",
      "test accuracy:  0.9346\n",
      "\n",
      "Epoch: 121\n",
      "train loss:  10.867040617391467\n",
      "train accuracy:  0.9912\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  18.195425549522042\n",
      "test accuracy:  0.9369\n",
      "\n",
      "Epoch: 122\n",
      "train loss:  11.229990359395742\n",
      "train accuracy:  0.99108\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  18.335826823487878\n",
      "test accuracy:  0.9383\n",
      "\n",
      "Epoch: 123\n",
      "train loss:  12.88961859792471\n",
      "train accuracy:  0.989\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  17.295239757746458\n",
      "test accuracy:  0.9388\n",
      "\n",
      "Epoch: 124\n",
      "train loss:  10.929857495240867\n",
      "train accuracy:  0.99156\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  17.901841763406992\n",
      "test accuracy:  0.9384\n",
      "\n",
      "Epoch: 125\n",
      "train loss:  13.380953254178166\n",
      "train accuracy:  0.98904\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  18.4537762850523\n",
      "test accuracy:  0.9349\n",
      "\n",
      "Epoch: 126\n",
      "train loss:  13.347205340862274\n",
      "train accuracy:  0.98894\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  19.314103044569492\n",
      "test accuracy:  0.9303\n",
      "\n",
      "Epoch: 127\n",
      "train loss:  13.532526221126318\n",
      "train accuracy:  0.98942\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  21.63170713186264\n",
      "test accuracy:  0.928\n",
      "\n",
      "Epoch: 128\n",
      "train loss:  13.52905555255711\n",
      "train accuracy:  0.9886\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  19.679073682054877\n",
      "test accuracy:  0.9312\n",
      "\n",
      "Epoch: 129\n",
      "train loss:  13.545726235955954\n",
      "train accuracy:  0.9888\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  18.831919241696596\n",
      "test accuracy:  0.9354\n",
      "\n",
      "Epoch: 130\n",
      "train loss:  13.730150444433093\n",
      "train accuracy:  0.98858\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  18.236595034599304\n",
      "test accuracy:  0.935\n",
      "\n",
      "Epoch: 131\n",
      "train loss:  12.85647538304329\n",
      "train accuracy:  0.9893\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  18.878124583512545\n",
      "test accuracy:  0.9323\n",
      "\n",
      "Epoch: 132\n",
      "train loss:  15.049981394782662\n",
      "train accuracy:  0.98762\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  19.08083748817444\n",
      "test accuracy:  0.9327\n",
      "\n",
      "Epoch: 133\n",
      "train loss:  15.003418968990445\n",
      "train accuracy:  0.9877\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  18.498359866440296\n",
      "test accuracy:  0.9344\n",
      "\n",
      "Epoch: 134\n",
      "train loss:  12.811658151447773\n",
      "train accuracy:  0.98964\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  22.703773200511932\n",
      "test accuracy:  0.9234\n",
      "\n",
      "Epoch: 135\n",
      "train loss:  15.526876515708864\n",
      "train accuracy:  0.98692\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  22.287795208394527\n",
      "test accuracy:  0.9229\n",
      "\n",
      "Epoch: 136\n",
      "train loss:  16.47803338803351\n",
      "train accuracy:  0.98592\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  20.20143112540245\n",
      "test accuracy:  0.9297\n",
      "\n",
      "Epoch: 137\n",
      "train loss:  15.064475597813725\n",
      "train accuracy:  0.98798\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  19.963835567235947\n",
      "test accuracy:  0.9304\n",
      "\n",
      "Epoch: 138\n",
      "train loss:  16.87589494138956\n",
      "train accuracy:  0.9855\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  20.784238137304783\n",
      "test accuracy:  0.9254\n",
      "\n",
      "Epoch: 139\n",
      "train loss:  16.882707135751843\n",
      "train accuracy:  0.98592\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  20.813866294920444\n",
      "test accuracy:  0.9253\n",
      "\n",
      "Epoch: 140\n",
      "train loss:  16.540887402370572\n",
      "train accuracy:  0.98612\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  20.090889144688845\n",
      "test accuracy:  0.9294\n",
      "\n",
      "Epoch: 141\n",
      "train loss:  17.08864925056696\n",
      "train accuracy:  0.98578\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  22.315816789865494\n",
      "test accuracy:  0.9234\n",
      "\n",
      "Epoch: 142\n",
      "train loss:  17.514839159324765\n",
      "train accuracy:  0.985\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  21.269278831779957\n",
      "test accuracy:  0.9234\n",
      "\n",
      "Epoch: 143\n",
      "train loss:  16.7257383428514\n",
      "train accuracy:  0.98594\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  23.372163772583008\n",
      "test accuracy:  0.9178\n",
      "\n",
      "Epoch: 144\n",
      "train loss:  17.352841082960367\n",
      "train accuracy:  0.98556\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  21.143735218793154\n",
      "test accuracy:  0.9259\n",
      "\n",
      "Epoch: 145\n",
      "train loss:  17.120632462203503\n",
      "train accuracy:  0.9854\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  21.442637592554092\n",
      "test accuracy:  0.9248\n",
      "\n",
      "Epoch: 146\n",
      "train loss:  17.31250584870577\n",
      "train accuracy:  0.98616\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  19.74347485229373\n",
      "test accuracy:  0.9329\n",
      "\n",
      "Epoch: 147\n",
      "train loss:  17.630178233608603\n",
      "train accuracy:  0.98566\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  20.270843237638474\n",
      "test accuracy:  0.9263\n",
      "\n",
      "Epoch: 148\n",
      "train loss:  16.49289245903492\n",
      "train accuracy:  0.98622\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  20.998866010457277\n",
      "test accuracy:  0.9298\n",
      "\n",
      "Epoch: 149\n",
      "train loss:  17.34373877197504\n",
      "train accuracy:  0.9855\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  22.933839440345764\n",
      "test accuracy:  0.9207\n",
      "current learning rate:  0.001\n",
      "\n",
      "Epoch: 150\n",
      "train loss:  8.636022572405636\n",
      "train accuracy:  0.99344\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  16.339120537042618\n",
      "test accuracy:  0.9442\n",
      "\n",
      "Epoch: 151\n",
      "train loss:  5.235173522494733\n",
      "train accuracy:  0.9969\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.984362717717886\n",
      "test accuracy:  0.9459\n",
      "\n",
      "Epoch: 152\n",
      "train loss:  3.8846122233662754\n",
      "train accuracy:  0.99804\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.769236352294683\n",
      "test accuracy:  0.9469\n",
      "\n",
      "Epoch: 153\n",
      "train loss:  3.3964273650199175\n",
      "train accuracy:  0.99826\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.809267349541187\n",
      "test accuracy:  0.9466\n",
      "\n",
      "Epoch: 154\n",
      "train loss:  2.994706624187529\n",
      "train accuracy:  0.9983\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.80453421920538\n",
      "test accuracy:  0.9465\n",
      "\n",
      "Epoch: 155\n",
      "train loss:  2.7786742988973856\n",
      "train accuracy:  0.9985\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.747978614643216\n",
      "test accuracy:  0.9479\n",
      "\n",
      "Epoch: 156\n",
      "train loss:  2.5336654046550393\n",
      "train accuracy:  0.99872\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.89111702889204\n",
      "test accuracy:  0.9489\n",
      "\n",
      "Epoch: 157\n",
      "train loss:  2.329383977688849\n",
      "train accuracy:  0.99898\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.591088980436325\n",
      "test accuracy:  0.9485\n",
      "\n",
      "Epoch: 158\n",
      "train loss:  1.9609612943604589\n",
      "train accuracy:  0.99928\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.646157456561923\n",
      "test accuracy:  0.9482\n",
      "\n",
      "Epoch: 159\n",
      "train loss:  1.9393472732044756\n",
      "train accuracy:  0.99922\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.624318275600672\n",
      "test accuracy:  0.9488\n",
      "\n",
      "Epoch: 160\n",
      "train loss:  1.8300698153907433\n",
      "train accuracy:  0.99926\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.522314505651593\n",
      "test accuracy:  0.9502\n",
      "\n",
      "Epoch: 161\n",
      "train loss:  1.7369658679235727\n",
      "train accuracy:  0.99942\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.618867367506027\n",
      "test accuracy:  0.9505\n",
      "\n",
      "Epoch: 162\n",
      "train loss:  1.6093602613545954\n",
      "train accuracy:  0.99964\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.573331970721483\n",
      "test accuracy:  0.9497\n",
      "\n",
      "Epoch: 163\n",
      "train loss:  1.5005869145970792\n",
      "train accuracy:  0.9995\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.585350789129734\n",
      "test accuracy:  0.9486\n",
      "\n",
      "Epoch: 164\n",
      "train loss:  1.593872619792819\n",
      "train accuracy:  0.99942\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.606304679065943\n",
      "test accuracy:  0.9487\n",
      "\n",
      "Epoch: 165\n",
      "train loss:  1.5879587214440107\n",
      "train accuracy:  0.99938\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.490257455036044\n",
      "test accuracy:  0.9506\n",
      "\n",
      "Epoch: 166\n",
      "train loss:  1.457038113847375\n",
      "train accuracy:  0.99964\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.721052270382643\n",
      "test accuracy:  0.949\n",
      "\n",
      "Epoch: 167\n",
      "train loss:  1.2179931411519647\n",
      "train accuracy:  0.99976\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.386471254751086\n",
      "test accuracy:  0.9502\n",
      "\n",
      "Epoch: 168\n",
      "train loss:  1.3535570977255702\n",
      "train accuracy:  0.99968\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.462865652516484\n",
      "test accuracy:  0.9506\n",
      "\n",
      "Epoch: 169\n",
      "train loss:  1.330433847848326\n",
      "train accuracy:  0.99968\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.455211129039526\n",
      "test accuracy:  0.9496\n",
      "\n",
      "Epoch: 170\n",
      "train loss:  1.2714231787249446\n",
      "train accuracy:  0.99968\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.46971782296896\n",
      "test accuracy:  0.9507\n",
      "\n",
      "Epoch: 171\n",
      "train loss:  1.2841395461000502\n",
      "train accuracy:  0.99966\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.317947294563055\n",
      "test accuracy:  0.9503\n",
      "\n",
      "Epoch: 172\n",
      "train loss:  1.2699052831158042\n",
      "train accuracy:  0.99966\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.3701601177454\n",
      "test accuracy:  0.9502\n",
      "\n",
      "Epoch: 173\n",
      "train loss:  1.1828599270666018\n",
      "train accuracy:  0.99966\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.222714640200138\n",
      "test accuracy:  0.9501\n",
      "\n",
      "Epoch: 174\n",
      "train loss:  1.147503487765789\n",
      "train accuracy:  0.99978\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.489087861031294\n",
      "test accuracy:  0.9501\n",
      "\n",
      "Epoch: 175\n",
      "train loss:  1.0777389432769269\n",
      "train accuracy:  0.99984\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.203954134136438\n",
      "test accuracy:  0.9508\n",
      "\n",
      "Epoch: 176\n",
      "train loss:  1.1111783739179373\n",
      "train accuracy:  0.99968\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.275537975132465\n",
      "test accuracy:  0.9495\n",
      "\n",
      "Epoch: 177\n",
      "train loss:  1.1519240352790803\n",
      "train accuracy:  0.99978\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.298295874148607\n",
      "test accuracy:  0.9499\n",
      "\n",
      "Epoch: 178\n",
      "train loss:  1.057872922741808\n",
      "train accuracy:  0.99978\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.372676432132721\n",
      "test accuracy:  0.9505\n",
      "\n",
      "Epoch: 179\n",
      "train loss:  1.1236812446732074\n",
      "train accuracy:  0.9997\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.235905228182673\n",
      "test accuracy:  0.9503\n",
      "\n",
      "Epoch: 180\n",
      "train loss:  1.1583253651624545\n",
      "train accuracy:  0.99956\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.299770284444094\n",
      "test accuracy:  0.9517\n",
      "\n",
      "Epoch: 181\n",
      "train loss:  1.0644086111569777\n",
      "train accuracy:  0.99976\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.458507964387536\n",
      "test accuracy:  0.9497\n",
      "\n",
      "Epoch: 182\n",
      "train loss:  0.9388992905151099\n",
      "train accuracy:  0.9999\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.384122371673584\n",
      "test accuracy:  0.9505\n",
      "\n",
      "Epoch: 183\n",
      "train loss:  1.0289553866023198\n",
      "train accuracy:  0.99978\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.15849014185369\n",
      "test accuracy:  0.9502\n",
      "\n",
      "Epoch: 184\n",
      "train loss:  1.031205628067255\n",
      "train accuracy:  0.99972\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.114386226981878\n",
      "test accuracy:  0.9501\n",
      "\n",
      "Epoch: 185\n",
      "train loss:  0.9361818197648972\n",
      "train accuracy:  0.9999\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.382950050756335\n",
      "test accuracy:  0.9496\n",
      "\n",
      "Epoch: 186\n",
      "train loss:  1.024460696382448\n",
      "train accuracy:  0.99974\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.218942938372493\n",
      "test accuracy:  0.9496\n",
      "\n",
      "Epoch: 187\n",
      "train loss:  1.017115727532655\n",
      "train accuracy:  0.99976\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.069010380655527\n",
      "test accuracy:  0.9517\n",
      "\n",
      "Epoch: 188\n",
      "train loss:  1.0165825735311955\n",
      "train accuracy:  0.99984\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  14.907794652506709\n",
      "test accuracy:  0.9519\n",
      "\n",
      "Epoch: 189\n",
      "train loss:  0.9771692915819585\n",
      "train accuracy:  0.99982\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.0648725181818\n",
      "test accuracy:  0.9503\n",
      "\n",
      "Epoch: 190\n",
      "train loss:  0.8888269863091409\n",
      "train accuracy:  0.99984\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.227352829650044\n",
      "test accuracy:  0.9498\n",
      "\n",
      "Epoch: 191\n",
      "train loss:  0.9624035758897662\n",
      "train accuracy:  0.99986\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.17323330976069\n",
      "test accuracy:  0.951\n",
      "\n",
      "Epoch: 192\n",
      "train loss:  0.9856910102535039\n",
      "train accuracy:  0.9998\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.246055249124765\n",
      "test accuracy:  0.9505\n",
      "\n",
      "Epoch: 193\n",
      "train loss:  0.9304495635442436\n",
      "train accuracy:  0.9999\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.074654497206211\n",
      "test accuracy:  0.9518\n",
      "\n",
      "Epoch: 194\n",
      "train loss:  0.904881423804909\n",
      "train accuracy:  0.99986\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.099167078733444\n",
      "test accuracy:  0.9521\n",
      "\n",
      "Epoch: 195\n",
      "train loss:  0.8987862772773951\n",
      "train accuracy:  0.99986\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.033713150769472\n",
      "test accuracy:  0.9509\n",
      "\n",
      "Epoch: 196\n",
      "train loss:  0.8817692615557462\n",
      "train accuracy:  0.99986\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  14.904804438352585\n",
      "test accuracy:  0.9521\n",
      "\n",
      "Epoch: 197\n",
      "train loss:  0.8422361873090267\n",
      "train accuracy:  0.99988\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.142486630007625\n",
      "test accuracy:  0.9502\n",
      "\n",
      "Epoch: 198\n",
      "train loss:  0.9006566650932655\n",
      "train accuracy:  0.99986\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.228581195697188\n",
      "test accuracy:  0.9511\n",
      "\n",
      "Epoch: 199\n",
      "train loss:  0.9034080009441823\n",
      "train accuracy:  0.99984\n",
      "saving model at: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.137750057503581\n",
      "test accuracy:  0.9518\n"
     ]
    }
   ],
   "source": [
    "train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: weights/cifar10_resnet18_SGD.pth\n",
      "test loss:  15.137750057503581\n",
      "test accuracy:  0.9518\n"
     ]
    }
   ],
   "source": [
    "##### if you already have a trained model ##############\n",
    "CIFAR10_PATH = 'weights/cifar10_resnet18_SGD.pth'\n",
    "CIFAR10_model = ResNet18().to(device)\n",
    "print(\"loading model from: {}\".format(CIFAR10_PATH))\n",
    "CIFAR10_model.load_state_dict(torch.load(CIFAR10_PATH))#, map_location=torch.device('cpu')))\n",
    "#test the model\n",
    "test(CIFAR10_model, 0, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diag_second_order(model, train_loader, var0 = 10, device='cpu'):\n",
    "\n",
    "    W = list(model.parameters())[-2]\n",
    "    b = list(model.parameters())[-1]\n",
    "    m, n = W.shape\n",
    "    print(\"n: {} inputs to linear layer with m: {} classes\".format(n, m))\n",
    "    lossfunc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    tau = 1/var0\n",
    "\n",
    "    extend(lossfunc, debug=False)\n",
    "    extend(model.fc, debug=False)\n",
    "\n",
    "    with backpack(DiagHessian()):\n",
    "\n",
    "        max_len = len(train_loader)\n",
    "        weights_cov = torch.zeros(max_len, m, n, device=device)\n",
    "        biases_cov = torch.zeros(max_len, m, device=device)\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "\n",
    "            if device == 'cuda':\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            lossfunc(model(x), y).backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Hessian of weight\n",
    "                W_ = W.diag_h\n",
    "                b_ = b.diag_h\n",
    "\n",
    "                #add_prior: since it will be flattened later we can just add the prior like that\n",
    "                W_ += tau * torch.ones(W_.size(), device=device)\n",
    "                b_ += tau * torch.ones(b_.size(), device=device)\n",
    "\n",
    "\n",
    "            weights_cov[batch_idx] = W_\n",
    "            biases_cov[batch_idx] = b_\n",
    "\n",
    "            print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "\n",
    "        print(len(weights_cov))\n",
    "        C_W = torch.mean(weights_cov, dim=0)\n",
    "        C_b = torch.mean(biases_cov, dim=0)\n",
    "\n",
    "    # Predictive distribution\n",
    "    with torch.no_grad():\n",
    "        M_W_post = W.t()\n",
    "        M_b_post = b\n",
    "\n",
    "        C_W_post = C_W\n",
    "        C_b_post = C_b\n",
    "        \n",
    "    print(\"M_W_post size: \", M_W_post.size())\n",
    "    print(\"M_b_post size: \", M_b_post.size())\n",
    "    print(\"C_W_post size: \", C_W_post.size())\n",
    "    print(\"C_b_post size: \", C_b_post.size())\n",
    "\n",
    "    return(M_W_post, M_b_post, C_W_post, C_b_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 512 inputs to linear layer with m: 10 classes\n",
      "Batch: 0/391\n",
      "Batch: 1/391\n",
      "Batch: 2/391\n",
      "Batch: 3/391\n",
      "Batch: 4/391\n",
      "Batch: 5/391\n",
      "Batch: 6/391\n",
      "Batch: 7/391\n",
      "Batch: 8/391\n",
      "Batch: 9/391\n",
      "Batch: 10/391\n",
      "Batch: 11/391\n",
      "Batch: 12/391\n",
      "Batch: 13/391\n",
      "Batch: 14/391\n",
      "Batch: 15/391\n",
      "Batch: 16/391\n",
      "Batch: 17/391\n",
      "Batch: 18/391\n",
      "Batch: 19/391\n",
      "Batch: 20/391\n",
      "Batch: 21/391\n",
      "Batch: 22/391\n",
      "Batch: 23/391\n",
      "Batch: 24/391\n",
      "Batch: 25/391\n",
      "Batch: 26/391\n",
      "Batch: 27/391\n",
      "Batch: 28/391\n",
      "Batch: 29/391\n",
      "Batch: 30/391\n",
      "Batch: 31/391\n",
      "Batch: 32/391\n",
      "Batch: 33/391\n",
      "Batch: 34/391\n",
      "Batch: 35/391\n",
      "Batch: 36/391\n",
      "Batch: 37/391\n",
      "Batch: 38/391\n",
      "Batch: 39/391\n",
      "Batch: 40/391\n",
      "Batch: 41/391\n",
      "Batch: 42/391\n",
      "Batch: 43/391\n",
      "Batch: 44/391\n",
      "Batch: 45/391\n",
      "Batch: 46/391\n",
      "Batch: 47/391\n",
      "Batch: 48/391\n",
      "Batch: 49/391\n",
      "Batch: 50/391\n",
      "Batch: 51/391\n",
      "Batch: 52/391\n",
      "Batch: 53/391\n",
      "Batch: 54/391\n",
      "Batch: 55/391\n",
      "Batch: 56/391\n",
      "Batch: 57/391\n",
      "Batch: 58/391\n",
      "Batch: 59/391\n",
      "Batch: 60/391\n",
      "Batch: 61/391\n",
      "Batch: 62/391\n",
      "Batch: 63/391\n",
      "Batch: 64/391\n",
      "Batch: 65/391\n",
      "Batch: 66/391\n",
      "Batch: 67/391\n",
      "Batch: 68/391\n",
      "Batch: 69/391\n",
      "Batch: 70/391\n",
      "Batch: 71/391\n",
      "Batch: 72/391\n",
      "Batch: 73/391\n",
      "Batch: 74/391\n",
      "Batch: 75/391\n",
      "Batch: 76/391\n",
      "Batch: 77/391\n",
      "Batch: 78/391\n",
      "Batch: 79/391\n",
      "Batch: 80/391\n",
      "Batch: 81/391\n",
      "Batch: 82/391\n",
      "Batch: 83/391\n",
      "Batch: 84/391\n",
      "Batch: 85/391\n",
      "Batch: 86/391\n",
      "Batch: 87/391\n",
      "Batch: 88/391\n",
      "Batch: 89/391\n",
      "Batch: 90/391\n",
      "Batch: 91/391\n",
      "Batch: 92/391\n",
      "Batch: 93/391\n",
      "Batch: 94/391\n",
      "Batch: 95/391\n",
      "Batch: 96/391\n",
      "Batch: 97/391\n",
      "Batch: 98/391\n",
      "Batch: 99/391\n",
      "Batch: 100/391\n",
      "Batch: 101/391\n",
      "Batch: 102/391\n",
      "Batch: 103/391\n",
      "Batch: 104/391\n",
      "Batch: 105/391\n",
      "Batch: 106/391\n",
      "Batch: 107/391\n",
      "Batch: 108/391\n",
      "Batch: 109/391\n",
      "Batch: 110/391\n",
      "Batch: 111/391\n",
      "Batch: 112/391\n",
      "Batch: 113/391\n",
      "Batch: 114/391\n",
      "Batch: 115/391\n",
      "Batch: 116/391\n",
      "Batch: 117/391\n",
      "Batch: 118/391\n",
      "Batch: 119/391\n",
      "Batch: 120/391\n",
      "Batch: 121/391\n",
      "Batch: 122/391\n",
      "Batch: 123/391\n",
      "Batch: 124/391\n",
      "Batch: 125/391\n",
      "Batch: 126/391\n",
      "Batch: 127/391\n",
      "Batch: 128/391\n",
      "Batch: 129/391\n",
      "Batch: 130/391\n",
      "Batch: 131/391\n",
      "Batch: 132/391\n",
      "Batch: 133/391\n",
      "Batch: 134/391\n",
      "Batch: 135/391\n",
      "Batch: 136/391\n",
      "Batch: 137/391\n",
      "Batch: 138/391\n",
      "Batch: 139/391\n",
      "Batch: 140/391\n",
      "Batch: 141/391\n",
      "Batch: 142/391\n",
      "Batch: 143/391\n",
      "Batch: 144/391\n",
      "Batch: 145/391\n",
      "Batch: 146/391\n",
      "Batch: 147/391\n",
      "Batch: 148/391\n",
      "Batch: 149/391\n",
      "Batch: 150/391\n",
      "Batch: 151/391\n",
      "Batch: 152/391\n",
      "Batch: 153/391\n",
      "Batch: 154/391\n",
      "Batch: 155/391\n",
      "Batch: 156/391\n",
      "Batch: 157/391\n",
      "Batch: 158/391\n",
      "Batch: 159/391\n",
      "Batch: 160/391\n",
      "Batch: 161/391\n",
      "Batch: 162/391\n",
      "Batch: 163/391\n",
      "Batch: 164/391\n",
      "Batch: 165/391\n",
      "Batch: 166/391\n",
      "Batch: 167/391\n",
      "Batch: 168/391\n",
      "Batch: 169/391\n",
      "Batch: 170/391\n",
      "Batch: 171/391\n",
      "Batch: 172/391\n",
      "Batch: 173/391\n",
      "Batch: 174/391\n",
      "Batch: 175/391\n",
      "Batch: 176/391\n",
      "Batch: 177/391\n",
      "Batch: 178/391\n",
      "Batch: 179/391\n",
      "Batch: 180/391\n",
      "Batch: 181/391\n",
      "Batch: 182/391\n",
      "Batch: 183/391\n",
      "Batch: 184/391\n",
      "Batch: 185/391\n",
      "Batch: 186/391\n",
      "Batch: 187/391\n",
      "Batch: 188/391\n",
      "Batch: 189/391\n",
      "Batch: 190/391\n",
      "Batch: 191/391\n",
      "Batch: 192/391\n",
      "Batch: 193/391\n",
      "Batch: 194/391\n",
      "Batch: 195/391\n",
      "Batch: 196/391\n",
      "Batch: 197/391\n",
      "Batch: 198/391\n",
      "Batch: 199/391\n",
      "Batch: 200/391\n",
      "Batch: 201/391\n",
      "Batch: 202/391\n",
      "Batch: 203/391\n",
      "Batch: 204/391\n",
      "Batch: 205/391\n",
      "Batch: 206/391\n",
      "Batch: 207/391\n",
      "Batch: 208/391\n",
      "Batch: 209/391\n",
      "Batch: 210/391\n",
      "Batch: 211/391\n",
      "Batch: 212/391\n",
      "Batch: 213/391\n",
      "Batch: 214/391\n",
      "Batch: 215/391\n",
      "Batch: 216/391\n",
      "Batch: 217/391\n",
      "Batch: 218/391\n",
      "Batch: 219/391\n",
      "Batch: 220/391\n",
      "Batch: 221/391\n",
      "Batch: 222/391\n",
      "Batch: 223/391\n",
      "Batch: 224/391\n",
      "Batch: 225/391\n",
      "Batch: 226/391\n",
      "Batch: 227/391\n",
      "Batch: 228/391\n",
      "Batch: 229/391\n",
      "Batch: 230/391\n",
      "Batch: 231/391\n",
      "Batch: 232/391\n",
      "Batch: 233/391\n",
      "Batch: 234/391\n",
      "Batch: 235/391\n",
      "Batch: 236/391\n",
      "Batch: 237/391\n",
      "Batch: 238/391\n",
      "Batch: 239/391\n",
      "Batch: 240/391\n",
      "Batch: 241/391\n",
      "Batch: 242/391\n",
      "Batch: 243/391\n",
      "Batch: 244/391\n",
      "Batch: 245/391\n",
      "Batch: 246/391\n",
      "Batch: 247/391\n",
      "Batch: 248/391\n",
      "Batch: 249/391\n",
      "Batch: 250/391\n",
      "Batch: 251/391\n",
      "Batch: 252/391\n",
      "Batch: 253/391\n",
      "Batch: 254/391\n",
      "Batch: 255/391\n",
      "Batch: 256/391\n",
      "Batch: 257/391\n",
      "Batch: 258/391\n",
      "Batch: 259/391\n",
      "Batch: 260/391\n",
      "Batch: 261/391\n",
      "Batch: 262/391\n",
      "Batch: 263/391\n",
      "Batch: 264/391\n",
      "Batch: 265/391\n",
      "Batch: 266/391\n",
      "Batch: 267/391\n",
      "Batch: 268/391\n",
      "Batch: 269/391\n",
      "Batch: 270/391\n",
      "Batch: 271/391\n",
      "Batch: 272/391\n",
      "Batch: 273/391\n",
      "Batch: 274/391\n",
      "Batch: 275/391\n",
      "Batch: 276/391\n",
      "Batch: 277/391\n",
      "Batch: 278/391\n",
      "Batch: 279/391\n",
      "Batch: 280/391\n",
      "Batch: 281/391\n",
      "Batch: 282/391\n",
      "Batch: 283/391\n",
      "Batch: 284/391\n",
      "Batch: 285/391\n",
      "Batch: 286/391\n",
      "Batch: 287/391\n",
      "Batch: 288/391\n",
      "Batch: 289/391\n",
      "Batch: 290/391\n",
      "Batch: 291/391\n",
      "Batch: 292/391\n",
      "Batch: 293/391\n",
      "Batch: 294/391\n",
      "Batch: 295/391\n",
      "Batch: 296/391\n",
      "Batch: 297/391\n",
      "Batch: 298/391\n",
      "Batch: 299/391\n",
      "Batch: 300/391\n",
      "Batch: 301/391\n",
      "Batch: 302/391\n",
      "Batch: 303/391\n",
      "Batch: 304/391\n",
      "Batch: 305/391\n",
      "Batch: 306/391\n",
      "Batch: 307/391\n",
      "Batch: 308/391\n",
      "Batch: 309/391\n",
      "Batch: 310/391\n",
      "Batch: 311/391\n",
      "Batch: 312/391\n",
      "Batch: 313/391\n",
      "Batch: 314/391\n",
      "Batch: 315/391\n",
      "Batch: 316/391\n",
      "Batch: 317/391\n",
      "Batch: 318/391\n",
      "Batch: 319/391\n",
      "Batch: 320/391\n",
      "Batch: 321/391\n",
      "Batch: 322/391\n",
      "Batch: 323/391\n",
      "Batch: 324/391\n",
      "Batch: 325/391\n",
      "Batch: 326/391\n",
      "Batch: 327/391\n",
      "Batch: 328/391\n",
      "Batch: 329/391\n",
      "Batch: 330/391\n",
      "Batch: 331/391\n",
      "Batch: 332/391\n",
      "Batch: 333/391\n",
      "Batch: 334/391\n",
      "Batch: 335/391\n",
      "Batch: 336/391\n",
      "Batch: 337/391\n",
      "Batch: 338/391\n",
      "Batch: 339/391\n",
      "Batch: 340/391\n",
      "Batch: 341/391\n",
      "Batch: 342/391\n",
      "Batch: 343/391\n",
      "Batch: 344/391\n",
      "Batch: 345/391\n",
      "Batch: 346/391\n",
      "Batch: 347/391\n",
      "Batch: 348/391\n",
      "Batch: 349/391\n",
      "Batch: 350/391\n",
      "Batch: 351/391\n",
      "Batch: 352/391\n",
      "Batch: 353/391\n",
      "Batch: 354/391\n",
      "Batch: 355/391\n",
      "Batch: 356/391\n",
      "Batch: 357/391\n",
      "Batch: 358/391\n",
      "Batch: 359/391\n",
      "Batch: 360/391\n",
      "Batch: 361/391\n",
      "Batch: 362/391\n",
      "Batch: 363/391\n",
      "Batch: 364/391\n",
      "Batch: 365/391\n",
      "Batch: 366/391\n",
      "Batch: 367/391\n",
      "Batch: 368/391\n",
      "Batch: 369/391\n",
      "Batch: 370/391\n",
      "Batch: 371/391\n",
      "Batch: 372/391\n",
      "Batch: 373/391\n",
      "Batch: 374/391\n",
      "Batch: 375/391\n",
      "Batch: 376/391\n",
      "Batch: 377/391\n",
      "Batch: 378/391\n",
      "Batch: 379/391\n",
      "Batch: 380/391\n",
      "Batch: 381/391\n",
      "Batch: 382/391\n",
      "Batch: 383/391\n",
      "Batch: 384/391\n",
      "Batch: 385/391\n",
      "Batch: 386/391\n",
      "Batch: 387/391\n",
      "Batch: 388/391\n",
      "Batch: 389/391\n",
      "Batch: 390/391\n",
      "391\n",
      "M_W_post size:  torch.Size([512, 10])\n",
      "M_b_post size:  torch.Size([10])\n",
      "C_W_post size:  torch.Size([10, 512])\n",
      "C_b_post size:  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D = Diag_second_order(model=CIFAR10_model,\n",
    "                                                               train_loader=trainloader,\n",
    "                                                               var0 = 10,\n",
    "                                                               device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_dist_values(py_in, targets):\n",
    "    acc_in = np.mean(np.argmax(py_in, 1) == targets)\n",
    "    #prob_correct = np.choose(targets, py_in.T).mean()\n",
    "    prob_correct = py_in[targets].mean()\n",
    "    average_entropy = -np.sum(py_in*np.log(py_in+1e-8), axis=1).mean()\n",
    "    MMC = py_in.max(1).mean()\n",
    "    return(acc_in, prob_correct, average_entropy, MMC)\n",
    "\n",
    "def get_out_dist_values(py_in, py_out, targets):\n",
    "    average_entropy = -np.sum(py_out*np.log(py_out+1e-8), axis=1).mean()\n",
    "    acc_out = np.mean(np.argmax(py_out, 1) == targets)\n",
    "    if max(targets) > len(py_in[0]):\n",
    "        targets = np.array(targets)\n",
    "        targets[targets >= len(py_in[0])] = 0\n",
    "    #prob_correct = np.choose(targets, py_out.T).mean()\n",
    "    prob_correct = py_out[targets].mean()\n",
    "    labels = np.zeros(len(py_in)+len(py_out), dtype='int32')\n",
    "    labels[:len(py_in)] = 1\n",
    "    examples = np.concatenate([py_in.max(1), py_out.max(1)])\n",
    "    auroc = roc_auc_score(labels, examples)\n",
    "    MMC = py_out.max(1).mean()\n",
    "    return(acc_out, prob_correct, average_entropy, MMC, auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gaussian_output_old(x, mu_w, mu_b, sigma_w, sigma_b):\n",
    "    #get the distributions per class\n",
    "    batch_size = x.size(0)\n",
    "    num_classes = len(mu_b)\n",
    "    #print(\"batch_size, num_classes: \", batch_size, num_classes)\n",
    "    mu_batch = torch.zeros(batch_size, num_classes)\n",
    "    sigma_batch = torch.zeros(batch_size, num_classes, num_classes)\n",
    "    for i in range(batch_size):\n",
    "        per_class_sigmas = torch.zeros(num_classes)\n",
    "        for j in range(num_classes):\n",
    "            #create a diagonal Hessian\n",
    "            hess = torch.diag(sigma_w[j])\n",
    "            #b = x[i] @ hess @ x[i].t()\n",
    "            #a = sigma_b[i]\n",
    "            per_class_sigmas[j] = x[i] @ hess @ x[i].t() + sigma_b[j]\n",
    "\n",
    "        #print(\"sizes: \", mu_w.size(), x[i].size(), mu_b.size())\n",
    "        per_class_mus = x[i] @ mu_w + mu_b\n",
    "        mu_batch[i] = per_class_mus\n",
    "        sigma_batch[i] = torch.diag(per_class_sigmas)\n",
    "\n",
    "    return(mu_batch, sigma_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gaussian_output(x, mu_w, mu_b, sigma_w, sigma_b):\n",
    "    #get the distributions per class\n",
    "    batch_size = x.size(0)\n",
    "    num_classes = mu_b.size(0)\n",
    "    \n",
    "    # get mu batch\n",
    "    mu_w_batch = mu_w.repeat(batch_size, 1, 1)\n",
    "    mu_b_batch = mu_b.repeat(batch_size, 1)\n",
    "    mu_batch = torch.bmm(x.view(batch_size, 1, -1), mu_w_batch).view(batch_size, -1) + mu_b_batch\n",
    "    \n",
    "    #get sigma batch\n",
    "    sigma_w_batch = sigma_w.repeat(batch_size, 1, 1)\n",
    "    sigma_b_batch = sigma_b.repeat(batch_size, 1)\n",
    "    sigmas_diag = torch.zeros(batch_size, num_classes, device='cuda')\n",
    "    for j in range(num_classes):\n",
    "        h1 = x * sigma_w_batch[:, j]\n",
    "        helper = torch.matmul(h1.view(batch_size, 1, -1), x.view(batch_size, -1, 1))\n",
    "        helper = helper.view(-1) + sigma_b_batch[:,j]\n",
    "        sigmas_diag[:,j] = helper\n",
    "        \n",
    "    sigma_batch = torch.stack([torch.diag(x) for x in sigmas_diag])\n",
    "\n",
    "    \n",
    "    return(mu_batch, sigma_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_diagonal_sampling(model, test_loader, M_W_post, M_b_post, C_W_post, C_b_post, n_samples, verbose=False, cuda=False, timing=False):\n",
    "    py = []\n",
    "    max_len = len(test_loader)\n",
    "    if timing:\n",
    "        time_sum = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "\n",
    "        if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        phi = model.phi(x)\n",
    "\n",
    "        mu, Sigma = get_Gaussian_output(phi, M_W_post, M_b_post, C_W_post, C_b_post)\n",
    "        #print(\"mu size: \", mu.size())\n",
    "        #print(\"sigma size: \", Sigma.size())\n",
    "\n",
    "        post_pred = MultivariateNormal(mu, Sigma)\n",
    "\n",
    "        # MC-integral\n",
    "        t0 = time.time()\n",
    "        py_ = 0\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            f_s = post_pred.rsample()\n",
    "            py_ += torch.softmax(f_s, 1)\n",
    "\n",
    "        py_ /= n_samples\n",
    "        py_ = py_.detach()\n",
    "\n",
    "        py.append(py_)\n",
    "        t1 = time.time()\n",
    "        if timing:\n",
    "            time_sum += (t1 - t0)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "\n",
    "    if timing: print(\"time used for sampling with {} samples: {}\".format(n_samples, time_sum))\n",
    "    \n",
    "    return torch.cat(py, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_CIFAR10 = testset.targets\n",
    "targets_CIFAR100 = CIFAR100_test.targets\n",
    "targets_SVHN = []\n",
    "for x,y in test_loader_SVHN:\n",
    "    targets_SVHN.append(y)\n",
    "targets_SVHN = torch.cat(targets_SVHN).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_test_in_MAP = predict_MAP(CIFAR10_model, testloader, cuda=True).cpu().numpy()\n",
    "CIFAR10_test_out_CIFAR100_MAP = predict_MAP(CIFAR10_model, CIFAR100_test_loader, cuda=True).cpu().numpy()\n",
    "CIFAR10_test_out_SVHN_MAP = predict_MAP(CIFAR10_model, test_loader_SVHN, cuda=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP = get_in_dist_values(CIFAR10_test_in_MAP, targets_CIFAR10)\n",
    "acc_out_CIFAR100_MAP, prob_correct_out_CIFAR100_MAP, ent_out_CIFAR100, MMC_out_CIFAR100_MAP, auroc_out_CIFAR100_MAP = get_out_dist_values(CIFAR10_test_in_MAP, CIFAR10_test_out_CIFAR100_MAP, targets_CIFAR100)\n",
    "acc_out_SVHN_MAP, prob_correct_out_SVHN_MAP, ent_out_SVHN, MMC_out_SVHN_MAP, auroc_out_SVHN_MAP = get_out_dist_values(CIFAR10_test_in_MAP, CIFAR10_test_out_SVHN_MAP, targets_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, MAP, CIFAR10] Accuracy: 0.952; average entropy: 0.066;     MMC: 0.979; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, CIFAR100] Accuracy: 0.009; Average entropy: 0.497;    MMC: 0.828; AUROC: 0.874; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, SVHN] Accuracy: 0.112; Average entropy: 0.604;    MMC: 0.793; AUROC: 0.925; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP, 'CIFAR10', 'MAP')\n",
    "print_out_dist_values(acc_out_CIFAR100_MAP, prob_correct_out_CIFAR100_MAP, ent_out_CIFAR100, MMC_out_CIFAR100_MAP, auroc_out_CIFAR100_MAP, 'CIFAR100', 'MAP')\n",
    "print_out_dist_values(acc_out_SVHN_MAP, prob_correct_out_SVHN_MAP, ent_out_SVHN, MMC_out_SVHN_MAP, auroc_out_SVHN_MAP, 'SVHN', 'MAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.952 with std 0.001\n",
      "MMC in: 0.978 with std 0.001\n",
      "MMC out CIFAR100: 0.828 with std 0.001\n",
      "MMC out SVHN: 0.777 with std 0.030\n",
      "AUROC out CIFAR100: 0.872 with std 0.004\n",
      "AUROC out SVHN: 0.925 with std 0.008\n"
     ]
    }
   ],
   "source": [
    "#MAP estimate\n",
    "#seeds are 123,124,125,126,127\n",
    "acc_in = [0.953, 0.952, 0.952, 0.950, 0.952]\n",
    "mmc_in = [0.979, 0.978, 0.979, 0.976, 0.979]\n",
    "mmc_out_CIFAR100 = [0.831, 0.827, 0.829, 0.827, 0.828]\n",
    "mmc_out_SVHN = [0.721, 0.809, 0.780, 0.784, 0.793]\n",
    "\n",
    "auroc_out_CIFAR100 = [0.872, 0.877, 0.874, 0.864, 0.874]\n",
    "auroc_out_SVHN = [0.939, 0.919, 0.927, 0.917, 0.925]\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR100), np.std(mmc_out_CIFAR100)))\n",
    "print(\"MMC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_SVHN), np.std(mmc_out_SVHN)))\n",
    "\n",
    "print(\"AUROC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR100), np.std(auroc_out_CIFAR100)))\n",
    "print(\"AUROC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_SVHN), np.std(auroc_out_SVHN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonal estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used for sampling with 1000 samples: 6.642724990844727\n",
      "time used for sampling with 1000 samples: 6.6236491203308105\n",
      "time used for sampling with 1000 samples: 17.064507722854614\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_test_in_D = predict_diagonal_sampling(CIFAR10_model, testloader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR10_test_out_CIFAR100_D = predict_diagonal_sampling(CIFAR10_model, CIFAR100_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR10_test_out_SVHN_D = predict_diagonal_sampling(CIFAR10_model, test_loader_SVHN, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, n_samples=1000, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D = get_in_dist_values(CIFAR10_test_in_D, targets_CIFAR10)\n",
    "acc_out_CIFAR100_D, prob_correct_out_CIFAR100_D, ent_out_CIFAR100_D, MMC_out_CIFAR100_D, auroc_out_CIFAR100_D = get_out_dist_values(CIFAR10_test_in_D, CIFAR10_test_out_CIFAR100_D, targets_CIFAR100)\n",
    "acc_out_SVHN_D, prob_correct_out_SVHN_D, ent_out_SVHN_D, MMC_out_SVHN_D, auroc_out_SVHN_D = get_out_dist_values(CIFAR10_test_in_D, CIFAR10_test_out_SVHN_D, targets_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, Diag, CIFAR10] Accuracy: 0.952; average entropy: 0.194;     MMC: 0.950; Prob @ correct: 0.100\n",
      "[Out-Diag, KFAC, CIFAR100] Accuracy: 0.009; Average entropy: 0.497;    MMC: 0.724; AUROC: 0.884; Prob @ correct: 0.100\n",
      "[Out-Diag, KFAC, SVHN] Accuracy: 0.112; Average entropy: 0.971;    MMC: 0.674; AUROC: 0.931; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D, 'CIFAR10', 'Diag')\n",
    "print_out_dist_values(acc_out_CIFAR100_D, prob_correct_out_CIFAR100_D, ent_out_CIFAR100, MMC_out_CIFAR100_D, auroc_out_CIFAR100_D, 'CIFAR100', 'Diag')\n",
    "print_out_dist_values(acc_out_SVHN_D, prob_correct_out_SVHN_D, ent_out_SVHN_D, MMC_out_SVHN_D, auroc_out_SVHN_D, 'SVHN', 'Diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Bridge time in: 6.635 with std 0.047\n",
      "Sampling Bridge time out CIFAR100: 6.577 with std 0.056\n",
      "Sampling Bridge time out SVHN: 16.983 with std 0.139\n",
      "accuracy: 0.952 with std 0.001\n",
      "MMC in: 0.949 with std 0.001\n",
      "MMC out CIFAR100: 0.724 with std 0.002\n",
      "MMC out SVHN: 0.659 with std 0.028\n",
      "AUROC out CIFAR100: 0.884 with std 0.004\n",
      "AUROC out SVHN: 0.931 with std 0.007\n"
     ]
    }
   ],
   "source": [
    "#Diag Sampling\n",
    "#seeds are 123,124,125,126,127\n",
    "time_lpb_in = [6.636955976486206, 6.716174125671387, 6.57476282119751, 6.605843544006348, 6.642724990844727]\n",
    "time_lpb_out_CIFAR100 = [6.614115238189697, 6.61441969871521, 6.560410737991333, 6.474161148071289, 6.6236491203308105]\n",
    "time_lpb_out_SVHN = [17.011831521987915, 17.094997882843018, 17.031347036361694,16.71169662475586, 17.064507722854614]\n",
    "\n",
    "acc_in = [0.953, 0.952, 0.952, 0.950, 0.952]\n",
    "mmc_in = [0.950, 0.950, 0.950, 0.947, 0.950]\n",
    "mmc_out_CIFAR100 = [0.727, 0.721, 0.725, 0.724, 0.724]\n",
    "mmc_out_SVHN = [0.605, 0.685, 0.661, 0.672, 0.674]\n",
    "\n",
    "auroc_out_CIFAR100 = [0.884, 0.889, 0.885, 0.876, 0.884]\n",
    "auroc_out_SVHN = [0.943, 0.927, 0.933, 0.921, 0.931]\n",
    "\n",
    "print(\"Sampling Bridge time in: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_in), np.std(time_lpb_in)))\n",
    "print(\"Sampling Bridge time out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_CIFAR100), np.std(time_lpb_out_CIFAR100)))\n",
    "print(\"Sampling Bridge time out SVHN: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_SVHN), np.std(time_lpb_out_SVHN)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR100), np.std(mmc_out_CIFAR100)))\n",
    "print(\"MMC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_SVHN), np.std(mmc_out_SVHN)))\n",
    "\n",
    "print(\"AUROC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR100), np.std(auroc_out_CIFAR100)))\n",
    "print(\"AUROC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_SVHN), np.std(auroc_out_SVHN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet Laplace estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_from_Normal(mu, Sigma):\n",
    "    batch_size, K = mu.size(0), mu.size(-1)\n",
    "    Sigma_d = torch.diagonal(Sigma, dim1=1, dim2=2)\n",
    "    sum_exp = torch.sum(torch.exp(-1*mu), dim=1).view(-1,1)\n",
    "    alpha = 1/Sigma_d * (1 - 2/K + torch.exp(mu)/K**2 * sum_exp)\n",
    "    \n",
    "    return(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "#U_post, V_post, B_post\n",
    "def predict_DIR_LPA(model, test_loader, M_W_post, M_b_post, C_W_post, C_b_post, verbose=False, cuda=False, timing=False):\n",
    "    alphas = []\n",
    "    if timing:\n",
    "        time_sum = 0\n",
    "\n",
    "    max_len = int(np.ceil(len(test_loader.dataset)/len(test_loader)))\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        \n",
    "        if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        phi = model.phi(x)\n",
    "\n",
    "        #mu_pred = phi @ M_W_post + M_b_post\n",
    "        #mu_pred -= mu_pred.mean(1).view(-1,1)\n",
    "        #Cov_pred = torch.diag(phi @ U_post @ phi.t()).reshape(-1, 1, 1) * V_post.unsqueeze(0) + B_post.unsqueeze(0)\n",
    "\n",
    "        mu_pred, Cov_pred = get_Gaussian_output(phi, M_W_post, M_b_post, C_W_post, C_b_post)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        alpha = get_alpha_from_Normal(mu_pred, Cov_pred).detach()\n",
    "        t1 = time.time()\n",
    "        if timing:\n",
    "            time_sum += (t1-t0)\n",
    "\n",
    "        alphas.append(alpha)\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Batch: {}/{}\".format(batch_idx, max_len))\n",
    "\n",
    "    if timing:\n",
    "        print(\"total time used for transform: {:.05f}\".format(time_sum))\n",
    "    \n",
    "    return(torch.cat(alphas, dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for transform: 0.01773\n",
      "total time used for transform: 0.01598\n",
      "total time used for transform: 0.04112\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_test_in_DIR_LPA = predict_DIR_LPA(CIFAR10_model, testloader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, cuda=True, verbose=False, timing=True).cpu().numpy()\n",
    "CIFAR10_test_out_CIFAR100_DIR_LPA = predict_DIR_LPA(CIFAR10_model, CIFAR100_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR10_test_out_SVHN_DIR_LPA = predict_DIR_LPA(CIFAR10_model, test_loader_SVHN, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize to get the MAP estimate (which is the mode) of the Dirichlet\n",
    "CIFAR10_test_in_DIR_LPAn = CIFAR10_test_in_DIR_LPA/CIFAR10_test_in_DIR_LPA.sum(1).reshape(-1,1)\n",
    "CIFAR10_test_out_CIFAR100_DIR_LPAn = CIFAR10_test_out_CIFAR100_DIR_LPA/CIFAR10_test_out_CIFAR100_DIR_LPA.sum(1).reshape(-1,1)\n",
    "CIFAR10_test_out_SVHN_DIR_LPAn = CIFAR10_test_out_SVHN_DIR_LPA/CIFAR10_test_out_SVHN_DIR_LPA.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA = get_in_dist_values(CIFAR10_test_in_DIR_LPAn, targets_CIFAR10)\n",
    "acc_out_CIFAR100_DIR_LPA, prob_correct_out_CIFAR100_DIR_LPA, ent_out_CIFAR100_DIR_LPA, MMC_out_CIFAR100_DIR_LPA, auroc_out_CIFAR100_DIR_LPA = get_out_dist_values(CIFAR10_test_in_DIR_LPAn, CIFAR10_test_out_CIFAR100_DIR_LPAn, targets_CIFAR100)\n",
    "acc_out_SVHN_DIR_LPA, prob_correct_out_SVHN_DIR_LPA, ent_out_SVHN_DIR_LPA, MMC_out_SVHN_DIR_LPA, auroc_out_SVHN_DIR_LPA = get_out_dist_values(CIFAR10_test_in_DIR_LPAn, CIFAR10_test_out_SVHN_DIR_LPAn, targets_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, DIR_LPA, CIFAR10] Accuracy: 0.952; average entropy: 0.120;     MMC: 0.970; Prob @ correct: 0.100\n",
      "[Out-DIR_LPA, KFAC, CIFAR100] Accuracy: 0.009; Average entropy: 0.765;    MMC: 0.771; AUROC: 0.862; Prob @ correct: 0.100\n",
      "[Out-DIR_LPA, KFAC, SVHN] Accuracy: 0.112; Average entropy: 0.962;    MMC: 0.715; AUROC: 0.925; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA, 'CIFAR10', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_CIFAR100_DIR_LPA, prob_correct_out_CIFAR100_DIR_LPA, ent_out_CIFAR100_DIR_LPA, MMC_out_CIFAR100_DIR_LPA, auroc_out_CIFAR100_DIR_LPA, 'CIFAR100', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_SVHN_DIR_LPA, prob_correct_out_SVHN_DIR_LPA, ent_out_SVHN_DIR_LPA, MMC_out_SVHN_DIR_LPA, auroc_out_SVHN_DIR_LPA, 'SVHN', 'DIR_LPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace Bridge time in: 0.017 with std 0.000\n",
      "Laplace Bridge time out CIFAR100: 0.016 with std 0.000\n",
      "Laplace Bridge time out notmnist: 0.041 with std 0.000\n",
      "accuracy: 0.952 with std 0.001\n",
      "MMC in: 0.969 with std 0.002\n",
      "MMC out CIFAR100: 0.774 with std 0.003\n",
      "MMC out SVHN: 0.704 with std 0.036\n",
      "AUROC out CIFAR100: 0.858 with std 0.004\n",
      "AUROC out SVHN: 0.923 with std 0.008\n"
     ]
    }
   ],
   "source": [
    "#Laplace Bridge\n",
    "#seeds are 123,124,125,126,127\n",
    "time_lpb_in = [0.01729, 0.01750, 0.01749,0.01724, 0.01773]\n",
    "time_lpb_out_CIFAR100 = [0.01571,0.01601, 0.01570, 0.01580, 0.01598]\n",
    "time_lpb_out_SVHN = [0.04075, 0.04107, 0.04046, 0.04067, 0.04112]\n",
    "\n",
    "\n",
    "acc_in = [0.953, 0.952, 0.952, 0.950, 0.952]\n",
    "mmc_in = [0.971, 0.970, 0.970, 0.966, 0.970]\n",
    "mmc_out_CIFAR100 = [0.779, 0.774, 0.775, 0.771, 0.771]\n",
    "mmc_out_SVHN = [0.639, 0.747, 0.719, 0.698, 0.715]\n",
    "\n",
    "\n",
    "auroc_out_CIFAR100 = [0.856, 0.863, 0.859, 0.852, 0.862]\n",
    "auroc_out_SVHN = [0.935, 0.912, 0.923, 0.919, 0.925]\n",
    "\n",
    "\n",
    "print(\"Laplace Bridge time in: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_in), np.std(time_lpb_in)))\n",
    "print(\"Laplace Bridge time out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_CIFAR100), np.std(time_lpb_out_CIFAR100)))\n",
    "print(\"Laplace Bridge time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_SVHN), np.std(time_lpb_out_SVHN)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR100), np.std(mmc_out_CIFAR100)))\n",
    "print(\"MMC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_SVHN), np.std(mmc_out_SVHN)))\n",
    "\n",
    "print(\"AUROC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR100), np.std(auroc_out_CIFAR100)))\n",
    "print(\"AUROC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_SVHN), np.std(auroc_out_SVHN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma, loggamma\n",
    "\n",
    "def beta_function(alpha):\n",
    "    return(np.exp(np.sum([loggamma(a_i) for a_i in alpha]) - loggamma(np.sum(alpha))))\n",
    "\n",
    "def alphas_norm(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    return(alphas/alphas.sum(axis=1).reshape(-1,1))\n",
    "\n",
    "def alphas_variance(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    norm = alphas_norm(alphas)\n",
    "    nom = norm * (1 - norm)\n",
    "    den = alphas.sum(axis=1).reshape(-1,1) + 1\n",
    "    return(nom/den)\n",
    "\n",
    "def log_beta_function(alpha):\n",
    "    return(np.sum([loggamma(a_i) for a_i in alpha]) - loggamma(np.sum(alpha)))\n",
    "\n",
    "def alphas_entropy(alphas):\n",
    "    K = len(alphas[0])\n",
    "    alphas = np.array(alphas)\n",
    "    entropy = []\n",
    "    for x in alphas:\n",
    "        B = log_beta_function(x)\n",
    "        alpha_0 = np.sum(x)\n",
    "        C = (alpha_0 - K)*digamma(alpha_0)\n",
    "        D = np.sum((x-1)*digamma(x))\n",
    "        entropy.append(B + C - D)\n",
    "    \n",
    "    return(np.array(entropy))\n",
    "        \n",
    "\n",
    "def alphas_log_prob(alphas):\n",
    "    alphas = np.array(alphas)\n",
    "    dig_sum = digamma(alphas.sum(axis=1).reshape(-1,1))\n",
    "    log_prob = digamma(alphas) - dig_sum\n",
    "    return(log_prob)\n",
    "\n",
    "def auroc_entropy(alphas_in, alphas_out):\n",
    "    \n",
    "    entropy_in = alphas_entropy(alphas_in)\n",
    "    entropy_out = alphas_entropy(alphas_out)\n",
    "    labels = np.zeros(len(entropy_in)+len(entropy_out), dtype='int32')\n",
    "    labels[:len(entropy_in)] = 1\n",
    "    examples = np.concatenate([entropy_in, entropy_out])\n",
    "    auroc_ent = roc_auc_score(labels, examples)\n",
    "    return(auroc_ent)\n",
    "\n",
    "def auroc_variance(alphas_in, alphas_out, method='mean'):\n",
    "    \n",
    "    if method=='mean':\n",
    "        variance_in = alphas_variance(alphas_in).mean(1)\n",
    "        variance_out = alphas_variance(alphas_out).mean(1)\n",
    "    elif method=='max':\n",
    "        variance_in = alphas_variance(alphas_in).max(1)\n",
    "        variance_out = alphas_variance(alphas_out).max(1)\n",
    "    labels = np.zeros(len(variance_in)+len(variance_out), dtype='int32')\n",
    "    labels[:len(variance_in)] = 1\n",
    "    examples = np.concatenate([variance_in, variance_out])\n",
    "    auroc_ent = roc_auc_score(labels, examples)\n",
    "    return(auroc_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc entropy: CIFAR10 in, CIFAR100 out:  0.858544905\n",
      "auroc entropy: CIFAR10 in, SVHN out:  0.9317378879840197\n"
     ]
    }
   ],
   "source": [
    "print(\"auroc entropy: CIFAR10 in, CIFAR100 out: \", 1-auroc_entropy(alphas_in=CIFAR10_test_in_DIR_LPA, alphas_out=CIFAR10_test_out_CIFAR100_DIR_LPA))\n",
    "print(\"auroc entropy: CIFAR10 in, SVHN out: \", 1-auroc_entropy(alphas_in=CIFAR10_test_in_DIR_LPA, alphas_out=CIFAR10_test_out_SVHN_DIR_LPA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc variance: CIFAR10 in, CIFAR100 out:  0.863521435\n",
      "auroc variance: CIFAR10 in, SVHN out:  0.9317233577904118\n"
     ]
    }
   ],
   "source": [
    "print(\"auroc variance: CIFAR10 in, CIFAR100 out: \", 1-auroc_variance(alphas_in=CIFAR10_test_in_DIR_LPA, alphas_out=CIFAR10_test_out_CIFAR100_DIR_LPA, method='mean'))\n",
    "print(\"auroc variance: CIFAR10 in, SVHN out: \", 1-auroc_variance(alphas_in=CIFAR10_test_in_DIR_LPA, alphas_out=CIFAR10_test_out_SVHN_DIR_LPA, method='mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train on SVHN test on CIFAR10 and CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SVHN_model = ResNet18(num_classes=10).to(device)\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_SVHN():\n",
    "    SVHN_path = 'weights/SVHN_resnet18_SGD.pth'\n",
    "    lr = 0.1\n",
    "    epoch = 0\n",
    "    for e in [100, 25, 25]:\n",
    "        print(\"current learning rate: \", lr)\n",
    "        for _ in range(e):\n",
    "            optimizer = optim.SGD(SVHN_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "            train(SVHN_model, epoch, optimizer, train_loader_SVHN, SVHN_path)\n",
    "            test(SVHN_model, epoch, test_loader_SVHN)\n",
    "            epoch += 1\n",
    "        lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current learning rate:  0.1\n",
      "\n",
      "Epoch: 0\n",
      "train loss:  507.79862785339355\n",
      "train accuracy:  0.18289028887523048\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  449.6289610862732\n",
      "test accuracy:  0.1997157344806392\n",
      "\n",
      "Epoch: 1\n",
      "train loss:  437.37904465198517\n",
      "train accuracy:  0.2229179471419791\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  400.31377387046814\n",
      "test accuracy:  0.32241087891825443\n",
      "\n",
      "Epoch: 2\n",
      "train loss:  275.33701664209366\n",
      "train accuracy:  0.5440611555009219\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  177.69543308019638\n",
      "test accuracy:  0.7128918254456054\n",
      "\n",
      "Epoch: 3\n",
      "train loss:  120.85261020064354\n",
      "train accuracy:  0.8134219422249539\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  85.54273121058941\n",
      "test accuracy:  0.8700445605408728\n",
      "\n",
      "Epoch: 4\n",
      "train loss:  70.2429259866476\n",
      "train accuracy:  0.8949754148740012\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  61.51713141798973\n",
      "test accuracy:  0.9048478795328826\n",
      "\n",
      "Epoch: 5\n",
      "train loss:  48.21308793127537\n",
      "train accuracy:  0.9315457897971727\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  48.90836404263973\n",
      "test accuracy:  0.9235940381069453\n",
      "\n",
      "Epoch: 6\n",
      "train loss:  35.67053624428809\n",
      "train accuracy:  0.9485248924400738\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  44.744964841753244\n",
      "test accuracy:  0.9310464044253227\n",
      "\n",
      "Epoch: 7\n",
      "train loss:  28.780669862404466\n",
      "train accuracy:  0.957821143208359\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  49.56370855867863\n",
      "test accuracy:  0.9240934234787953\n",
      "\n",
      "Epoch: 8\n",
      "train loss:  23.65544592961669\n",
      "train accuracy:  0.9643899815611555\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  50.29314821586013\n",
      "test accuracy:  0.933658574062692\n",
      "\n",
      "Epoch: 9\n",
      "train loss:  19.297518542036414\n",
      "train accuracy:  0.9721496619545175\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  44.042344365268946\n",
      "test accuracy:  0.9325061462814997\n",
      "\n",
      "Epoch: 10\n",
      "train loss:  16.89285532012582\n",
      "train accuracy:  0.9741087891825445\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  25.928795233368874\n",
      "test accuracy:  0.9597418561770129\n",
      "\n",
      "Epoch: 11\n",
      "train loss:  15.470815962180495\n",
      "train accuracy:  0.9761831591886908\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  27.084487738087773\n",
      "test accuracy:  0.9581284572833436\n",
      "\n",
      "Epoch: 12\n",
      "train loss:  14.886835562065244\n",
      "train accuracy:  0.9769130301167793\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  18.846704074181616\n",
      "test accuracy:  0.969460663798402\n",
      "\n",
      "Epoch: 13\n",
      "train loss:  13.03473200649023\n",
      "train accuracy:  0.9798325138291334\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  26.001703791320324\n",
      "test accuracy:  0.9584741856177013\n",
      "\n",
      "Epoch: 14\n",
      "train loss:  11.823019040748477\n",
      "train accuracy:  0.9813690842040566\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  31.394973708316684\n",
      "test accuracy:  0.9528272894898586\n",
      "\n",
      "Epoch: 15\n",
      "train loss:  10.480591619387269\n",
      "train accuracy:  0.9842501536570375\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  19.648732823319733\n",
      "test accuracy:  0.9685387215734481\n",
      "\n",
      "Epoch: 16\n",
      "train loss:  11.377556256949902\n",
      "train accuracy:  0.9819068838352797\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  22.74193008430302\n",
      "test accuracy:  0.9636985248924401\n",
      "\n",
      "Epoch: 17\n",
      "train loss:  12.15028834529221\n",
      "train accuracy:  0.9806007990165949\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  18.78094266075641\n",
      "test accuracy:  0.9698063921327597\n",
      "\n",
      "Epoch: 18\n",
      "train loss:  10.142344320192933\n",
      "train accuracy:  0.9849416103257529\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  28.445488780736923\n",
      "test accuracy:  0.9550553165334973\n",
      "\n",
      "Epoch: 19\n",
      "train loss:  9.518930239602923\n",
      "train accuracy:  0.9848647818070068\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  30.168405475094914\n",
      "test accuracy:  0.9520974185617701\n",
      "\n",
      "Epoch: 20\n",
      "train loss:  9.018665213137865\n",
      "train accuracy:  0.9863245236631838\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  18.800625298172235\n",
      "test accuracy:  0.9702673632452367\n",
      "\n",
      "Epoch: 21\n",
      "train loss:  9.938528396189213\n",
      "train accuracy:  0.9847879532882606\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  24.162275995127857\n",
      "test accuracy:  0.961240012292563\n",
      "\n",
      "Epoch: 22\n",
      "train loss:  10.8319735173136\n",
      "train accuracy:  0.9837507682851875\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  29.094486091285944\n",
      "test accuracy:  0.9537876459741856\n",
      "\n",
      "Epoch: 23\n",
      "train loss:  10.944019801914692\n",
      "train accuracy:  0.982521511985249\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  35.46630698442459\n",
      "test accuracy:  0.9451828518746158\n",
      "\n",
      "Epoch: 24\n",
      "train loss:  12.446450352668762\n",
      "train accuracy:  0.9809849416103258\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  25.35664828494191\n",
      "test accuracy:  0.9592424708051629\n",
      "\n",
      "Epoch: 25\n",
      "train loss:  9.062683664262295\n",
      "train accuracy:  0.9862476951444377\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  18.068965289276093\n",
      "test accuracy:  0.9698063921327597\n",
      "\n",
      "Epoch: 26\n",
      "train loss:  7.6749859396368265\n",
      "train accuracy:  0.9888982790411801\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  16.774725606665015\n",
      "test accuracy:  0.9728027043638598\n",
      "\n",
      "Epoch: 27\n",
      "train loss:  9.429164430126548\n",
      "train accuracy:  0.9857483097725875\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  13.34803797211498\n",
      "test accuracy:  0.9792178856791641\n",
      "\n",
      "Epoch: 28\n",
      "train loss:  8.574571170844138\n",
      "train accuracy:  0.9875921942224954\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  23.9155250787735\n",
      "test accuracy:  0.9614320835894284\n",
      "\n",
      "Epoch: 29\n",
      "train loss:  10.86925994232297\n",
      "train accuracy:  0.9834818684695759\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  12.623011785326526\n",
      "test accuracy:  0.9804087277197295\n",
      "\n",
      "Epoch: 30\n",
      "train loss:  7.840976981446147\n",
      "train accuracy:  0.9885909649661955\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  10.244140725582838\n",
      "test accuracy:  0.9834434542102028\n",
      "\n",
      "Epoch: 31\n",
      "train loss:  5.681706464849412\n",
      "train accuracy:  0.9922403196066379\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  9.750100796110928\n",
      "test accuracy:  0.9852489244007375\n",
      "\n",
      "Epoch: 32\n",
      "train loss:  8.354566363617778\n",
      "train accuracy:  0.9878995082974801\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  26.84653776139021\n",
      "test accuracy:  0.9575138291333744\n",
      "\n",
      "Epoch: 33\n",
      "train loss:  13.656647300347686\n",
      "train accuracy:  0.9777965580823602\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  44.215946685522795\n",
      "test accuracy:  0.9316226183159189\n",
      "\n",
      "Epoch: 34\n",
      "train loss:  8.558426152914762\n",
      "train accuracy:  0.9876306084818685\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  23.911344485357404\n",
      "test accuracy:  0.9622387830362631\n",
      "\n",
      "Epoch: 35\n",
      "train loss:  6.968549190089107\n",
      "train accuracy:  0.9897818070067609\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  23.333674791734666\n",
      "test accuracy:  0.9648125384142594\n",
      "\n",
      "Epoch: 36\n",
      "train loss:  9.307289170101285\n",
      "train accuracy:  0.9858635525507068\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  17.15140410186723\n",
      "test accuracy:  0.9726106330669945\n",
      "\n",
      "Epoch: 37\n",
      "train loss:  9.592839441262186\n",
      "train accuracy:  0.9857098955132145\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  11.504986909218132\n",
      "test accuracy:  0.9812538414259373\n",
      "\n",
      "Epoch: 38\n",
      "train loss:  8.335863041691482\n",
      "train accuracy:  0.9869391518131531\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  11.921464256942272\n",
      "test accuracy:  0.9807160417947142\n",
      "\n",
      "Epoch: 39\n",
      "train loss:  9.70247264392674\n",
      "train accuracy:  0.9855946527350953\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  10.078562834765762\n",
      "test accuracy:  0.9840964966195451\n",
      "\n",
      "Epoch: 40\n",
      "train loss:  8.043975263834\n",
      "train accuracy:  0.9873232944068838\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  43.87006660178304\n",
      "test accuracy:  0.9323140749846343\n",
      "\n",
      "Epoch: 41\n",
      "train loss:  9.662591714411974\n",
      "train accuracy:  0.9849031960663799\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  35.13204220868647\n",
      "test accuracy:  0.9474877074370006\n",
      "\n",
      "Epoch: 42\n",
      "train loss:  8.421089258044958\n",
      "train accuracy:  0.9872080516287646\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  12.075986402109265\n",
      "test accuracy:  0.9803318992009834\n",
      "\n",
      "Epoch: 43\n",
      "train loss:  7.368784263730049\n",
      "train accuracy:  0.9891671788567916\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  6.877345200045966\n",
      "test accuracy:  0.9898970497848801\n",
      "\n",
      "Epoch: 44\n",
      "train loss:  4.195253210142255\n",
      "train accuracy:  0.9943915181315304\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  13.520202498184517\n",
      "test accuracy:  0.9789489858635525\n",
      "\n",
      "Epoch: 45\n",
      "train loss:  7.094799663871527\n",
      "train accuracy:  0.9897818070067609\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  19.486976396292448\n",
      "test accuracy:  0.9696911493546404\n",
      "\n",
      "Epoch: 46\n",
      "train loss:  8.498406309634447\n",
      "train accuracy:  0.9875537799631223\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  18.519281562417746\n",
      "test accuracy:  0.9705362630608482\n",
      "\n",
      "Epoch: 47\n",
      "train loss:  11.571518940851092\n",
      "train accuracy:  0.9816379840196681\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  29.954905297607183\n",
      "test accuracy:  0.9528272894898586\n",
      "\n",
      "Epoch: 48\n",
      "train loss:  7.484072707593441\n",
      "train accuracy:  0.9890903503380455\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  10.51825590338558\n",
      "test accuracy:  0.9827904118008605\n",
      "\n",
      "Epoch: 49\n",
      "train loss:  7.2803454622626305\n",
      "train accuracy:  0.9892824216349109\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  36.24358595907688\n",
      "test accuracy:  0.943838352796558\n",
      "\n",
      "Epoch: 50\n",
      "train loss:  8.558190394192934\n",
      "train accuracy:  0.9877842655193608\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  30.757234128192067\n",
      "test accuracy:  0.9528657037492317\n",
      "\n",
      "Epoch: 51\n",
      "train loss:  11.227285568602383\n",
      "train accuracy:  0.9825983405039951\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  9.323636638000607\n",
      "test accuracy:  0.9856714812538414\n",
      "\n",
      "Epoch: 52\n",
      "train loss:  7.382606914266944\n",
      "train accuracy:  0.9886293792255685\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  13.824609175790101\n",
      "test accuracy:  0.979179471419791\n",
      "\n",
      "Epoch: 53\n",
      "train loss:  5.715011216700077\n",
      "train accuracy:  0.9923171481253842\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  10.778701779665425\n",
      "test accuracy:  0.9829824830977258\n",
      "\n",
      "Epoch: 54\n",
      "train loss:  4.488346464931965\n",
      "train accuracy:  0.9943146896127842\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  12.637602775357664\n",
      "test accuracy:  0.9805623847572219\n",
      "\n",
      "Epoch: 55\n",
      "train loss:  9.569643765687943\n",
      "train accuracy:  0.9848263675476336\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  29.493774130940437\n",
      "test accuracy:  0.9542102028272895\n",
      "\n",
      "Epoch: 56\n",
      "train loss:  13.029143268242478\n",
      "train accuracy:  0.9797940995697603\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  18.493202473036945\n",
      "test accuracy:  0.9718423478795328\n",
      "\n",
      "Epoch: 57\n",
      "train loss:  8.462234916165471\n",
      "train accuracy:  0.9878995082974801\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  9.89729364681989\n",
      "test accuracy:  0.9843653964351567\n",
      "\n",
      "Epoch: 58\n",
      "train loss:  7.5177846141159534\n",
      "train accuracy:  0.9885525507068224\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  10.829679312184453\n",
      "test accuracy:  0.9829056545789797\n",
      "\n",
      "Epoch: 59\n",
      "train loss:  5.617852182593197\n",
      "train accuracy:  0.9922019053472649\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  12.112697757780552\n",
      "test accuracy:  0.9819068838352797\n",
      "\n",
      "Epoch: 60\n",
      "train loss:  7.024771235883236\n",
      "train accuracy:  0.9895129071911494\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  34.65587465558201\n",
      "test accuracy:  0.9499078057775046\n",
      "\n",
      "Epoch: 61\n",
      "train loss:  9.854432839900255\n",
      "train accuracy:  0.9842885679164106\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  33.777737848460674\n",
      "test accuracy:  0.947180393362016\n",
      "\n",
      "Epoch: 62\n",
      "train loss:  6.243640344589949\n",
      "train accuracy:  0.9912415488629379\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  12.970525737851858\n",
      "test accuracy:  0.9792562999385371\n",
      "\n",
      "Epoch: 63\n",
      "train loss:  6.143762295134366\n",
      "train accuracy:  0.9910878918254457\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  16.31992524303496\n",
      "test accuracy:  0.9756837738168408\n",
      "\n",
      "Epoch: 64\n",
      "train loss:  8.34827957302332\n",
      "train accuracy:  0.9873617086662569\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  11.037436002865434\n",
      "test accuracy:  0.9833666256914567\n",
      "\n",
      "Epoch: 65\n",
      "train loss:  8.796568360179663\n",
      "train accuracy:  0.9861708666256914\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  11.52284206962213\n",
      "test accuracy:  0.9818300553165334\n",
      "\n",
      "Epoch: 66\n",
      "train loss:  9.51278467848897\n",
      "train accuracy:  0.9853257529194838\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  21.73899192083627\n",
      "test accuracy:  0.9663875230485556\n",
      "\n",
      "Epoch: 67\n",
      "train loss:  6.9593251049518585\n",
      "train accuracy:  0.9900891210817455\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  9.715466841123998\n",
      "test accuracy:  0.9846342962507683\n",
      "\n",
      "Epoch: 68\n",
      "train loss:  8.412770751863718\n",
      "train accuracy:  0.9875537799631223\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  9.109594114590436\n",
      "test accuracy:  0.9863245236631838\n",
      "\n",
      "Epoch: 69\n",
      "train loss:  6.679308954626322\n",
      "train accuracy:  0.9897818070067609\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  11.82507874444127\n",
      "test accuracy:  0.9819452980946527\n",
      "\n",
      "Epoch: 70\n",
      "train loss:  7.574068075045943\n",
      "train accuracy:  0.9886677934849416\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  22.258586831390858\n",
      "test accuracy:  0.9687307928703135\n",
      "\n",
      "Epoch: 71\n",
      "train loss:  5.991304091177881\n",
      "train accuracy:  0.991779348494161\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  10.585810450138524\n",
      "test accuracy:  0.9835586969883221\n",
      "\n",
      "Epoch: 72\n",
      "train loss:  9.205032207071781\n",
      "train accuracy:  0.9862092808850645\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  17.49693196360022\n",
      "test accuracy:  0.9734173325138291\n",
      "\n",
      "Epoch: 73\n",
      "train loss:  5.691847864538431\n",
      "train accuracy:  0.9921634910878918\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  20.76028776075691\n",
      "test accuracy:  0.9689228641671789\n",
      "\n",
      "Epoch: 74\n",
      "train loss:  7.632950500585139\n",
      "train accuracy:  0.988821450522434\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  13.988610858097672\n",
      "test accuracy:  0.977220344191764\n",
      "\n",
      "Epoch: 75\n",
      "train loss:  7.1390452440828085\n",
      "train accuracy:  0.9892824216349109\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  15.053425233345479\n",
      "test accuracy:  0.9769514443761524\n",
      "\n",
      "Epoch: 76\n",
      "train loss:  8.385584393516183\n",
      "train accuracy:  0.9874385371850031\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  24.309949128888547\n",
      "test accuracy:  0.9627381684081131\n",
      "\n",
      "Epoch: 77\n",
      "train loss:  7.71061448007822\n",
      "train accuracy:  0.9885525507068224\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  19.99911834485829\n",
      "test accuracy:  0.9678472649047326\n",
      "\n",
      "Epoch: 78\n",
      "train loss:  7.465426029637456\n",
      "train accuracy:  0.9886677934849416\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  16.194718866376206\n",
      "test accuracy:  0.9751075599262446\n",
      "\n",
      "Epoch: 79\n",
      "train loss:  7.841951010748744\n",
      "train accuracy:  0.9880915795943455\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  8.827630588784814\n",
      "test accuracy:  0.986478180700676\n",
      "\n",
      "Epoch: 80\n",
      "train loss:  5.332568310201168\n",
      "train accuracy:  0.9929317762753535\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  12.11134167201817\n",
      "test accuracy:  0.9812154271665642\n",
      "\n",
      "Epoch: 81\n",
      "train loss:  4.459091430529952\n",
      "train accuracy:  0.9938153042409342\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  16.510819100774825\n",
      "test accuracy:  0.9739935464044254\n",
      "\n",
      "Epoch: 82\n",
      "train loss:  10.56685140915215\n",
      "train accuracy:  0.9848647818070068\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  15.01865437370725\n",
      "test accuracy:  0.9764520590043024\n",
      "\n",
      "Epoch: 83\n",
      "train loss:  9.069511391222477\n",
      "train accuracy:  0.9865550092194223\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  34.34482722170651\n",
      "test accuracy:  0.947679778733866\n",
      "\n",
      "Epoch: 84\n",
      "train loss:  7.72682087123394\n",
      "train accuracy:  0.9887062077443147\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  13.28313011676073\n",
      "test accuracy:  0.9807544560540873\n",
      "\n",
      "Epoch: 85\n",
      "train loss:  8.752557208761573\n",
      "train accuracy:  0.986439766441303\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  15.199439235031605\n",
      "test accuracy:  0.9759526736324524\n",
      "\n",
      "Epoch: 86\n",
      "train loss:  8.02195387147367\n",
      "train accuracy:  0.9884757221880762\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  21.268071062862873\n",
      "test accuracy:  0.9698063921327597\n",
      "\n",
      "Epoch: 87\n",
      "train loss:  6.052495764568448\n",
      "train accuracy:  0.9913567916410572\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  14.763567780377343\n",
      "test accuracy:  0.9767593730792871\n",
      "\n",
      "Epoch: 88\n",
      "train loss:  4.9441800359636545\n",
      "train accuracy:  0.9933159188690842\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  6.741570959798992\n",
      "test accuracy:  0.9899738783036263\n",
      "\n",
      "Epoch: 89\n",
      "train loss:  6.774502608925104\n",
      "train accuracy:  0.9897818070067609\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  22.934409951791167\n",
      "test accuracy:  0.9658881376767056\n",
      "\n",
      "Epoch: 90\n",
      "train loss:  10.123280674219131\n",
      "train accuracy:  0.9847495390288875\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  30.374355018138885\n",
      "test accuracy:  0.9521742470805162\n",
      "\n",
      "Epoch: 91\n",
      "train loss:  7.369854387827218\n",
      "train accuracy:  0.9895513214505224\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  15.931423098314553\n",
      "test accuracy:  0.9754917025199754\n",
      "\n",
      "Epoch: 92\n",
      "train loss:  9.68344078026712\n",
      "train accuracy:  0.9855946527350953\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  13.065986917819828\n",
      "test accuracy:  0.9797940995697603\n",
      "\n",
      "Epoch: 93\n",
      "train loss:  6.745038039050996\n",
      "train accuracy:  0.9903580208973571\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  15.549351187422872\n",
      "test accuracy:  0.9755685310387215\n",
      "\n",
      "Epoch: 94\n",
      "train loss:  3.992655798792839\n",
      "train accuracy:  0.9949677320221266\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  4.825957166729495\n",
      "test accuracy:  0.9939689612784266\n",
      "\n",
      "Epoch: 95\n",
      "train loss:  6.8803591169416904\n",
      "train accuracy:  0.9902043638598648\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  55.39508444815874\n",
      "test accuracy:  0.9232098955132145\n",
      "\n",
      "Epoch: 96\n",
      "train loss:  8.765535457059741\n",
      "train accuracy:  0.9863629379225568\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  26.130548683926463\n",
      "test accuracy:  0.9597034419176398\n",
      "\n",
      "Epoch: 97\n",
      "train loss:  9.277285642921925\n",
      "train accuracy:  0.9860940381069453\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  16.85992672853172\n",
      "test accuracy:  0.9741087891825445\n",
      "\n",
      "Epoch: 98\n",
      "train loss:  6.142121194861829\n",
      "train accuracy:  0.9909342347879533\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  14.012433679774404\n",
      "test accuracy:  0.9782575291948371\n",
      "\n",
      "Epoch: 99\n",
      "train loss:  4.425650401972234\n",
      "train accuracy:  0.994161032575292\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  11.067034950479865\n",
      "test accuracy:  0.9831361401352182\n",
      "current learning rate:  0.01\n",
      "\n",
      "Epoch: 100\n",
      "train loss:  1.9038981839548796\n",
      "train accuracy:  0.9981561155500922\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.4769952595233917\n",
      "test accuracy:  0.9999615857406269\n",
      "\n",
      "Epoch: 101\n",
      "train loss:  0.39207988721318543\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.3643133534351364\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 102\n",
      "train loss:  0.31796109676361084\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.31954775750637054\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 103\n",
      "train loss:  0.28682590019889176\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.294506118982099\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 104\n",
      "train loss:  0.2700630675535649\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2793324626982212\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 105\n",
      "train loss:  0.2608020219486207\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.26988461741711944\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 106\n",
      "train loss:  0.25604142737574875\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2641359902918339\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 107\n",
      "train loss:  0.25424107536673546\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.26093255227897316\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 108\n",
      "train loss:  0.2545296710450202\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2595374621450901\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 109\n",
      "train loss:  0.2563159763813019\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.25948053726460785\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 110\n",
      "train loss:  0.25920475157909095\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2603482393315062\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 111\n",
      "train loss:  0.2629257228691131\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2619445224991068\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 112\n",
      "train loss:  0.26724869501776993\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.26405748596880585\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 113\n",
      "train loss:  0.27201496553607285\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2665709679713473\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 114\n",
      "train loss:  0.27709922567009926\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2693311348557472\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 115\n",
      "train loss:  0.2823849432170391\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.27224805823061615\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 116\n",
      "train loss:  0.2877812534570694\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2752292813966051\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 117\n",
      "train loss:  0.2932186797261238\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.27825778850819916\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 118\n",
      "train loss:  0.29862534371204674\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.28126818814780563\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 119\n",
      "train loss:  0.3039381727576256\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2841892814030871\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 120\n",
      "train loss:  0.30913398670963943\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2870147339999676\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 121\n",
      "train loss:  0.3141745366156101\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.28968559205532074\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 122\n",
      "train loss:  0.31902583432383835\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29218847304582596\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 123\n",
      "train loss:  0.3236631329637021\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29452983662486076\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 124\n",
      "train loss:  0.32806971413083375\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29667043313384056\n",
      "test accuracy:  1.0\n",
      "current learning rate:  0.001\n",
      "\n",
      "Epoch: 125\n",
      "train loss:  0.32716378942131996\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2925761428195983\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 126\n",
      "train loss:  0.32759640854783356\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29280245932750404\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 127\n",
      "train loss:  0.3280310370028019\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29303166759200394\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 128\n",
      "train loss:  0.3284661110956222\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2932585577946156\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 129\n",
      "train loss:  0.32889995723962784\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2934856452047825\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 130\n",
      "train loss:  0.3293335847556591\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.293713565915823\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 131\n",
      "train loss:  0.329765010625124\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29393774759955704\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 132\n",
      "train loss:  0.33019520342350006\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2941601190250367\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 133\n",
      "train loss:  0.3306232194881886\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2943810608703643\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 134\n",
      "train loss:  0.33104917663149536\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29460098478011787\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 135\n",
      "train loss:  0.3314720231574029\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2948172229807824\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 136\n",
      "train loss:  0.3318928580265492\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2950311414897442\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 137\n",
      "train loss:  0.33231119182892144\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29524319991469383\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 138\n",
      "train loss:  0.3327265754342079\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29545295727439225\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 139\n",
      "train loss:  0.33313916507177055\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29565955814905465\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 140\n",
      "train loss:  0.333549925358966\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2958648067433387\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 141\n",
      "train loss:  0.333957027643919\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2960686970036477\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 142\n",
      "train loss:  0.3343619790393859\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29627017048187554\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 143\n",
      "train loss:  0.334763552993536\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29646771657280624\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 144\n",
      "train loss:  0.33516272041015327\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29666370153427124\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 145\n",
      "train loss:  0.33555945265106857\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2968566541094333\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 146\n",
      "train loss:  0.33595355600118637\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2970490131992847\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 147\n",
      "train loss:  0.3363439403474331\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.29723812523297966\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 148\n",
      "train loss:  0.33673230558633804\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2974250155966729\n",
      "test accuracy:  1.0\n",
      "\n",
      "Epoch: 149\n",
      "train loss:  0.33711724844761193\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2976105746347457\n",
      "test accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "train_all_SVHN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: weights/SVHN_resnet18_SGD.pth\n",
      "test loss:  0.2976105746347457\n",
      "test accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "##### if you already have a trained model ##############\n",
    "SVHN_PATH = 'weights/SVHN_resnet18_SGD.pth'\n",
    "SVHN_model = ResNet18().to(device)\n",
    "print(\"loading model from: {}\".format(SVHN_PATH))\n",
    "SVHN_model.load_state_dict(torch.load(SVHN_PATH))#, map_location=torch.device('cpu')))\n",
    "#test the model\n",
    "test(SVHN_model, 0, test_loader_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 512 inputs to linear layer with m: 10 classes\n",
      "Batch: 0/204\n",
      "Batch: 1/204\n",
      "Batch: 2/204\n",
      "Batch: 3/204\n",
      "Batch: 4/204\n",
      "Batch: 5/204\n",
      "Batch: 6/204\n",
      "Batch: 7/204\n",
      "Batch: 8/204\n",
      "Batch: 9/204\n",
      "Batch: 10/204\n",
      "Batch: 11/204\n",
      "Batch: 12/204\n",
      "Batch: 13/204\n",
      "Batch: 14/204\n",
      "Batch: 15/204\n",
      "Batch: 16/204\n",
      "Batch: 17/204\n",
      "Batch: 18/204\n",
      "Batch: 19/204\n",
      "Batch: 20/204\n",
      "Batch: 21/204\n",
      "Batch: 22/204\n",
      "Batch: 23/204\n",
      "Batch: 24/204\n",
      "Batch: 25/204\n",
      "Batch: 26/204\n",
      "Batch: 27/204\n",
      "Batch: 28/204\n",
      "Batch: 29/204\n",
      "Batch: 30/204\n",
      "Batch: 31/204\n",
      "Batch: 32/204\n",
      "Batch: 33/204\n",
      "Batch: 34/204\n",
      "Batch: 35/204\n",
      "Batch: 36/204\n",
      "Batch: 37/204\n",
      "Batch: 38/204\n",
      "Batch: 39/204\n",
      "Batch: 40/204\n",
      "Batch: 41/204\n",
      "Batch: 42/204\n",
      "Batch: 43/204\n",
      "Batch: 44/204\n",
      "Batch: 45/204\n",
      "Batch: 46/204\n",
      "Batch: 47/204\n",
      "Batch: 48/204\n",
      "Batch: 49/204\n",
      "Batch: 50/204\n",
      "Batch: 51/204\n",
      "Batch: 52/204\n",
      "Batch: 53/204\n",
      "Batch: 54/204\n",
      "Batch: 55/204\n",
      "Batch: 56/204\n",
      "Batch: 57/204\n",
      "Batch: 58/204\n",
      "Batch: 59/204\n",
      "Batch: 60/204\n",
      "Batch: 61/204\n",
      "Batch: 62/204\n",
      "Batch: 63/204\n",
      "Batch: 64/204\n",
      "Batch: 65/204\n",
      "Batch: 66/204\n",
      "Batch: 67/204\n",
      "Batch: 68/204\n",
      "Batch: 69/204\n",
      "Batch: 70/204\n",
      "Batch: 71/204\n",
      "Batch: 72/204\n",
      "Batch: 73/204\n",
      "Batch: 74/204\n",
      "Batch: 75/204\n",
      "Batch: 76/204\n",
      "Batch: 77/204\n",
      "Batch: 78/204\n",
      "Batch: 79/204\n",
      "Batch: 80/204\n",
      "Batch: 81/204\n",
      "Batch: 82/204\n",
      "Batch: 83/204\n",
      "Batch: 84/204\n",
      "Batch: 85/204\n",
      "Batch: 86/204\n",
      "Batch: 87/204\n",
      "Batch: 88/204\n",
      "Batch: 89/204\n",
      "Batch: 90/204\n",
      "Batch: 91/204\n",
      "Batch: 92/204\n",
      "Batch: 93/204\n",
      "Batch: 94/204\n",
      "Batch: 95/204\n",
      "Batch: 96/204\n",
      "Batch: 97/204\n",
      "Batch: 98/204\n",
      "Batch: 99/204\n",
      "Batch: 100/204\n",
      "Batch: 101/204\n",
      "Batch: 102/204\n",
      "Batch: 103/204\n",
      "Batch: 104/204\n",
      "Batch: 105/204\n",
      "Batch: 106/204\n",
      "Batch: 107/204\n",
      "Batch: 108/204\n",
      "Batch: 109/204\n",
      "Batch: 110/204\n",
      "Batch: 111/204\n",
      "Batch: 112/204\n",
      "Batch: 113/204\n",
      "Batch: 114/204\n",
      "Batch: 115/204\n",
      "Batch: 116/204\n",
      "Batch: 117/204\n",
      "Batch: 118/204\n",
      "Batch: 119/204\n",
      "Batch: 120/204\n",
      "Batch: 121/204\n",
      "Batch: 122/204\n",
      "Batch: 123/204\n",
      "Batch: 124/204\n",
      "Batch: 125/204\n",
      "Batch: 126/204\n",
      "Batch: 127/204\n",
      "Batch: 128/204\n",
      "Batch: 129/204\n",
      "Batch: 130/204\n",
      "Batch: 131/204\n",
      "Batch: 132/204\n",
      "Batch: 133/204\n",
      "Batch: 134/204\n",
      "Batch: 135/204\n",
      "Batch: 136/204\n",
      "Batch: 137/204\n",
      "Batch: 138/204\n",
      "Batch: 139/204\n",
      "Batch: 140/204\n",
      "Batch: 141/204\n",
      "Batch: 142/204\n",
      "Batch: 143/204\n",
      "Batch: 144/204\n",
      "Batch: 145/204\n",
      "Batch: 146/204\n",
      "Batch: 147/204\n",
      "Batch: 148/204\n",
      "Batch: 149/204\n",
      "Batch: 150/204\n",
      "Batch: 151/204\n",
      "Batch: 152/204\n",
      "Batch: 153/204\n",
      "Batch: 154/204\n",
      "Batch: 155/204\n",
      "Batch: 156/204\n",
      "Batch: 157/204\n",
      "Batch: 158/204\n",
      "Batch: 159/204\n",
      "Batch: 160/204\n",
      "Batch: 161/204\n",
      "Batch: 162/204\n",
      "Batch: 163/204\n",
      "Batch: 164/204\n",
      "Batch: 165/204\n",
      "Batch: 166/204\n",
      "Batch: 167/204\n",
      "Batch: 168/204\n",
      "Batch: 169/204\n",
      "Batch: 170/204\n",
      "Batch: 171/204\n",
      "Batch: 172/204\n",
      "Batch: 173/204\n",
      "Batch: 174/204\n",
      "Batch: 175/204\n",
      "Batch: 176/204\n",
      "Batch: 177/204\n",
      "Batch: 178/204\n",
      "Batch: 179/204\n",
      "Batch: 180/204\n",
      "Batch: 181/204\n",
      "Batch: 182/204\n",
      "Batch: 183/204\n",
      "Batch: 184/204\n",
      "Batch: 185/204\n",
      "Batch: 186/204\n",
      "Batch: 187/204\n",
      "Batch: 188/204\n",
      "Batch: 189/204\n",
      "Batch: 190/204\n",
      "Batch: 191/204\n",
      "Batch: 192/204\n",
      "Batch: 193/204\n",
      "Batch: 194/204\n",
      "Batch: 195/204\n",
      "Batch: 196/204\n",
      "Batch: 197/204\n",
      "Batch: 198/204\n",
      "Batch: 199/204\n",
      "Batch: 200/204\n",
      "Batch: 201/204\n",
      "Batch: 202/204\n",
      "Batch: 203/204\n",
      "204\n",
      "M_W_post size:  torch.Size([512, 10])\n",
      "M_b_post size:  torch.Size([10])\n",
      "C_W_post size:  torch.Size([10, 512])\n",
      "C_b_post size:  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN = Diag_second_order(model=SVHN_model,\n",
    "                                                               train_loader=train_loader_SVHN,\n",
    "                                                               var0 = 10,\n",
    "                                                               device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVHN_test_in_MAP = predict_MAP(SVHN_model, test_loader_SVHN, cuda=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR10_MAP = predict_MAP(SVHN_model, testloader, cuda=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR100_MAP = predict_MAP(SVHN_model, CIFAR100_test_loader, cuda=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP = get_in_dist_values(SVHN_test_in_MAP, targets_SVHN)\n",
    "acc_out_CIFAR10_MAP, prob_correct_out_CIFAR10_MAP, ent_out_CIFAR10, MMC_out_CIFAR10_MAP, auroc_out_CIFAR10_MAP = get_out_dist_values(SVHN_test_in_MAP, SVHN_test_out_CIFAR10_MAP, targets_CIFAR10)\n",
    "acc_out_CIFAR100_MAP, prob_correct_out_CIFAR100_MAP, ent_out_CIFAR100_MAP, MMC_out_CIFAR100_MAP, auroc_out_CIFAR100_MAP = get_out_dist_values(SVHN_test_in_MAP, SVHN_test_out_CIFAR100_MAP, targets_CIFAR100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, MAP, SVHN] Accuracy: 1.000; average entropy: 0.012;     MMC: 0.999; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, CIFAR10] Accuracy: 0.091; Average entropy: 1.237;    MMC: 0.595; AUROC: 0.997; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, CIFAR100] Accuracy: 0.011; Average entropy: 1.205;    MMC: 0.606; AUROC: 0.996; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP, 'SVHN', 'MAP')\n",
    "print_out_dist_values(acc_out_CIFAR10_MAP, prob_correct_out_CIFAR10_MAP, ent_out_CIFAR10, MMC_out_CIFAR10_MAP, auroc_out_CIFAR10_MAP, 'CIFAR10', 'MAP')\n",
    "print_out_dist_values(acc_out_CIFAR100_MAP, prob_correct_out_CIFAR100_MAP, ent_out_CIFAR100_MAP, MMC_out_CIFAR100_MAP, auroc_out_CIFAR100_MAP, 'CIFAR100', 'MAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.000 with std 0.000\n",
      "MMC in: 0.999 with std 0.000\n",
      "MMC out CIFAR10: 0.616 with std 0.013\n",
      "MMC out CIFAR100: 0.621 with std 0.010\n",
      "AUROC out CIFAR10: 0.996 with std 0.000\n",
      "AUROC out CIFAR100: 0.996 with std 0.000\n"
     ]
    }
   ],
   "source": [
    "#MAP estimate\n",
    "#seeds are 123,124,125,126,127\n",
    "acc_in = [1, 1, 1, 1, 1]\n",
    "mmc_in = [0.999, 0.999, 0.998, 0.999, 0.999]\n",
    "mmc_out_CIFAR10 = [0.633, 0.622, 0.620, 0.609, 0.595]\n",
    "mmc_out_CIFAR100 = [0.632, 0.630, 0.624, 0.614, 0.606]\n",
    "\n",
    "auroc_out_CIFAR10 = [0.996, 0.996, 0.996, 0.997, 0.997]\n",
    "auroc_out_CIFAR100 = [0.996, 0.996, 0.996, 0.996, 0.996]\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR10), np.std(mmc_out_CIFAR10)))\n",
    "print(\"MMC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR100), np.std(mmc_out_CIFAR100)))\n",
    "\n",
    "print(\"AUROC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR10), np.std(auroc_out_CIFAR10)))\n",
    "print(\"AUROC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR100), np.std(auroc_out_CIFAR100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diag sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used for sampling with 1000 samples: 17.128493070602417\n",
      "time used for sampling with 1000 samples: 6.649439334869385\n",
      "time used for sampling with 1000 samples: 6.62450098991394\n"
     ]
    }
   ],
   "source": [
    "SVHN_test_in_D = predict_diagonal_sampling(SVHN_model, test_loader_SVHN, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR10_D = predict_diagonal_sampling(SVHN_model, testloader, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR100_D = predict_diagonal_sampling(SVHN_model, CIFAR100_test_loader, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, n_samples=1000, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D = get_in_dist_values(SVHN_test_in_D, targets_SVHN)\n",
    "acc_out_CIFAR10_D, prob_correct_out_CIFAR10_D, ent_out_CIFAR10_D, MMC_out_CIFAR10_D, auroc_out_CIFAR10_D = get_out_dist_values(SVHN_test_in_D, SVHN_test_out_CIFAR10_D, targets_CIFAR10)\n",
    "acc_out_CIFAR100_D, prob_correct_out_CIFAR100_D, ent_out_CIFAR100_D, MMC_out_CIFAR100_D, auroc_out_CIFAR100_D = get_out_dist_values(SVHN_test_in_D, SVHN_test_out_CIFAR100_D, targets_CIFAR100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, Diag, SVHN] Accuracy: 1.000; average entropy: 0.095;     MMC: 0.986; Prob @ correct: 0.100\n",
      "[Out-Diag, KFAC, CIFAR10] Accuracy: 0.091; Average entropy: 1.456;    MMC: 0.519; AUROC: 0.996; Prob @ correct: 0.100\n",
      "[Out-Diag, KFAC, CIFAR100] Accuracy: 0.011; Average entropy: 1.427;    MMC: 0.530; AUROC: 0.995; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D, 'SVHN', 'Diag')\n",
    "print_out_dist_values(acc_out_CIFAR10_D, prob_correct_out_CIFAR10_D, ent_out_CIFAR10_D, MMC_out_CIFAR10_D, auroc_out_CIFAR10_D, 'CIFAR10', 'Diag')\n",
    "print_out_dist_values(acc_out_CIFAR100_D, prob_correct_out_CIFAR100_D, ent_out_CIFAR100_D, MMC_out_CIFAR100_D, auroc_out_CIFAR100_D, 'CIFAR100', 'Diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Bridge time in: 17.067 with std 0.182\n",
      "Sampling Bridge time out CIFAR10: 6.608 with std 0.046\n",
      "Sampling Bridge time out CIFAR100: 6.607 with std 0.028\n",
      "accuracy: 1.000 with std 0.000\n",
      "MMC in: 0.986 with std 0.000\n",
      "MMC out CIFAR10: 0.537 with std 0.012\n",
      "MMC out CIFAR100: 0.543 with std 0.009\n",
      "AUROC out CIFAR10: 0.995 with std 0.000\n",
      "AUROC out CIFAR100: 0.994 with std 0.000\n"
     ]
    }
   ],
   "source": [
    "#Diag Sampling\n",
    "#seeds are 123,124,125,126,127\n",
    "time_lpb_in = [17.116743326187134, 17.32284379005432, 16.99942111968994, 16.769160509109497,17.128493070602417]\n",
    "time_lpb_out_CIFAR10 = [6.582088470458984, 6.672346353530884,6.582506418228149, 6.551377058029175, 6.649439334869385]\n",
    "time_lpb_out_CIFAR100 = [6.610069751739502, 6.648055791854858, 6.576121807098389, 6.577383518218994, 6.62450098991394]\n",
    "\n",
    "acc_in = [1, 1, 1, 1, 1]\n",
    "mmc_in = [0.986, 0.986, 0.986, 0.986, 0.986]\n",
    "mmc_out_CIFAR10 = [0.554, 0.543, 0.540, 0.530, 0.519]\n",
    "mmc_out_CIFAR100 = [0.554, 0.550, 0.544, 0.535, 0.530]\n",
    "\n",
    "auroc_out_CIFAR10 = [0.995, 0.995, 0.995, 0.996, 0.996]\n",
    "auroc_out_CIFAR100 = [0.994, 0.994, 0.994, 0.995, 0.995]\n",
    "\n",
    "print(\"Sampling Bridge time in: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_in), np.std(time_lpb_in)))\n",
    "print(\"Sampling Bridge time out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_CIFAR10), np.std(time_lpb_out_CIFAR10)))\n",
    "print(\"Sampling Bridge time out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_CIFAR100), np.std(time_lpb_out_CIFAR100)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR10), np.std(mmc_out_CIFAR10)))\n",
    "print(\"MMC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR100), np.std(mmc_out_CIFAR100)))\n",
    "\n",
    "print(\"AUROC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR10), np.std(auroc_out_CIFAR10)))\n",
    "print(\"AUROC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR100), np.std(auroc_out_CIFAR100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for transform: 0.04123\n",
      "total time used for transform: 0.01716\n",
      "total time used for transform: 0.01610\n"
     ]
    }
   ],
   "source": [
    "SVHN_test_in_DIR_LPA = predict_DIR_LPA(SVHN_model, test_loader_SVHN, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, cuda=True, timing=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR10_DIR_LPA = predict_DIR_LPA(SVHN_model, testloader, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, cuda=True, timing=True).cpu().numpy()\n",
    "SVHN_test_out_CIFAR100_DIR_LPA = predict_DIR_LPA(SVHN_model, CIFAR100_test_loader, M_W_post_D_SVHN, M_b_post_D_SVHN, C_W_post_D_SVHN, C_b_post_D_SVHN, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize to get the MAP estimate (which is the mode) of the Dirichlet\n",
    "SVHN_test_in_DIR_LPAn = SVHN_test_in_DIR_LPA/SVHN_test_in_DIR_LPA.sum(1).reshape(-1,1)\n",
    "SVHN_test_out_CIFAR10_DIR_LPAn = SVHN_test_out_CIFAR10_DIR_LPA/SVHN_test_out_CIFAR10_DIR_LPA.sum(1).reshape(-1,1)\n",
    "SVHN_test_out_CIFAR100_DIR_LPAn = SVHN_test_out_CIFAR100_DIR_LPA/SVHN_test_out_CIFAR100_DIR_LPA.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA = get_in_dist_values(SVHN_test_in_DIR_LPAn, targets_SVHN)\n",
    "acc_out_CIFAR10_DIR_LPA, prob_correct_out_CIFAR10_DIR_LPA, ent_out_CIFAR10_DIR_LPA, MMC_out_CIFAR10_DIR_LPA, auroc_out_CIFAR10_DIR_LPA = get_out_dist_values(SVHN_test_in_DIR_LPAn, SVHN_test_out_CIFAR10_DIR_LPAn, targets_CIFAR10)\n",
    "acc_out_CIFAR100_DIR_LPA, prob_correct_out_CIFAR100_DIR_LPA, ent_out_CIFAR100_DIR_LPA, MMC_out_CIFAR100_DIR_LPA, auroc_out_CIFAR100_DIR_LPA = get_out_dist_values(SVHN_test_in_DIR_LPAn, SVHN_test_out_CIFAR100_DIR_LPAn, targets_CIFAR100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, DIR_LPA, SVHN] Accuracy: 1.000; average entropy: 0.062;     MMC: 0.991; Prob @ correct: 0.100\n",
      "[Out-DIR_LPA, KFAC, CIFAR10] Accuracy: 0.091; Average entropy: 1.909;    MMC: 0.365; AUROC: 0.997; Prob @ correct: 0.100\n",
      "[Out-DIR_LPA, KFAC, CIFAR100] Accuracy: 0.011; Average entropy: 1.876;    MMC: 0.379; AUROC: 0.996; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA, 'SVHN', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_CIFAR10_DIR_LPA, prob_correct_out_CIFAR10_DIR_LPA, ent_out_CIFAR10_DIR_LPA, MMC_out_CIFAR10_DIR_LPA, auroc_out_CIFAR10_DIR_LPA, 'CIFAR10', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_CIFAR100_DIR_LPA, prob_correct_out_CIFAR100_DIR_LPA, ent_out_CIFAR100_DIR_LPA, MMC_out_CIFAR100_DIR_LPA, auroc_out_CIFAR100_DIR_LPA, 'CIFAR100', 'DIR_LPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace Bridge time in: 0.041 with std 0.000\n",
      "Laplace Bridge time out CIFAR10: 0.017 with std 0.000\n",
      "Laplace Bridge time out CIFAR100: 0.016 with std 0.000\n",
      "accuracy: 1.000 with std 0.000\n",
      "MMC in: 0.991 with std 0.000\n",
      "MMC out CIFAR10: 0.392 with std 0.016\n",
      "MMC out CIFAR100: 0.400 with std 0.013\n",
      "AUROC out CIFAR10: 0.996 with std 0.000\n",
      "AUROC out CIFAR100: 0.996 with std 0.000\n"
     ]
    }
   ],
   "source": [
    "#Laplace Bridge\n",
    "#seeds are 123,124,125,126,127\n",
    "time_lpb_in = [0.04104,0.04193, 0.04068, 0.04087, 0.04123]\n",
    "time_lpb_out_CIFAR10 = [0.01681,0.01735, 0.01730, 0.01707, 0.01716]\n",
    "time_lpb_out_CIFAR100 = [0.01592, 0.01599, 0.01594, 0.01584, 0.01610]\n",
    "\n",
    "\n",
    "acc_in = [1, 1, 1, 1, 1]\n",
    "mmc_in = [0.992, 0.992, 0.991, 0.991, 0.991]\n",
    "mmc_out_CIFAR10 = [0.411, 0.401, 0.398, 0.386, 0.365]\n",
    "mmc_out_CIFAR100 = [0.413, 0.412, 0.405, 0.391, 0.379]\n",
    "\n",
    "auroc_out_CIFAR10 = [0.996, 0.996, 0.996, 0.997, 0.997]\n",
    "auroc_out_CIFAR100 = [0.995, 0.996, 0.995, 0.996, 0.996]\n",
    "\n",
    "\n",
    "print(\"Laplace Bridge time in: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_in), np.std(time_lpb_in)))\n",
    "print(\"Laplace Bridge time out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_CIFAR10), np.std(time_lpb_out_CIFAR10)))\n",
    "print(\"Laplace Bridge time out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_CIFAR100), np.std(time_lpb_out_CIFAR100)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR10), np.std(mmc_out_CIFAR10)))\n",
    "print(\"MMC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR100), np.std(mmc_out_CIFAR100)))\n",
    "\n",
    "print(\"AUROC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR10), np.std(auroc_out_CIFAR10)))\n",
    "print(\"AUROC out CIFAR100: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR100), np.std(auroc_out_CIFAR100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train on CIFAR100 and test on CIFAR10, SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CIFAR100_model = ResNet18(num_classes=100).to(device)\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_CIFAR100():\n",
    "    CIFAR100_path = 'weights/CIFAR100_resnet18_SGD.pth'\n",
    "    lr = 0.1\n",
    "    epoch = 0\n",
    "    for e in [100, 50, 50]:\n",
    "        print(\"current learning rate: \", lr)\n",
    "        for _ in range(e):\n",
    "            optimizer = optim.SGD(CIFAR100_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "            train(CIFAR100_model, epoch, optimizer, CIFAR100_train_loader, CIFAR100_path)\n",
    "            test(CIFAR100_model, epoch, CIFAR100_test_loader)\n",
    "            epoch += 1\n",
    "        lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current learning rate:  0.1\n",
      "\n",
      "Epoch: 0\n",
      "train loss:  1519.3433899879456\n",
      "train accuracy:  0.10318\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  273.86104464530945\n",
      "test accuracy:  0.1629\n",
      "\n",
      "Epoch: 1\n",
      "train loss:  1241.35178565979\n",
      "train accuracy:  0.21398\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  240.93409395217896\n",
      "test accuracy:  0.2384\n",
      "\n",
      "Epoch: 2\n",
      "train loss:  1060.6093380451202\n",
      "train accuracy:  0.30282\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  212.40526700019836\n",
      "test accuracy:  0.3127\n",
      "\n",
      "Epoch: 3\n",
      "train loss:  892.957280755043\n",
      "train accuracy:  0.39456\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  211.19521307945251\n",
      "test accuracy:  0.341\n",
      "\n",
      "Epoch: 4\n",
      "train loss:  748.8530287742615\n",
      "train accuracy:  0.47738\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  214.3763608932495\n",
      "test accuracy:  0.3502\n",
      "\n",
      "Epoch: 5\n",
      "train loss:  633.174714922905\n",
      "train accuracy:  0.54868\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  211.86059832572937\n",
      "test accuracy:  0.3707\n",
      "\n",
      "Epoch: 6\n",
      "train loss:  545.163779437542\n",
      "train accuracy:  0.6016\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  221.5753357410431\n",
      "test accuracy:  0.3563\n",
      "\n",
      "Epoch: 7\n",
      "train loss:  478.1413180232048\n",
      "train accuracy:  0.64312\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  213.63994240760803\n",
      "test accuracy:  0.3782\n",
      "\n",
      "Epoch: 8\n",
      "train loss:  407.1239330172539\n",
      "train accuracy:  0.6926\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  216.24189805984497\n",
      "test accuracy:  0.3886\n",
      "\n",
      "Epoch: 9\n",
      "train loss:  358.2215328216553\n",
      "train accuracy:  0.72244\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  244.29642271995544\n",
      "test accuracy:  0.3669\n",
      "\n",
      "Epoch: 10\n",
      "train loss:  311.38639134168625\n",
      "train accuracy:  0.75718\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  261.6777594089508\n",
      "test accuracy:  0.3515\n",
      "\n",
      "Epoch: 11\n",
      "train loss:  284.34690096974373\n",
      "train accuracy:  0.77886\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  232.39333724975586\n",
      "test accuracy:  0.3931\n",
      "\n",
      "Epoch: 12\n",
      "train loss:  254.79921942949295\n",
      "train accuracy:  0.79948\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  231.10125398635864\n",
      "test accuracy:  0.3967\n",
      "\n",
      "Epoch: 13\n",
      "train loss:  236.67460623383522\n",
      "train accuracy:  0.81316\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  242.54660725593567\n",
      "test accuracy:  0.3925\n",
      "\n",
      "Epoch: 14\n",
      "train loss:  220.8453966677189\n",
      "train accuracy:  0.8272\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  282.8220520019531\n",
      "test accuracy:  0.3692\n",
      "\n",
      "Epoch: 15\n",
      "train loss:  221.6640549302101\n",
      "train accuracy:  0.82668\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  233.42301893234253\n",
      "test accuracy:  0.4027\n",
      "\n",
      "Epoch: 16\n",
      "train loss:  204.5950165092945\n",
      "train accuracy:  0.84008\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  230.6710593700409\n",
      "test accuracy:  0.4088\n",
      "\n",
      "Epoch: 17\n",
      "train loss:  199.6768283843994\n",
      "train accuracy:  0.84522\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  266.7383394241333\n",
      "test accuracy:  0.3549\n",
      "\n",
      "Epoch: 18\n",
      "train loss:  190.5213344693184\n",
      "train accuracy:  0.8528\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  249.547128200531\n",
      "test accuracy:  0.3918\n",
      "\n",
      "Epoch: 19\n",
      "train loss:  193.2715134769678\n",
      "train accuracy:  0.84948\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  246.65673995018005\n",
      "test accuracy:  0.3942\n",
      "\n",
      "Epoch: 20\n",
      "train loss:  196.14058762788773\n",
      "train accuracy:  0.84568\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  254.47068548202515\n",
      "test accuracy:  0.3771\n",
      "\n",
      "Epoch: 21\n",
      "train loss:  185.20565137267113\n",
      "train accuracy:  0.85766\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  256.92818427085876\n",
      "test accuracy:  0.3698\n",
      "\n",
      "Epoch: 22\n",
      "train loss:  181.7274989783764\n",
      "train accuracy:  0.85804\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  249.47916054725647\n",
      "test accuracy:  0.369\n",
      "\n",
      "Epoch: 23\n",
      "train loss:  182.37285429239273\n",
      "train accuracy:  0.8594\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  231.24216079711914\n",
      "test accuracy:  0.4099\n",
      "\n",
      "Epoch: 24\n",
      "train loss:  181.68679490685463\n",
      "train accuracy:  0.86126\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  269.65346455574036\n",
      "test accuracy:  0.3828\n",
      "\n",
      "Epoch: 25\n",
      "train loss:  178.4563806951046\n",
      "train accuracy:  0.8614\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  243.533127784729\n",
      "test accuracy:  0.4124\n",
      "\n",
      "Epoch: 26\n",
      "train loss:  172.37080788612366\n",
      "train accuracy:  0.86722\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  236.13210725784302\n",
      "test accuracy:  0.4054\n",
      "\n",
      "Epoch: 27\n",
      "train loss:  175.36175560951233\n",
      "train accuracy:  0.86332\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  246.54867005348206\n",
      "test accuracy:  0.3992\n",
      "\n",
      "Epoch: 28\n",
      "train loss:  182.74293145537376\n",
      "train accuracy:  0.85858\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  245.21122288703918\n",
      "test accuracy:  0.4089\n",
      "\n",
      "Epoch: 29\n",
      "train loss:  166.35448110103607\n",
      "train accuracy:  0.87144\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  246.23614931106567\n",
      "test accuracy:  0.3877\n",
      "\n",
      "Epoch: 30\n",
      "train loss:  171.30809012055397\n",
      "train accuracy:  0.8677\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  241.02705371379852\n",
      "test accuracy:  0.4127\n",
      "\n",
      "Epoch: 31\n",
      "train loss:  173.75517907738686\n",
      "train accuracy:  0.86432\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  230.38679099082947\n",
      "test accuracy:  0.4155\n",
      "\n",
      "Epoch: 32\n",
      "train loss:  170.83442175388336\n",
      "train accuracy:  0.86878\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  242.98730969429016\n",
      "test accuracy:  0.4088\n",
      "\n",
      "Epoch: 33\n",
      "train loss:  166.47737134993076\n",
      "train accuracy:  0.87116\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  256.3429913520813\n",
      "test accuracy:  0.4024\n",
      "\n",
      "Epoch: 34\n",
      "train loss:  173.30683790147305\n",
      "train accuracy:  0.86654\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  271.04534792900085\n",
      "test accuracy:  0.3869\n",
      "\n",
      "Epoch: 35\n",
      "train loss:  171.74047753214836\n",
      "train accuracy:  0.86718\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  258.43635630607605\n",
      "test accuracy:  0.3874\n",
      "\n",
      "Epoch: 36\n",
      "train loss:  161.15541142225266\n",
      "train accuracy:  0.87676\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  246.76572346687317\n",
      "test accuracy:  0.4014\n",
      "\n",
      "Epoch: 37\n",
      "train loss:  165.58331057429314\n",
      "train accuracy:  0.87002\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  244.36190581321716\n",
      "test accuracy:  0.3939\n",
      "\n",
      "Epoch: 38\n",
      "train loss:  167.1105335354805\n",
      "train accuracy:  0.8715\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  286.0277330875397\n",
      "test accuracy:  0.3586\n",
      "\n",
      "Epoch: 39\n",
      "train loss:  168.5481248050928\n",
      "train accuracy:  0.87128\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  251.16184067726135\n",
      "test accuracy:  0.4113\n",
      "\n",
      "Epoch: 40\n",
      "train loss:  169.15667827427387\n",
      "train accuracy:  0.86966\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  253.32846450805664\n",
      "test accuracy:  0.3882\n",
      "\n",
      "Epoch: 41\n",
      "train loss:  163.7902190387249\n",
      "train accuracy:  0.87328\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  266.6159110069275\n",
      "test accuracy:  0.4055\n",
      "\n",
      "Epoch: 42\n",
      "train loss:  157.05823054909706\n",
      "train accuracy:  0.87876\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  235.02931237220764\n",
      "test accuracy:  0.419\n",
      "\n",
      "Epoch: 43\n",
      "train loss:  169.88404205441475\n",
      "train accuracy:  0.86982\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  259.58264541625977\n",
      "test accuracy:  0.4039\n",
      "\n",
      "Epoch: 44\n",
      "train loss:  161.23053766787052\n",
      "train accuracy:  0.8749\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  254.20191764831543\n",
      "test accuracy:  0.3964\n",
      "\n",
      "Epoch: 45\n",
      "train loss:  162.61936923861504\n",
      "train accuracy:  0.873\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  238.18411445617676\n",
      "test accuracy:  0.4068\n",
      "\n",
      "Epoch: 46\n",
      "train loss:  163.36741290986538\n",
      "train accuracy:  0.87482\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  244.00686621665955\n",
      "test accuracy:  0.4217\n",
      "\n",
      "Epoch: 47\n",
      "train loss:  159.80318777263165\n",
      "train accuracy:  0.87764\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  247.5364224910736\n",
      "test accuracy:  0.4255\n",
      "\n",
      "Epoch: 48\n",
      "train loss:  155.44421733915806\n",
      "train accuracy:  0.88054\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  253.38505816459656\n",
      "test accuracy:  0.4073\n",
      "\n",
      "Epoch: 49\n",
      "train loss:  167.0361543893814\n",
      "train accuracy:  0.87174\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  254.76819849014282\n",
      "test accuracy:  0.4002\n",
      "\n",
      "Epoch: 50\n",
      "train loss:  157.40994395315647\n",
      "train accuracy:  0.8787\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  305.4148938655853\n",
      "test accuracy:  0.3665\n",
      "\n",
      "Epoch: 51\n",
      "train loss:  158.4743780940771\n",
      "train accuracy:  0.87938\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  270.80628752708435\n",
      "test accuracy:  0.3933\n",
      "\n",
      "Epoch: 52\n",
      "train loss:  164.59687712788582\n",
      "train accuracy:  0.87376\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  245.9244203567505\n",
      "test accuracy:  0.4064\n",
      "\n",
      "Epoch: 53\n",
      "train loss:  154.1295166760683\n",
      "train accuracy:  0.8813\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  257.7778968811035\n",
      "test accuracy:  0.3912\n",
      "\n",
      "Epoch: 54\n",
      "train loss:  162.79114352166653\n",
      "train accuracy:  0.87502\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  266.5663924217224\n",
      "test accuracy:  0.3872\n",
      "\n",
      "Epoch: 55\n",
      "train loss:  158.47857534885406\n",
      "train accuracy:  0.87808\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  261.74596667289734\n",
      "test accuracy:  0.4016\n",
      "\n",
      "Epoch: 56\n",
      "train loss:  157.2150834351778\n",
      "train accuracy:  0.87792\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  237.32545852661133\n",
      "test accuracy:  0.4203\n",
      "\n",
      "Epoch: 57\n",
      "train loss:  161.18627746403217\n",
      "train accuracy:  0.87508\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  250.47545075416565\n",
      "test accuracy:  0.4083\n",
      "\n",
      "Epoch: 58\n",
      "train loss:  161.0221960991621\n",
      "train accuracy:  0.87472\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  238.39048552513123\n",
      "test accuracy:  0.421\n",
      "\n",
      "Epoch: 59\n",
      "train loss:  149.69170567393303\n",
      "train accuracy:  0.88328\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  227.89780449867249\n",
      "test accuracy:  0.4257\n",
      "\n",
      "Epoch: 60\n",
      "train loss:  160.34877417981625\n",
      "train accuracy:  0.87814\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  235.96204924583435\n",
      "test accuracy:  0.4213\n",
      "\n",
      "Epoch: 61\n",
      "train loss:  159.38594564795494\n",
      "train accuracy:  0.87814\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  257.56472611427307\n",
      "test accuracy:  0.4056\n",
      "\n",
      "Epoch: 62\n",
      "train loss:  156.84329643845558\n",
      "train accuracy:  0.87982\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  282.78553104400635\n",
      "test accuracy:  0.3791\n",
      "\n",
      "Epoch: 63\n",
      "train loss:  157.5366922467947\n",
      "train accuracy:  0.8785\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  245.2711923122406\n",
      "test accuracy:  0.392\n",
      "\n",
      "Epoch: 64\n",
      "train loss:  154.13976648449898\n",
      "train accuracy:  0.88184\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  218.7549307346344\n",
      "test accuracy:  0.4387\n",
      "\n",
      "Epoch: 65\n",
      "train loss:  152.59718725085258\n",
      "train accuracy:  0.8836\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  241.38401651382446\n",
      "test accuracy:  0.4253\n",
      "\n",
      "Epoch: 66\n",
      "train loss:  161.65693606436253\n",
      "train accuracy:  0.87508\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  230.5794563293457\n",
      "test accuracy:  0.4272\n",
      "\n",
      "Epoch: 67\n",
      "train loss:  153.92585296928883\n",
      "train accuracy:  0.88256\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  227.07068991661072\n",
      "test accuracy:  0.4374\n",
      "\n",
      "Epoch: 68\n",
      "train loss:  154.9257868230343\n",
      "train accuracy:  0.88122\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  242.48771786689758\n",
      "test accuracy:  0.424\n",
      "\n",
      "Epoch: 69\n",
      "train loss:  163.99315942823887\n",
      "train accuracy:  0.87364\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  238.33010244369507\n",
      "test accuracy:  0.4169\n",
      "\n",
      "Epoch: 70\n",
      "train loss:  154.56552846729755\n",
      "train accuracy:  0.88032\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  244.47140216827393\n",
      "test accuracy:  0.394\n",
      "\n",
      "Epoch: 71\n",
      "train loss:  157.53422716259956\n",
      "train accuracy:  0.87748\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  238.40867638587952\n",
      "test accuracy:  0.4372\n",
      "\n",
      "Epoch: 72\n",
      "train loss:  155.70230086147785\n",
      "train accuracy:  0.88146\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  226.52381336688995\n",
      "test accuracy:  0.4273\n",
      "\n",
      "Epoch: 73\n",
      "train loss:  154.59554363787174\n",
      "train accuracy:  0.8826\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  237.95810103416443\n",
      "test accuracy:  0.4381\n",
      "\n",
      "Epoch: 74\n",
      "train loss:  159.0987561494112\n",
      "train accuracy:  0.87686\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  227.70798563957214\n",
      "test accuracy:  0.4277\n",
      "\n",
      "Epoch: 75\n",
      "train loss:  158.2532939016819\n",
      "train accuracy:  0.87844\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  225.46672224998474\n",
      "test accuracy:  0.4291\n",
      "\n",
      "Epoch: 76\n",
      "train loss:  156.09766560792923\n",
      "train accuracy:  0.88044\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  252.4641990661621\n",
      "test accuracy:  0.4144\n",
      "\n",
      "Epoch: 77\n",
      "train loss:  159.20792663097382\n",
      "train accuracy:  0.87942\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  251.01205611228943\n",
      "test accuracy:  0.4065\n",
      "\n",
      "Epoch: 78\n",
      "train loss:  150.81628903746605\n",
      "train accuracy:  0.88334\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  233.07583498954773\n",
      "test accuracy:  0.4351\n",
      "\n",
      "Epoch: 79\n",
      "train loss:  159.4407375305891\n",
      "train accuracy:  0.87736\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  229.11241602897644\n",
      "test accuracy:  0.4326\n",
      "\n",
      "Epoch: 80\n",
      "train loss:  154.42508395016193\n",
      "train accuracy:  0.88266\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  245.52081322669983\n",
      "test accuracy:  0.4127\n",
      "\n",
      "Epoch: 81\n",
      "train loss:  154.0382798165083\n",
      "train accuracy:  0.8816\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  218.5450222492218\n",
      "test accuracy:  0.4339\n",
      "\n",
      "Epoch: 82\n",
      "train loss:  157.9856783747673\n",
      "train accuracy:  0.8792\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  235.17643761634827\n",
      "test accuracy:  0.4227\n",
      "\n",
      "Epoch: 83\n",
      "train loss:  152.3158988803625\n",
      "train accuracy:  0.88316\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  229.00595998764038\n",
      "test accuracy:  0.4315\n",
      "\n",
      "Epoch: 84\n",
      "train loss:  158.7613503932953\n",
      "train accuracy:  0.87666\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  250.91801381111145\n",
      "test accuracy:  0.3983\n",
      "\n",
      "Epoch: 85\n",
      "train loss:  159.54347988963127\n",
      "train accuracy:  0.87758\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  231.71449327468872\n",
      "test accuracy:  0.4219\n",
      "\n",
      "Epoch: 86\n",
      "train loss:  153.57408794760704\n",
      "train accuracy:  0.8805\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  230.60728931427002\n",
      "test accuracy:  0.4288\n",
      "\n",
      "Epoch: 87\n",
      "train loss:  154.5005936473608\n",
      "train accuracy:  0.8818\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  239.49962282180786\n",
      "test accuracy:  0.4274\n",
      "\n",
      "Epoch: 88\n",
      "train loss:  157.43191200494766\n",
      "train accuracy:  0.87976\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  240.2693133354187\n",
      "test accuracy:  0.4325\n",
      "\n",
      "Epoch: 89\n",
      "train loss:  159.19753643870354\n",
      "train accuracy:  0.87706\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  262.6392254829407\n",
      "test accuracy:  0.4022\n",
      "\n",
      "Epoch: 90\n",
      "train loss:  147.41928000748158\n",
      "train accuracy:  0.88682\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  261.6589574813843\n",
      "test accuracy:  0.3987\n",
      "\n",
      "Epoch: 91\n",
      "train loss:  158.80978944897652\n",
      "train accuracy:  0.87854\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  233.1523356437683\n",
      "test accuracy:  0.4406\n",
      "\n",
      "Epoch: 92\n",
      "train loss:  150.2796148210764\n",
      "train accuracy:  0.88538\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  219.25501799583435\n",
      "test accuracy:  0.4424\n",
      "\n",
      "Epoch: 93\n",
      "train loss:  153.72649917006493\n",
      "train accuracy:  0.8818\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  247.5474977493286\n",
      "test accuracy:  0.4174\n",
      "\n",
      "Epoch: 94\n",
      "train loss:  163.57816264033318\n",
      "train accuracy:  0.8728\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  241.83428597450256\n",
      "test accuracy:  0.4301\n",
      "\n",
      "Epoch: 95\n",
      "train loss:  156.5449643433094\n",
      "train accuracy:  0.87992\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  261.5189905166626\n",
      "test accuracy:  0.4029\n",
      "\n",
      "Epoch: 96\n",
      "train loss:  153.16668978333473\n",
      "train accuracy:  0.88142\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  231.61976385116577\n",
      "test accuracy:  0.4222\n",
      "\n",
      "Epoch: 97\n",
      "train loss:  153.76391077041626\n",
      "train accuracy:  0.88244\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  274.89586329460144\n",
      "test accuracy:  0.3922\n",
      "\n",
      "Epoch: 98\n",
      "train loss:  159.0780380219221\n",
      "train accuracy:  0.87934\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  290.2373812198639\n",
      "test accuracy:  0.3956\n",
      "\n",
      "Epoch: 99\n",
      "train loss:  154.08959494531155\n",
      "train accuracy:  0.88192\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  229.2637584209442\n",
      "test accuracy:  0.4389\n",
      "current learning rate:  0.01\n",
      "\n",
      "Epoch: 100\n",
      "train loss:  72.3873930759728\n",
      "train accuracy:  0.94672\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.2671661376953\n",
      "test accuracy:  0.5738\n",
      "\n",
      "Epoch: 101\n",
      "train loss:  10.145320985466242\n",
      "train accuracy:  0.99822\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  142.28275299072266\n",
      "test accuracy:  0.5784\n",
      "\n",
      "Epoch: 102\n",
      "train loss:  5.724619660526514\n",
      "train accuracy:  0.99954\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  141.5980041027069\n",
      "test accuracy:  0.5806\n",
      "\n",
      "Epoch: 103\n",
      "train loss:  4.648144256323576\n",
      "train accuracy:  0.99974\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  140.77542650699615\n",
      "test accuracy:  0.5811\n",
      "\n",
      "Epoch: 104\n",
      "train loss:  4.113895263522863\n",
      "train accuracy:  0.99986\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  139.93467950820923\n",
      "test accuracy:  0.5809\n",
      "\n",
      "Epoch: 105\n",
      "train loss:  3.8110728561878204\n",
      "train accuracy:  0.99992\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  139.08640611171722\n",
      "test accuracy:  0.5807\n",
      "\n",
      "Epoch: 106\n",
      "train loss:  3.635069537907839\n",
      "train accuracy:  0.99994\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  138.28375947475433\n",
      "test accuracy:  0.5812\n",
      "\n",
      "Epoch: 107\n",
      "train loss:  3.535899594426155\n",
      "train accuracy:  0.99994\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  137.54673075675964\n",
      "test accuracy:  0.5824\n",
      "\n",
      "Epoch: 108\n",
      "train loss:  3.4792345501482487\n",
      "train accuracy:  0.99998\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  136.89119446277618\n",
      "test accuracy:  0.582\n",
      "\n",
      "Epoch: 109\n",
      "train loss:  3.447210308164358\n",
      "train accuracy:  0.99998\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  136.30811214447021\n",
      "test accuracy:  0.5821\n",
      "\n",
      "Epoch: 110\n",
      "train loss:  3.4310074783861637\n",
      "train accuracy:  0.99998\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  135.80951595306396\n",
      "test accuracy:  0.5824\n",
      "\n",
      "Epoch: 111\n",
      "train loss:  3.4240611903369427\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  135.39003264904022\n",
      "test accuracy:  0.5831\n",
      "\n",
      "Epoch: 112\n",
      "train loss:  3.422402746975422\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  135.04446029663086\n",
      "test accuracy:  0.5822\n",
      "\n",
      "Epoch: 113\n",
      "train loss:  3.4232940673828125\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  134.77638137340546\n",
      "test accuracy:  0.5824\n",
      "\n",
      "Epoch: 114\n",
      "train loss:  3.4244879819452763\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  134.58136761188507\n",
      "test accuracy:  0.583\n",
      "\n",
      "Epoch: 115\n",
      "train loss:  3.4247677959501743\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  134.45647394657135\n",
      "test accuracy:  0.5826\n",
      "\n",
      "Epoch: 116\n",
      "train loss:  3.4234757870435715\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  134.39830374717712\n",
      "test accuracy:  0.5824\n",
      "\n",
      "Epoch: 117\n",
      "train loss:  3.420379564166069\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  134.402073264122\n",
      "test accuracy:  0.5829\n",
      "\n",
      "Epoch: 118\n",
      "train loss:  3.4151865169405937\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  134.45869183540344\n",
      "test accuracy:  0.5841\n",
      "\n",
      "Epoch: 119\n",
      "train loss:  3.4077158235013485\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  134.57272863388062\n",
      "test accuracy:  0.584\n",
      "\n",
      "Epoch: 120\n",
      "train loss:  3.398590635508299\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  134.73163723945618\n",
      "test accuracy:  0.5842\n",
      "\n",
      "Epoch: 121\n",
      "train loss:  3.3875852562487125\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  134.94232642650604\n",
      "test accuracy:  0.5842\n",
      "\n",
      "Epoch: 122\n",
      "train loss:  3.3752734065055847\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  135.19232547283173\n",
      "test accuracy:  0.5838\n",
      "\n",
      "Epoch: 123\n",
      "train loss:  3.362207032740116\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  135.46892893314362\n",
      "test accuracy:  0.5848\n",
      "\n",
      "Epoch: 124\n",
      "train loss:  3.348635047674179\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  135.77312707901\n",
      "test accuracy:  0.5854\n",
      "\n",
      "Epoch: 125\n",
      "train loss:  3.3353028409183025\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  136.11017632484436\n",
      "test accuracy:  0.5852\n",
      "\n",
      "Epoch: 126\n",
      "train loss:  3.321971792727709\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  136.47042560577393\n",
      "test accuracy:  0.5861\n",
      "\n",
      "Epoch: 127\n",
      "train loss:  3.30875289067626\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  136.83422648906708\n",
      "test accuracy:  0.5853\n",
      "\n",
      "Epoch: 128\n",
      "train loss:  3.296284258365631\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  137.2125767469406\n",
      "test accuracy:  0.585\n",
      "\n",
      "Epoch: 129\n",
      "train loss:  3.284418448805809\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  137.5980008840561\n",
      "test accuracy:  0.5851\n",
      "\n",
      "Epoch: 130\n",
      "train loss:  3.2735808938741684\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  137.98120987415314\n",
      "test accuracy:  0.5853\n",
      "\n",
      "Epoch: 131\n",
      "train loss:  3.2635382264852524\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  138.3714964389801\n",
      "test accuracy:  0.5858\n",
      "\n",
      "Epoch: 132\n",
      "train loss:  3.254338439553976\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  138.753915309906\n",
      "test accuracy:  0.5859\n",
      "\n",
      "Epoch: 133\n",
      "train loss:  3.2461994029581547\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  139.1311365365982\n",
      "test accuracy:  0.5861\n",
      "\n",
      "Epoch: 134\n",
      "train loss:  3.2385522685945034\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  139.4929642677307\n",
      "test accuracy:  0.5858\n",
      "\n",
      "Epoch: 135\n",
      "train loss:  3.2581643983721733\n",
      "train accuracy:  0.99998\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  139.8741295337677\n",
      "test accuracy:  0.5861\n",
      "\n",
      "Epoch: 136\n",
      "train loss:  3.3430472537875175\n",
      "train accuracy:  0.99998\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  140.14191567897797\n",
      "test accuracy:  0.5852\n",
      "\n",
      "Epoch: 137\n",
      "train loss:  3.2924509085714817\n",
      "train accuracy:  0.99996\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  140.4350471496582\n",
      "test accuracy:  0.5856\n",
      "\n",
      "Epoch: 138\n",
      "train loss:  3.2153575383126736\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  140.8120001554489\n",
      "test accuracy:  0.5863\n",
      "\n",
      "Epoch: 139\n",
      "train loss:  3.2026566192507744\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  141.13450276851654\n",
      "test accuracy:  0.5867\n",
      "\n",
      "Epoch: 140\n",
      "train loss:  3.1939489766955376\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  141.44437289237976\n",
      "test accuracy:  0.5863\n",
      "\n",
      "Epoch: 141\n",
      "train loss:  3.185529489070177\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  141.7320272922516\n",
      "test accuracy:  0.5867\n",
      "\n",
      "Epoch: 142\n",
      "train loss:  3.176817398518324\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  142.0136936903\n",
      "test accuracy:  0.5873\n",
      "\n",
      "Epoch: 143\n",
      "train loss:  3.1676474064588547\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  142.28322565555573\n",
      "test accuracy:  0.5873\n",
      "\n",
      "Epoch: 144\n",
      "train loss:  3.157404586672783\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  142.5437136888504\n",
      "test accuracy:  0.5873\n",
      "\n",
      "Epoch: 145\n",
      "train loss:  3.1459410153329372\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  142.7972869873047\n",
      "test accuracy:  0.588\n",
      "\n",
      "Epoch: 146\n",
      "train loss:  3.133609563112259\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.0395438671112\n",
      "test accuracy:  0.5879\n",
      "\n",
      "Epoch: 147\n",
      "train loss:  3.120854552835226\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.29242360591888\n",
      "test accuracy:  0.5882\n",
      "\n",
      "Epoch: 148\n",
      "train loss:  3.107387512922287\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.5323907136917\n",
      "test accuracy:  0.5884\n",
      "\n",
      "Epoch: 149\n",
      "train loss:  3.0932597219944\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.77819430828094\n",
      "test accuracy:  0.5884\n",
      "current learning rate:  0.001\n",
      "\n",
      "Epoch: 150\n",
      "train loss:  3.0022614635527134\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.83606708049774\n",
      "test accuracy:  0.5875\n",
      "\n",
      "Epoch: 151\n",
      "train loss:  2.989316903054714\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.85145843029022\n",
      "test accuracy:  0.588\n",
      "\n",
      "Epoch: 152\n",
      "train loss:  2.9866022542119026\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.88223695755005\n",
      "test accuracy:  0.5882\n",
      "\n",
      "Epoch: 153\n",
      "train loss:  2.9858764819800854\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.91616642475128\n",
      "test accuracy:  0.5883\n",
      "\n",
      "Epoch: 154\n",
      "train loss:  2.985791526734829\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.95086812973022\n",
      "test accuracy:  0.5884\n",
      "\n",
      "Epoch: 155\n",
      "train loss:  2.985960930585861\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  143.98522531986237\n",
      "test accuracy:  0.5887\n",
      "\n",
      "Epoch: 156\n",
      "train loss:  2.986261110752821\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.0181155204773\n",
      "test accuracy:  0.5885\n",
      "\n",
      "Epoch: 157\n",
      "train loss:  2.986629355698824\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.0502667427063\n",
      "test accuracy:  0.5884\n",
      "\n",
      "Epoch: 158\n",
      "train loss:  2.9870186299085617\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.08086037635803\n",
      "test accuracy:  0.5884\n",
      "\n",
      "Epoch: 159\n",
      "train loss:  2.9874098040163517\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.11063182353973\n",
      "test accuracy:  0.5883\n",
      "\n",
      "Epoch: 160\n",
      "train loss:  2.9877752140164375\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.14032411575317\n",
      "test accuracy:  0.5882\n",
      "\n",
      "Epoch: 161\n",
      "train loss:  2.9881173446774483\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.16837358474731\n",
      "test accuracy:  0.5882\n",
      "\n",
      "Epoch: 162\n",
      "train loss:  2.9884160943329334\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.19642806053162\n",
      "test accuracy:  0.5883\n",
      "\n",
      "Epoch: 163\n",
      "train loss:  2.988673023879528\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.2239283323288\n",
      "test accuracy:  0.5884\n",
      "\n",
      "Epoch: 164\n",
      "train loss:  2.9889000803232193\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.251185297966\n",
      "test accuracy:  0.5884\n",
      "\n",
      "Epoch: 165\n",
      "train loss:  2.98909130692482\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.27860617637634\n",
      "test accuracy:  0.5884\n",
      "\n",
      "Epoch: 166\n",
      "train loss:  2.989232014864683\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.30537176132202\n",
      "test accuracy:  0.5885\n",
      "\n",
      "Epoch: 167\n",
      "train loss:  2.9893325679004192\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.3318237066269\n",
      "test accuracy:  0.5885\n",
      "\n",
      "Epoch: 168\n",
      "train loss:  2.9893948771059513\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.35910654067993\n",
      "test accuracy:  0.5887\n",
      "\n",
      "Epoch: 169\n",
      "train loss:  2.9894087985157967\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.38603329658508\n",
      "test accuracy:  0.5886\n",
      "\n",
      "Epoch: 170\n",
      "train loss:  2.9893815144896507\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.4128270149231\n",
      "test accuracy:  0.5885\n",
      "\n",
      "Epoch: 171\n",
      "train loss:  2.9893214106559753\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.44000160694122\n",
      "test accuracy:  0.5883\n",
      "\n",
      "Epoch: 172\n",
      "train loss:  2.9892162419855595\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.46657192707062\n",
      "test accuracy:  0.5883\n",
      "\n",
      "Epoch: 173\n",
      "train loss:  2.989067278802395\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.49301743507385\n",
      "test accuracy:  0.5881\n",
      "\n",
      "Epoch: 174\n",
      "train loss:  2.988870296627283\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.5194629430771\n",
      "test accuracy:  0.588\n",
      "\n",
      "Epoch: 175\n",
      "train loss:  2.9886185191571712\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.54559338092804\n",
      "test accuracy:  0.588\n",
      "\n",
      "Epoch: 176\n",
      "train loss:  2.9883139207959175\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.57225286960602\n",
      "test accuracy:  0.5882\n",
      "\n",
      "Epoch: 177\n",
      "train loss:  2.987971056252718\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.59864211082458\n",
      "test accuracy:  0.5882\n",
      "\n",
      "Epoch: 178\n",
      "train loss:  2.987594209611416\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.62515425682068\n",
      "test accuracy:  0.5881\n",
      "\n",
      "Epoch: 179\n",
      "train loss:  2.987165194004774\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.6518154144287\n",
      "test accuracy:  0.588\n",
      "\n",
      "Epoch: 180\n",
      "train loss:  2.986690189689398\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.67843616008759\n",
      "test accuracy:  0.5877\n",
      "\n",
      "Epoch: 181\n",
      "train loss:  2.9861889854073524\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.7048728466034\n",
      "test accuracy:  0.5877\n",
      "\n",
      "Epoch: 182\n",
      "train loss:  2.985644292086363\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.73179960250854\n",
      "test accuracy:  0.5876\n",
      "\n",
      "Epoch: 183\n",
      "train loss:  2.9850540794432163\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.75833404064178\n",
      "test accuracy:  0.5875\n",
      "\n",
      "Epoch: 184\n",
      "train loss:  2.9844236969947815\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.7850240468979\n",
      "test accuracy:  0.5875\n",
      "\n",
      "Epoch: 185\n",
      "train loss:  2.9837600141763687\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.81081652641296\n",
      "test accuracy:  0.5872\n",
      "\n",
      "Epoch: 186\n",
      "train loss:  2.9830606393516064\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.83694696426392\n",
      "test accuracy:  0.5868\n",
      "\n",
      "Epoch: 187\n",
      "train loss:  2.982325002551079\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.86266028881073\n",
      "test accuracy:  0.5869\n",
      "\n",
      "Epoch: 188\n",
      "train loss:  2.981546249240637\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.88834500312805\n",
      "test accuracy:  0.5872\n",
      "\n",
      "Epoch: 189\n",
      "train loss:  2.980734072625637\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.9141345024109\n",
      "test accuracy:  0.5872\n",
      "\n",
      "Epoch: 190\n",
      "train loss:  2.979885444045067\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.94006872177124\n",
      "test accuracy:  0.5871\n",
      "\n",
      "Epoch: 191\n",
      "train loss:  2.9790170677006245\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.9660506248474\n",
      "test accuracy:  0.5872\n",
      "\n",
      "Epoch: 192\n",
      "train loss:  2.9781120344996452\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  144.991357088089\n",
      "test accuracy:  0.587\n",
      "\n",
      "Epoch: 193\n",
      "train loss:  2.97718084231019\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  145.01775908470154\n",
      "test accuracy:  0.587\n",
      "\n",
      "Epoch: 194\n",
      "train loss:  2.976221974939108\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  145.0438950061798\n",
      "test accuracy:  0.5869\n",
      "\n",
      "Epoch: 195\n",
      "train loss:  2.975238174200058\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  145.06918394565582\n",
      "test accuracy:  0.5872\n",
      "\n",
      "Epoch: 196\n",
      "train loss:  2.9742268472909927\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  145.09473276138306\n",
      "test accuracy:  0.5875\n",
      "\n",
      "Epoch: 197\n",
      "train loss:  2.973177570849657\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  145.12020027637482\n",
      "test accuracy:  0.5876\n",
      "\n",
      "Epoch: 198\n",
      "train loss:  2.9721126966178417\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  145.1458500623703\n",
      "test accuracy:  0.5876\n",
      "\n",
      "Epoch: 199\n",
      "train loss:  2.971014093607664\n",
      "train accuracy:  1.0\n",
      "saving model at: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  145.17129373550415\n",
      "test accuracy:  0.5876\n"
     ]
    }
   ],
   "source": [
    "train_all_CIFAR100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: weights/CIFAR100_resnet18_SGD.pth\n",
      "test loss:  145.17129373550415\n",
      "test accuracy:  0.5876\n"
     ]
    }
   ],
   "source": [
    "##### if you already have a trained model ##############\n",
    "CIFAR100_PATH = 'weights/CIFAR100_resnet18_SGD.pth'\n",
    "CIFAR100_model = ResNet18(num_classes=100).to(device)\n",
    "print(\"loading model from: {}\".format(CIFAR100_PATH))\n",
    "CIFAR100_model.load_state_dict(torch.load(CIFAR100_PATH))#, map_location=torch.device('cpu')))\n",
    "#test the model\n",
    "test(CIFAR100_model, 0, CIFAR100_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 512 inputs to linear layer with m: 100 classes\n",
      "Batch: 0/391\n",
      "Batch: 1/391\n",
      "Batch: 2/391\n",
      "Batch: 3/391\n",
      "Batch: 4/391\n",
      "Batch: 5/391\n",
      "Batch: 6/391\n",
      "Batch: 7/391\n",
      "Batch: 8/391\n",
      "Batch: 9/391\n",
      "Batch: 10/391\n",
      "Batch: 11/391\n",
      "Batch: 12/391\n",
      "Batch: 13/391\n",
      "Batch: 14/391\n",
      "Batch: 15/391\n",
      "Batch: 16/391\n",
      "Batch: 17/391\n",
      "Batch: 18/391\n",
      "Batch: 19/391\n",
      "Batch: 20/391\n",
      "Batch: 21/391\n",
      "Batch: 22/391\n",
      "Batch: 23/391\n",
      "Batch: 24/391\n",
      "Batch: 25/391\n",
      "Batch: 26/391\n",
      "Batch: 27/391\n",
      "Batch: 28/391\n",
      "Batch: 29/391\n",
      "Batch: 30/391\n",
      "Batch: 31/391\n",
      "Batch: 32/391\n",
      "Batch: 33/391\n",
      "Batch: 34/391\n",
      "Batch: 35/391\n",
      "Batch: 36/391\n",
      "Batch: 37/391\n",
      "Batch: 38/391\n",
      "Batch: 39/391\n",
      "Batch: 40/391\n",
      "Batch: 41/391\n",
      "Batch: 42/391\n",
      "Batch: 43/391\n",
      "Batch: 44/391\n",
      "Batch: 45/391\n",
      "Batch: 46/391\n",
      "Batch: 47/391\n",
      "Batch: 48/391\n",
      "Batch: 49/391\n",
      "Batch: 50/391\n",
      "Batch: 51/391\n",
      "Batch: 52/391\n",
      "Batch: 53/391\n",
      "Batch: 54/391\n",
      "Batch: 55/391\n",
      "Batch: 56/391\n",
      "Batch: 57/391\n",
      "Batch: 58/391\n",
      "Batch: 59/391\n",
      "Batch: 60/391\n",
      "Batch: 61/391\n",
      "Batch: 62/391\n",
      "Batch: 63/391\n",
      "Batch: 64/391\n",
      "Batch: 65/391\n",
      "Batch: 66/391\n",
      "Batch: 67/391\n",
      "Batch: 68/391\n",
      "Batch: 69/391\n",
      "Batch: 70/391\n",
      "Batch: 71/391\n",
      "Batch: 72/391\n",
      "Batch: 73/391\n",
      "Batch: 74/391\n",
      "Batch: 75/391\n",
      "Batch: 76/391\n",
      "Batch: 77/391\n",
      "Batch: 78/391\n",
      "Batch: 79/391\n",
      "Batch: 80/391\n",
      "Batch: 81/391\n",
      "Batch: 82/391\n",
      "Batch: 83/391\n",
      "Batch: 84/391\n",
      "Batch: 85/391\n",
      "Batch: 86/391\n",
      "Batch: 87/391\n",
      "Batch: 88/391\n",
      "Batch: 89/391\n",
      "Batch: 90/391\n",
      "Batch: 91/391\n",
      "Batch: 92/391\n",
      "Batch: 93/391\n",
      "Batch: 94/391\n",
      "Batch: 95/391\n",
      "Batch: 96/391\n",
      "Batch: 97/391\n",
      "Batch: 98/391\n",
      "Batch: 99/391\n",
      "Batch: 100/391\n",
      "Batch: 101/391\n",
      "Batch: 102/391\n",
      "Batch: 103/391\n",
      "Batch: 104/391\n",
      "Batch: 105/391\n",
      "Batch: 106/391\n",
      "Batch: 107/391\n",
      "Batch: 108/391\n",
      "Batch: 109/391\n",
      "Batch: 110/391\n",
      "Batch: 111/391\n",
      "Batch: 112/391\n",
      "Batch: 113/391\n",
      "Batch: 114/391\n",
      "Batch: 115/391\n",
      "Batch: 116/391\n",
      "Batch: 117/391\n",
      "Batch: 118/391\n",
      "Batch: 119/391\n",
      "Batch: 120/391\n",
      "Batch: 121/391\n",
      "Batch: 122/391\n",
      "Batch: 123/391\n",
      "Batch: 124/391\n",
      "Batch: 125/391\n",
      "Batch: 126/391\n",
      "Batch: 127/391\n",
      "Batch: 128/391\n",
      "Batch: 129/391\n",
      "Batch: 130/391\n",
      "Batch: 131/391\n",
      "Batch: 132/391\n",
      "Batch: 133/391\n",
      "Batch: 134/391\n",
      "Batch: 135/391\n",
      "Batch: 136/391\n",
      "Batch: 137/391\n",
      "Batch: 138/391\n",
      "Batch: 139/391\n",
      "Batch: 140/391\n",
      "Batch: 141/391\n",
      "Batch: 142/391\n",
      "Batch: 143/391\n",
      "Batch: 144/391\n",
      "Batch: 145/391\n",
      "Batch: 146/391\n",
      "Batch: 147/391\n",
      "Batch: 148/391\n",
      "Batch: 149/391\n",
      "Batch: 150/391\n",
      "Batch: 151/391\n",
      "Batch: 152/391\n",
      "Batch: 153/391\n",
      "Batch: 154/391\n",
      "Batch: 155/391\n",
      "Batch: 156/391\n",
      "Batch: 157/391\n",
      "Batch: 158/391\n",
      "Batch: 159/391\n",
      "Batch: 160/391\n",
      "Batch: 161/391\n",
      "Batch: 162/391\n",
      "Batch: 163/391\n",
      "Batch: 164/391\n",
      "Batch: 165/391\n",
      "Batch: 166/391\n",
      "Batch: 167/391\n",
      "Batch: 168/391\n",
      "Batch: 169/391\n",
      "Batch: 170/391\n",
      "Batch: 171/391\n",
      "Batch: 172/391\n",
      "Batch: 173/391\n",
      "Batch: 174/391\n",
      "Batch: 175/391\n",
      "Batch: 176/391\n",
      "Batch: 177/391\n",
      "Batch: 178/391\n",
      "Batch: 179/391\n",
      "Batch: 180/391\n",
      "Batch: 181/391\n",
      "Batch: 182/391\n",
      "Batch: 183/391\n",
      "Batch: 184/391\n",
      "Batch: 185/391\n",
      "Batch: 186/391\n",
      "Batch: 187/391\n",
      "Batch: 188/391\n",
      "Batch: 189/391\n",
      "Batch: 190/391\n",
      "Batch: 191/391\n",
      "Batch: 192/391\n",
      "Batch: 193/391\n",
      "Batch: 194/391\n",
      "Batch: 195/391\n",
      "Batch: 196/391\n",
      "Batch: 197/391\n",
      "Batch: 198/391\n",
      "Batch: 199/391\n",
      "Batch: 200/391\n",
      "Batch: 201/391\n",
      "Batch: 202/391\n",
      "Batch: 203/391\n",
      "Batch: 204/391\n",
      "Batch: 205/391\n",
      "Batch: 206/391\n",
      "Batch: 207/391\n",
      "Batch: 208/391\n",
      "Batch: 209/391\n",
      "Batch: 210/391\n",
      "Batch: 211/391\n",
      "Batch: 212/391\n",
      "Batch: 213/391\n",
      "Batch: 214/391\n",
      "Batch: 215/391\n",
      "Batch: 216/391\n",
      "Batch: 217/391\n",
      "Batch: 218/391\n",
      "Batch: 219/391\n",
      "Batch: 220/391\n",
      "Batch: 221/391\n",
      "Batch: 222/391\n",
      "Batch: 223/391\n",
      "Batch: 224/391\n",
      "Batch: 225/391\n",
      "Batch: 226/391\n",
      "Batch: 227/391\n",
      "Batch: 228/391\n",
      "Batch: 229/391\n",
      "Batch: 230/391\n",
      "Batch: 231/391\n",
      "Batch: 232/391\n",
      "Batch: 233/391\n",
      "Batch: 234/391\n",
      "Batch: 235/391\n",
      "Batch: 236/391\n",
      "Batch: 237/391\n",
      "Batch: 238/391\n",
      "Batch: 239/391\n",
      "Batch: 240/391\n",
      "Batch: 241/391\n",
      "Batch: 242/391\n",
      "Batch: 243/391\n",
      "Batch: 244/391\n",
      "Batch: 245/391\n",
      "Batch: 246/391\n",
      "Batch: 247/391\n",
      "Batch: 248/391\n",
      "Batch: 249/391\n",
      "Batch: 250/391\n",
      "Batch: 251/391\n",
      "Batch: 252/391\n",
      "Batch: 253/391\n",
      "Batch: 254/391\n",
      "Batch: 255/391\n",
      "Batch: 256/391\n",
      "Batch: 257/391\n",
      "Batch: 258/391\n",
      "Batch: 259/391\n",
      "Batch: 260/391\n",
      "Batch: 261/391\n",
      "Batch: 262/391\n",
      "Batch: 263/391\n",
      "Batch: 264/391\n",
      "Batch: 265/391\n",
      "Batch: 266/391\n",
      "Batch: 267/391\n",
      "Batch: 268/391\n",
      "Batch: 269/391\n",
      "Batch: 270/391\n",
      "Batch: 271/391\n",
      "Batch: 272/391\n",
      "Batch: 273/391\n",
      "Batch: 274/391\n",
      "Batch: 275/391\n",
      "Batch: 276/391\n",
      "Batch: 277/391\n",
      "Batch: 278/391\n",
      "Batch: 279/391\n",
      "Batch: 280/391\n",
      "Batch: 281/391\n",
      "Batch: 282/391\n",
      "Batch: 283/391\n",
      "Batch: 284/391\n",
      "Batch: 285/391\n",
      "Batch: 286/391\n",
      "Batch: 287/391\n",
      "Batch: 288/391\n",
      "Batch: 289/391\n",
      "Batch: 290/391\n",
      "Batch: 291/391\n",
      "Batch: 292/391\n",
      "Batch: 293/391\n",
      "Batch: 294/391\n",
      "Batch: 295/391\n",
      "Batch: 296/391\n",
      "Batch: 297/391\n",
      "Batch: 298/391\n",
      "Batch: 299/391\n",
      "Batch: 300/391\n",
      "Batch: 301/391\n",
      "Batch: 302/391\n",
      "Batch: 303/391\n",
      "Batch: 304/391\n",
      "Batch: 305/391\n",
      "Batch: 306/391\n",
      "Batch: 307/391\n",
      "Batch: 308/391\n",
      "Batch: 309/391\n",
      "Batch: 310/391\n",
      "Batch: 311/391\n",
      "Batch: 312/391\n",
      "Batch: 313/391\n",
      "Batch: 314/391\n",
      "Batch: 315/391\n",
      "Batch: 316/391\n",
      "Batch: 317/391\n",
      "Batch: 318/391\n",
      "Batch: 319/391\n",
      "Batch: 320/391\n",
      "Batch: 321/391\n",
      "Batch: 322/391\n",
      "Batch: 323/391\n",
      "Batch: 324/391\n",
      "Batch: 325/391\n",
      "Batch: 326/391\n",
      "Batch: 327/391\n",
      "Batch: 328/391\n",
      "Batch: 329/391\n",
      "Batch: 330/391\n",
      "Batch: 331/391\n",
      "Batch: 332/391\n",
      "Batch: 333/391\n",
      "Batch: 334/391\n",
      "Batch: 335/391\n",
      "Batch: 336/391\n",
      "Batch: 337/391\n",
      "Batch: 338/391\n",
      "Batch: 339/391\n",
      "Batch: 340/391\n",
      "Batch: 341/391\n",
      "Batch: 342/391\n",
      "Batch: 343/391\n",
      "Batch: 344/391\n",
      "Batch: 345/391\n",
      "Batch: 346/391\n",
      "Batch: 347/391\n",
      "Batch: 348/391\n",
      "Batch: 349/391\n",
      "Batch: 350/391\n",
      "Batch: 351/391\n",
      "Batch: 352/391\n",
      "Batch: 353/391\n",
      "Batch: 354/391\n",
      "Batch: 355/391\n",
      "Batch: 356/391\n",
      "Batch: 357/391\n",
      "Batch: 358/391\n",
      "Batch: 359/391\n",
      "Batch: 360/391\n",
      "Batch: 361/391\n",
      "Batch: 362/391\n",
      "Batch: 363/391\n",
      "Batch: 364/391\n",
      "Batch: 365/391\n",
      "Batch: 366/391\n",
      "Batch: 367/391\n",
      "Batch: 368/391\n",
      "Batch: 369/391\n",
      "Batch: 370/391\n",
      "Batch: 371/391\n",
      "Batch: 372/391\n",
      "Batch: 373/391\n",
      "Batch: 374/391\n",
      "Batch: 375/391\n",
      "Batch: 376/391\n",
      "Batch: 377/391\n",
      "Batch: 378/391\n",
      "Batch: 379/391\n",
      "Batch: 380/391\n",
      "Batch: 381/391\n",
      "Batch: 382/391\n",
      "Batch: 383/391\n",
      "Batch: 384/391\n",
      "Batch: 385/391\n",
      "Batch: 386/391\n",
      "Batch: 387/391\n",
      "Batch: 388/391\n",
      "Batch: 389/391\n",
      "Batch: 390/391\n",
      "391\n",
      "M_W_post size:  torch.Size([512, 100])\n",
      "M_b_post size:  torch.Size([100])\n",
      "C_W_post size:  torch.Size([100, 512])\n",
      "C_b_post size:  torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100 = Diag_second_order(model=CIFAR100_model,\n",
    "                                                               train_loader=CIFAR100_train_loader,\n",
    "                                                               var0 = 75,\n",
    "                                                               device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP estimate for CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR100_test_in_MAP = predict_MAP(CIFAR100_model, CIFAR100_test_loader, cuda=True).cpu().numpy()\n",
    "CIFAR100_test_out_CIFAR10_MAP = predict_MAP(CIFAR100_model, testloader, cuda=True).cpu().numpy()\n",
    "CIFAR100_test_out_SVHN_MAP = predict_MAP(CIFAR100_model, test_loader_SVHN, cuda=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP = get_in_dist_values(CIFAR100_test_in_MAP, targets_CIFAR100)\n",
    "acc_out_CIFAR10_MAP, prob_correct_out_CIFAR10_MAP, ent_out_CIFAR10, MMC_out_CIFAR10_MAP, auroc_out_CIFAR10_MAP = get_out_dist_values(CIFAR100_test_in_MAP, CIFAR100_test_out_CIFAR10_MAP, targets_CIFAR10)\n",
    "acc_out_SVHN_MAP, prob_correct_out_SVHN_MAP, ent_out_SVHN_MAP, MMC_out_SVHN_MAP, auroc_out_SVHN_MAP = get_out_dist_values(CIFAR100_test_in_MAP, CIFAR100_test_out_SVHN_MAP, targets_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, MAP, CIFAR100] Accuracy: 0.588; average entropy: 2.154;     MMC: 0.556; Prob @ correct: 0.010\n",
      "[Out-MAP, KFAC, CIFAR10] Accuracy: 0.008; Average entropy: 3.346;    MMC: 0.300; AUROC: 0.704; Prob @ correct: 0.010\n",
      "[Out-MAP, KFAC, SVHN] Accuracy: 0.003; Average entropy: 2.970;    MMC: 0.394; AUROC: 0.632; Prob @ correct: 0.010\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP, 'CIFAR100', 'MAP')\n",
    "print_out_dist_values(acc_out_CIFAR10_MAP, prob_correct_out_CIFAR10_MAP, ent_out_CIFAR10, MMC_out_CIFAR10_MAP, auroc_out_CIFAR10_MAP, 'CIFAR10', 'MAP')\n",
    "print_out_dist_values(acc_out_SVHN_MAP, prob_correct_out_SVHN_MAP, ent_out_SVHN_MAP, MMC_out_SVHN_MAP, auroc_out_SVHN_MAP, 'SVHN', 'MAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.592 with std 0.006\n",
      "MMC in: 0.564 with std 0.018\n",
      "MMC out CIFAR10: 0.298 with std 0.004\n",
      "MMC out SVHN: 0.372 with std 0.015\n",
      "AUROC out CIFAR10: 0.706 with std 0.003\n",
      "AUROC out SVHN: 0.649 with std 0.012\n"
     ]
    }
   ],
   "source": [
    "#MAP estimate\n",
    "#seeds are 123,124,125,126,127\n",
    "acc_in = [0.589, 0.585, 0.599, 0.599, 0.588]\n",
    "mmc_in = [0.552, 0.552, 0.599, 0.560, 0.556]\n",
    "mmc_out_CIFAR10 = [0.294, 0.296, 0.293, 0.305, 0.300]\n",
    "mmc_out_SVHN = [0.386, 0.355, 0.359, 0.368, 0.394]\n",
    "\n",
    "auroc_out_CIFAR10 = [0.705, 0.706, 0.713, 0.704, 0.704]\n",
    "auroc_out_SVHN = [0.637, 0.656, 0.660, 0.658, 0.632]\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR10), np.std(mmc_out_CIFAR10)))\n",
    "print(\"MMC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_SVHN), np.std(mmc_out_SVHN)))\n",
    "\n",
    "print(\"AUROC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR10), np.std(auroc_out_CIFAR10)))\n",
    "print(\"AUROC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_SVHN), np.std(auroc_out_SVHN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diag sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used for sampling with 1000 samples: 6.7309911251068115\n",
      "time used for sampling with 1000 samples: 6.638373136520386\n",
      "time used for sampling with 1000 samples: 17.25725769996643\n"
     ]
    }
   ],
   "source": [
    "CIFAR100_test_in_D = predict_diagonal_sampling(CIFAR100_model, CIFAR100_test_loader, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR100_test_out_CIFAR10_D = predict_diagonal_sampling(CIFAR100_model, testloader, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, n_samples=1000, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR100_test_out_SVHN_D = predict_diagonal_sampling(CIFAR100_model, test_loader_SVHN, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, n_samples=1000, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marius/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D = get_in_dist_values(CIFAR100_test_in_D, targets_CIFAR100)\n",
    "acc_out_CIFAR10_D, prob_correct_out_CIFAR10_D, ent_out_CIFAR10_D, MMC_out_CIFAR10_D, auroc_out_CIFAR10_D = get_out_dist_values(CIFAR100_test_in_D, CIFAR100_test_out_CIFAR10_D, targets_CIFAR10)\n",
    "acc_out_SVHN_D, prob_correct_out_SVHN_D, ent_out_SVHN_D, MMC_out_SVHN_D, auroc_out_SVHN_D = get_out_dist_values(CIFAR100_test_in_D, CIFAR100_test_out_SVHN_D, targets_CIFAR100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, Diag, CIFAR100] Accuracy: 0.587; average entropy: 2.292;     MMC: 0.527; Prob @ correct: 0.010\n",
      "[Out-Diag, KFAC, CIFAR10] Accuracy: 0.008; Average entropy: 3.442;    MMC: 0.278; AUROC: 0.705; Prob @ correct: 0.010\n",
      "[Out-Diag, KFAC, SVHN] Accuracy: 0.000; Average entropy: 3.100;    MMC: 0.368; AUROC: 0.630; Prob @ correct: 0.010\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D, 'CIFAR100', 'Diag')\n",
    "print_out_dist_values(acc_out_CIFAR10_D, prob_correct_out_CIFAR10_D, ent_out_CIFAR10_D, MMC_out_CIFAR10_D, auroc_out_CIFAR10_D, 'CIFAR10', 'Diag')\n",
    "print_out_dist_values(acc_out_SVHN_D, prob_correct_out_SVHN_D, ent_out_SVHN_D, MMC_out_SVHN_D, auroc_out_SVHN_D, 'SVHN', 'Diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Bridge time in: 6.680 with std 0.042\n",
      "Sampling Bridge time out CIFAR10: 6.672 with std 0.075\n",
      "Sampling Bridge time out SVHN: 17.160 with std 0.147\n",
      "accuracy: 0.591 with std 0.005\n",
      "MMC in: 0.527 with std 0.004\n",
      "MMC out CIFAR10: 0.276 with std 0.004\n",
      "MMC out SVHN: 0.349 with std 0.014\n",
      "AUROC out CIFAR10: 0.707 with std 0.004\n",
      "AUROC out SVHN: 0.647 with std 0.011\n"
     ]
    }
   ],
   "source": [
    "#Diag Sampling\n",
    "#seeds are 123,124,125,126,127\n",
    "time_lpb_in = [6.612371921539307, 6.715473651885986,6.670197486877441, 6.670319318771362, 6.7309911251068115]\n",
    "time_lpb_out_CIFAR10 = [6.698111295700073, 6.796610593795776, 6.656818866729736, 6.569646596908569, 6.638373136520386]\n",
    "time_lpb_out_SVHN = [17.256507873535156,17.233936071395874, 17.18303084373474, 16.87152647972107, 17.25725769996643]\n",
    "\n",
    "acc_in = [0.588, 0.586, 0.598, 0.598, 0.587]\n",
    "mmc_in = [0.523, 0.524, 0.531, 0.532, 0.527]\n",
    "mmc_out_CIFAR10 = [0.273, 0.274, 0.271, 0.283, 0.278]\n",
    "mmc_out_SVHN = [0.362, 0.332, 0.335, 0.346, 0.368]\n",
    "\n",
    "auroc_out_CIFAR10 = [0.706, 0.706, 0.714, 0.704, 0.705]\n",
    "auroc_out_SVHN = [0.636, 0.654, 0.658, 0.655, 0.630]\n",
    "\n",
    "print(\"Sampling Bridge time in: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_in), np.std(time_lpb_in)))\n",
    "print(\"Sampling Bridge time out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_CIFAR10), np.std(time_lpb_out_CIFAR10)))\n",
    "print(\"Sampling Bridge time out SVHN: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_SVHN), np.std(time_lpb_out_SVHN)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR10), np.std(mmc_out_CIFAR10)))\n",
    "print(\"MMC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_SVHN), np.std(mmc_out_SVHN)))\n",
    "\n",
    "print(\"AUROC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR10), np.std(auroc_out_CIFAR10)))\n",
    "print(\"AUROC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_SVHN), np.std(auroc_out_SVHN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for transform: 0.01648\n",
      "total time used for transform: 0.01797\n",
      "total time used for transform: 0.04194\n"
     ]
    }
   ],
   "source": [
    "CIFAR100_test_in_DIR_LPA = predict_DIR_LPA(CIFAR100_model, CIFAR100_test_loader, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR100_test_out_CIFAR10_DIR_LPA = predict_DIR_LPA(CIFAR100_model, testloader, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, cuda=True, timing=True).cpu().numpy()\n",
    "CIFAR100_test_out_SVHN_DIR_LPA = predict_DIR_LPA(CIFAR100_model, test_loader_SVHN, M_W_post_D_CIFAR100, M_b_post_D_CIFAR100, C_W_post_D_CIFAR100, C_b_post_D_CIFAR100, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize to get the MAP estimate (which is the mode) of the Dirichlet\n",
    "CIFAR100_test_in_DIR_LPAn = CIFAR100_test_in_DIR_LPA/CIFAR100_test_in_DIR_LPA.sum(1).reshape(-1,1)\n",
    "CIFAR100_test_out_CIFAR10_DIR_LPAn = CIFAR100_test_out_CIFAR10_DIR_LPA/CIFAR100_test_out_CIFAR10_DIR_LPA.sum(1).reshape(-1,1)\n",
    "CIFAR100_test_out_SVHN_DIR_LPAn = CIFAR100_test_out_SVHN_DIR_LPA/CIFAR100_test_out_SVHN_DIR_LPA.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA = get_in_dist_values(CIFAR100_test_in_DIR_LPAn, targets_CIFAR100)\n",
    "acc_out_CIFAR10_DIR_LPA, prob_correct_out_CIFAR10_DIR_LPA, ent_out_CIFAR10_DIR_LPA, MMC_out_CIFAR10_DIR_LPA, auroc_out_CIFAR10_DIR_LPA = get_out_dist_values(CIFAR100_test_in_DIR_LPAn, CIFAR100_test_out_CIFAR10_DIR_LPAn, targets_CIFAR10)\n",
    "acc_out_SVHN_DIR_LPA, prob_correct_out_SVHN_DIR_LPA, ent_out_SVHN_DIR_LPA, MMC_out_SVHN_DIR_LPA, auroc_out_SVHN_DIR_LPA = get_out_dist_values(CIFAR100_test_in_DIR_LPAn, CIFAR100_test_out_SVHN_DIR_LPAn, targets_SVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, DIR_LPA, CIFAR100] Accuracy: 0.587; average entropy: 3.577;     MMC: 0.265; Prob @ correct: 0.010\n",
      "[Out-DIR_LPA, KFAC, CIFAR10] Accuracy: 0.008; Average entropy: 4.394;    MMC: 0.070; AUROC: 0.701; Prob @ correct: 0.010\n",
      "[Out-DIR_LPA, KFAC, SVHN] Accuracy: 0.003; Average entropy: 4.413;    MMC: 0.074; AUROC: 0.643; Prob @ correct: 0.010\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_DIR_LPA, prob_correct_in_DIR_LPA, ent_in_DIR_LPA, MMC_in_DIR_LPA, 'CIFAR100', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_CIFAR10_DIR_LPA, prob_correct_out_CIFAR10_DIR_LPA, ent_out_CIFAR10_DIR_LPA, MMC_out_CIFAR10_DIR_LPA, auroc_out_CIFAR10_DIR_LPA, 'CIFAR10', 'DIR_LPA')\n",
    "print_out_dist_values(acc_out_SVHN_DIR_LPA, prob_correct_out_SVHN_DIR_LPA, ent_out_SVHN_DIR_LPA, MMC_out_SVHN_DIR_LPA, auroc_out_SVHN_DIR_LPA, 'SVHN', 'DIR_LPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace Bridge time in: 0.017 with std 0.002\n",
      "Laplace Bridge time out CIFAR10: 0.018 with std 0.000\n",
      "Laplace Bridge time out SVHN: 0.042 with std 0.000\n",
      "accuracy: 0.592 with std 0.006\n",
      "MMC in: 0.263 with std 0.003\n",
      "MMC out CIFAR10: 0.068 with std 0.003\n",
      "MMC out SVHN: 0.074 with std 0.012\n",
      "AUROC out CIFAR10: 0.703 with std 0.003\n",
      "AUROC out SVHN: 0.661 with std 0.013\n"
     ]
    }
   ],
   "source": [
    "#Laplace Bridge\n",
    "#seeds are 123,124,125,126,127\n",
    "time_lpb_in = [0.02161,0.01674, 0.01601, 0.01607, 0.01648]\n",
    "time_lpb_out_CIFAR10 = [0.01830, 0.01804, 0.01745, 0.01724, 0.01797]\n",
    "time_lpb_out_SVHN = [0.04126, 0.04220, 0.04133, 0.04108, 0.04194]\n",
    "\n",
    "\n",
    "acc_in = [0.590, 0.585, 0.600, 0.598, 0.587]\n",
    "mmc_in = [0.259, 0.260, 0.266, 0.263, 0.265]\n",
    "mmc_out_CIFAR10 = [0.067, 0.067, 0.063, 0.073, 0.070]\n",
    "mmc_out_SVHN = [0.096, 0.063, 0.063, 0.072, 0.074]\n",
    "\n",
    "auroc_out_CIFAR10 = [0.702, 0.703, 0.709, 0.700, 0.701]\n",
    "auroc_out_SVHN = [0.648, 0.668, 0.671, 0.673, 0.643]\n",
    "\n",
    "\n",
    "print(\"Laplace Bridge time in: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_in), np.std(time_lpb_in)))\n",
    "print(\"Laplace Bridge time out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_CIFAR10), np.std(time_lpb_out_CIFAR10)))\n",
    "print(\"Laplace Bridge time out SVHN: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_SVHN), np.std(time_lpb_out_SVHN)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_CIFAR10), np.std(mmc_out_CIFAR10)))\n",
    "print(\"MMC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_SVHN), np.std(mmc_out_SVHN)))\n",
    "\n",
    "print(\"AUROC out CIFAR10: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_CIFAR10), np.std(auroc_out_CIFAR10)))\n",
    "print(\"AUROC out SVHN: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_SVHN), np.std(auroc_out_SVHN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
