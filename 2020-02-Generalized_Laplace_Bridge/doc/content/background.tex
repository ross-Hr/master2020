\section{Background}
\label{sec:background}

\subsection{Change of Variable for Probability Density Function}
\label{subsec:variable_change_pdf}

%TODO reference to book
\subsubsection*{1D}
Let $X$ have a continuous density $f_X$. Let $g: \mathbb{R} \rightarrow \mathbb{R}$ be piece-wise strictly monotone and continuously differentiable, i.e. there exists intervals $I_1,I_2, ..., I_n$ which partition $\mathbb{R}$ such that $g$ is strictly monotone and continuously differentiable on the interior of each $I_i$. For each $i, g:I_i \rightarrow \mathbb{R}$ is invertible on $g(I_i)$ and let $g_i^{-1}$ be the inverse function. Let $Y = g(X)$ and $\wedge = \{y| y=g(x), x \in \mathbb{R}\}$ be the range $g$. Then the density function $f_Y$ of $Y$ exists and is given by 
\begin{equation}
\label{eq:1D_variable_transform}
f_Y(y) = \sum_{i=1}^{n} f_X(g_i^{-1}(y)) \left\vert\frac{\partial g_i^{-1}(y)}{\partial y} \right\vert \mathbf{1}_\wedge(y) 
\end{equation}

\subsubsection*{Higher dim}
TODO: Jacobian. But how do the intervals work?


\subsection{Laplace Approximation}

The Laplace approximation (LPA) is a tool to fit a normal distribution to the PDF of a given other distribution. The only constraints for the other distribution are: one peak (mode/ point of maximum) and twice differentiable. Laplace proposed a simple 2-term Taylor expansion on the log pdf. If $\hat{\theta}$ denotes the mode of a pdf $h(\theta)$, then it is also the mode of the log-pdf $q(\theta) = \log h(\theta)$. The 2-term Taylor expansion of $q(\theta)$ therefore is:

\begin{align}
	q(\theta) &\approx q(\hat{\theta}) + q'(\hat{\theta})(\theta - \hat{\theta}) + \frac{1}{2}(\theta- \hat{\theta})q''(\hat{\theta}) (\theta - \hat{\theta})\\
	&= 	q(\hat{\theta}) + 0 +  \frac{1}{2}(\theta- \hat{\theta})q''(\hat{\theta}) (\theta - \hat{\theta}) \qquad \text{[since } q'(\theta) = 0]\\
	&= c - \frac{(\theta - \mu)^2}{2\sigma^2}
\end{align}
where $c$ is a constant, $\mu = \hat{\theta}$ and $\sigma^2 = \{-q''(\hat{\theta})\}^{-1}$. The right hand side of the last line matches the log-pdf of a normal distribution $N(\mu, \sigma^2)$. Therefore the pdf $h(\theta)$ is approximated by the pdf of the normal distribution $N(\mu, \sigma^2)$ where $\mu = \hat{\theta}$ and $\sigma^2 = \{-q''(\hat{\theta})\}^{-1}$. Note, that even though this derivation is done for the one dimensional case only, it is also true for the multidimensional case. The second derivative just becomes the Hessian of the pdf at the mode.

\subsection{Exponential Family}

A pdf that can be written in the form 
\begin{equation*}\label{EF}
p(x \vert w) = h(x)\cdot \exp\left(\phi(x)^\top w - \log Z(w) \right), \qquad\text{where}\qquad Z(w):= \int_{\mathbf{X}} \exp\left(\phi(x)^\top w\right) \,dh(x)
\end{equation*}
is called an exponential family where $\phi(x): \mathbb{X} \rightarrow \mathbb{R}^d$ is called sufficient statistics, $w \in \mathbb{R}^d$: natural parameters, $\log Z(w): \mathbb{R}^d \rightarrow \mathbb{R}$: (log) partition function (normalization constant), and $h(x): \mathbb{X} \rightarrow \mathbb{R}_{+}$: base measure.

\subsection{Chi2 <-> Normal}
\label{subsec:chi2-normal}

%TODO reference the book where I took it from
It is already well-known that the Chi-squared distribution describes the sum of independent, standard normal random variables. To introduce a certain 'trick' we show the forward and backward transformation between chi2 and normal.\\
Let $X$ be normal with $\mu = 0, \sigma^2 = 1$. Let $Y = X^2$ and therefore $g(x) = x^2$, which is neither monotone nor injective. Take $I_1 = (-\infty, 0)$ and $I_2 = [0, +\infty)$. Then $g$ is monotone and injective on $I_1$ and $I_2$ and $I_1 \cup I_2 = \mathbb{R}$. $g(I_1) = (0, \infty)$ and $g(I_2) = [0, \infty)$. Then $g_1^{-1}: [0, \infty) \rightarrow \mathbb{R}$ by $g_1^{-1}(y) = -\sqrt{y}$ and $g_2^{-1}: [0, \infty) \rightarrow \mathbb{R}$ by $g_2^{-1}(y) = \sqrt{y}$. Then
$$\left\vert \frac{\partial g_i^{-1}(y)}{\partial y} \right\vert = \left\vert \frac{1}{2 \sqrt{y}} \right\vert = \frac{1}{2 \sqrt{y}}$$

Applying Equation \ref{eq:1D_variable_transform} we can transform a normal distribution to a chi-squared.

\begin{align}
	f_Y(y) &= f_X(g_1^{-1}(y))	\left\vert\frac{\partial g_1^{-1}(y)}{\partial y} \right\vert \mathbf{1}_\wedge(y) + f_X(g_2^{-1}(y))	\left\vert\frac{\partial g_2^{-1}(y)}{\partial y} \right\vert \mathbf{1}_\wedge(y) \nonumber\\
	&= \frac{1}{\sqrt{2\pi}} \exp(-\frac{y}{2}) \frac{1}{2\sqrt{y}} + \frac{1}{\sqrt{2\pi}} \exp(-\frac{y}{2}) \frac{1}{2\sqrt{y}} \qquad(y > 0)\\
	&= \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{y}}\exp(-\frac{y}{2}) \nonumber
\end{align}

The 'trick' was to split up the variable transformation in two parts to adjust for the fact that the space of the chi-squared and the Normal are different. We can reverse the same procedure to get from a chi-squared to a normal distribution. We keep the variable names from before. Let $X = \sqrt{Y}$ and therefore $h(x) = \sqrt{x}$. Then $h_1^{-1}: \mathbb{R} \rightarrow (-\infty, 0)$ by $h_1^{-1}(x) = -x^2$ and $h_2^{-1}: \mathbb{R} \rightarrow [0, \infty)$ by $h_2^{-1}(x) = x^2$. Then

$$\left\vert\frac{\partial h_i^{-1}(y)}{\partial y} \right\vert = \vert 2y \vert $$

and
 
\begin{align}
	f_X(x) &= f_y(h_1^{-1}(x)) \left\vert\frac{\partial h_1^{-1}(y)}{\partial y} \right\vert \mathbf{1}_\wedge(y) + f_y(h_2^{-1}(x)) \left\vert\frac{\partial h_2^{-1}(y)}{\partial y} \right\vert \mathbf{1}_\wedge(y) \nonumber \\
	&= \frac{1}{\sqrt{2\pi}} \frac{1}{2\sqrt{x^2}} \exp(-\frac{x^2}{2}) |2x| \mathbf{1}_{(-\infty, 0)}(x) + \frac{1}{\sqrt{2\pi}} \frac{1}{2\sqrt{x^2}} \exp(-\frac{x^2}{2}) |2x| \mathbf{1}_{[0, \infty)}(x) \\
	&= \frac{1}{\sqrt{2\pi}} \exp(-\frac{x^2}{2}) \nonumber
\end{align}

which is defined on the entirety of $\mathbb{R}$.

\subsubsection{A generalization to matrices}

"In general, an $n\times n$ matrix with n distinct nonzero eigenvalues has $2^n$ square roots. Such a matrix, $A$, has a decomposition $VDV^{-1}$ where $V$ is the matrix whose columns are eigenvectors of $A$ and $D$ is the diagonal matrix whose diagonal elements are the corresponding $n$ eigenvalues $\lambda_i$. Thus the square roots of $A$ are given by $VD^{\frac{1}{2}} V{-1}$, where $D^\frac{1}{2}$ is any square root matrix of $D$, which, for distinct eigenvalues, must be diagonal with diagonal elements equal to square roots of the diagonal elements of $D$; since there are two possible choices for a square root of each diagonal element of $D$, there are $2^n$ choices for the matrix $D^{\frac{1}{2}}$" - wikipedia. 

The Chi-squared distribution can be seen as the 1D special case of the matrix transformation and therefore has $2^1=2$ possible options for $\lambda^{\frac{1}{2}}$, namely $-\lambda^{\frac{1}{2}}$ and $+\lambda^{\frac{1}{2}}$. A higher dimensional functions such as Wishart and inverse Wishart have $2^n$ different possibilities which have to be accounted for within the transformation. 

\subsection*{An alternative way to see the variable transform}

Another way to interpret the transformation from Chi2 to normal and back is by introducing the sign function. Say we have a variable $X$ which is distributed according to a Gaussian with $\mu=0, \sigma=1$ as in the above setting. We introduce the following transformation $g(a,b)$:
\begin{align*}
	A &= \text{sign}(X) \\
	B &= |X| \\
	W &= \text{sign}(X) \\
	Z &= \sqrt{|X|} \cdot |W|
\end{align*}

with the inverse $g^{-1}(w,z)$.

The determinant of the Jacobian of this transformation is as follows

\begin{align*}
	\det(J) &= \left|\begin{bmatrix}
		\frac{\partial w}{\partial a} & \frac{\partial w}{\partial b} \\
		\frac{\partial z}{\partial a} & \frac{\partial z}{\partial b}
	\end{bmatrix}\right| \\
	&= \left|\begin{bmatrix}
	1 & 0 \\
	0 & \frac{|w|}{2\sqrt{z}}
	\end{bmatrix}\right| \\
	&= \frac{|w|}{2\sqrt{z}}
\end{align*}

The joint distribution of $w$ and $z$ can now be seen as 

\begin{align*}
		f_{w,z}(w,z) &= f_{a,b}(g^{-z}(w,z)) \det(J) \\
		&= f_a(w) f_b(w,z) \frac{|w|}{2\sqrt{z}} \\
		&= f_a(w) \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{|w|^2 z}{2}\right) \frac{|w|}{2\sqrt{z}} \\
\end{align*}

Since we know that $W$ can only take the values $-1$ and $1$ we can easily integrate over $W$. 

\begin{align*}
	f_{z} &= \int dw f_{w,z} \\
	&= \int dw f_a(w) \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{|w|^2 z}{2}\right) \frac{|w|}{2\sqrt{z}} \\
	&= 1 \cdot  \int dw \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{|w|^2 z}{2}\right) \frac{|w|}{2\sqrt{z}} \\
	&= \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{|-1|^2 z}{2}\right) \frac{|-1|}{2\sqrt{z}}  + \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{|1|^2 z}{2}\right) \frac{|1|}{2\sqrt{z}} \\
	&= \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{z}} \exp\left(-\frac{z}{2}\right) \qquad z > 0\\
\end{align*}

Similarly we can construct a transformation for the inverse transformation, i.e. from Chi2 to Gaussian. Assume we have a variable $X$ which is distributed according to a Chi2 distribution. Our transform is

\begin{align*}
	A &= \text{sign}(X) \\
	B &= X \\
	W &= \text{sign}(X) \\
	Z &= X^2 \cdot \sqrt(W)
\end{align*}

Then we get a determinant of the Jacobian

\begin{align*}
	\det(J) &= \left|\begin{bmatrix}
			\frac{\partial w}{\partial a} & \frac{\partial w}{\partial b} \\
			\frac{\partial z}{\partial a} & \frac{\partial z}{\partial b}
			\end{bmatrix}\right| \\
			&= \left|\begin{bmatrix}
			1 & 0 \\
			0 & 2z \sqrt{w}
			\end{bmatrix}\right| \\
			&= 2z \sqrt{w}
\end{align*}

And the joint distribution becomes 

\begin{align*}
	f_{w,z} &=  f_{a,b}(g^{-z}(w,z)) \det(J) \\
	&= f_a(w) f_b(w,z) 2z \sqrt{w} \\
	&= f_a(w) \frac{1}{z^2 w} \exp \left(-\frac{z^2 w}{2}\right) 2z \sqrt{w} \\
\end{align*}

We can marginalize over $W$ as follows. Note that $X$ is distributed according to a Chi2 and therefore only allows positive values. $W$ is therefore only $1$. 


TODO: this kinda feels like cheating AND has to be refined. No variables should be assigned twice. 